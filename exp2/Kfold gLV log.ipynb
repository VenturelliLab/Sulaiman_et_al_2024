{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a637f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-417ev842 because the default path (/home/jaron/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm, linregress\n",
    "\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from glove.log_model import *\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06280f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EXP0019_MS001_processed.csv',\n",
       " 'EXP0019_DSM27147_processed.csv',\n",
       " 'EXP0019_MS008_processed.csv',\n",
       " 'EXP0019_MS014_processed.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import file names\n",
    "files = os.listdir(\"data/\")\n",
    "files = [f for f in files if \"processed\" in f]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66f683",
   "metadata": {},
   "source": [
    "# Function to make predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104c6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_df(model, df, species):\n",
    "    \n",
    "    # save measured and predicted values\n",
    "    pred_species = []\n",
    "    pred = []\n",
    "    stdv = []\n",
    "    true = []\n",
    "\n",
    "    # pull just the community data\n",
    "    test_data = process_df(df, species) \n",
    "\n",
    "    # plot the results\n",
    "    for exp, t_span, Y_m, s_present in test_data:\n",
    "\n",
    "        # increase evaluation time\n",
    "        t_eval = np.linspace(t_span[0], t_span[-1])\n",
    "\n",
    "        # predict \n",
    "        Y_p, L, U = model.predict(Y_m, t_eval, log=True)\n",
    "        Y_std = U - Y_p\n",
    "        \n",
    "        # un log scale y\n",
    "        Y_m = np.einsum(\"ij,j->ij\", np.exp(Y_m), s_present)\n",
    "\n",
    "        ### append only end-point prediction results for non-zero outcomes ###\n",
    "        inds_pos = Y_m[-1,:] > 0 \n",
    "        pred_species.append(np.array(species)[inds_pos])\n",
    "        true.append(Y_m[-1,:][inds_pos])\n",
    "        pred.append(Y_p[-1,:][inds_pos])\n",
    "        stdv.append(Y_std[-1,:][inds_pos])\n",
    "        \n",
    "    # concatenate list\n",
    "    pred_species = np.concatenate(pred_species)\n",
    "    true = np.concatenate(true)\n",
    "    pred = np.concatenate(pred)\n",
    "    stdv = np.concatenate(stdv)\n",
    "        \n",
    "    return pred_species, true, pred, stdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1147f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 38, Updated regularization: 1.00e-05\n",
      "Loss: 410.838, Residuals: -1.920\n",
      "Loss: 308.875, Residuals: -1.482\n",
      "Loss: 271.141, Residuals: -0.766\n",
      "Loss: 247.717, Residuals: 0.412\n",
      "Loss: 194.458, Residuals: 0.024\n",
      "Loss: 181.162, Residuals: -0.005\n",
      "Loss: 177.343, Residuals: 0.361\n",
      "Loss: 170.099, Residuals: 0.303\n",
      "Loss: 157.177, Residuals: 0.188\n",
      "Loss: 142.462, Residuals: 0.443\n",
      "Loss: 137.660, Residuals: 0.735\n",
      "Loss: 128.928, Residuals: 0.656\n",
      "Loss: 121.618, Residuals: 0.051\n",
      "Loss: 120.840, Residuals: 0.178\n",
      "Loss: 114.334, Residuals: 0.164\n",
      "Loss: 111.436, Residuals: 0.661\n",
      "Loss: 111.329, Residuals: 0.634\n",
      "Loss: 107.368, Residuals: 0.558\n",
      "Loss: 101.672, Residuals: 0.357\n",
      "Loss: 101.430, Residuals: 0.375\n",
      "Loss: 99.204, Residuals: 0.308\n",
      "Loss: 95.390, Residuals: 0.190\n",
      "Loss: 95.028, Residuals: 0.219\n",
      "Loss: 92.085, Residuals: 0.151\n",
      "Loss: 92.030, Residuals: 0.126\n",
      "Loss: 91.927, Residuals: 0.136\n",
      "Loss: 91.737, Residuals: 0.150\n",
      "Loss: 90.044, Residuals: 0.140\n",
      "Loss: 89.991, Residuals: 0.096\n",
      "Loss: 89.533, Residuals: 0.113\n",
      "Loss: 88.667, Residuals: 0.108\n",
      "Loss: 88.372, Residuals: 0.153\n",
      "Loss: 86.016, Residuals: 0.157\n",
      "Loss: 85.983, Residuals: 0.136\n",
      "Loss: 85.929, Residuals: 0.127\n",
      "Loss: 85.871, Residuals: 0.120\n",
      "Loss: 83.908, Residuals: 0.122\n",
      "Loss: 83.896, Residuals: 0.109\n",
      "Loss: 83.877, Residuals: 0.092\n",
      "Loss: 83.700, Residuals: 0.092\n",
      "Loss: 83.379, Residuals: 0.098\n",
      "Loss: 82.792, Residuals: 0.109\n",
      "Loss: 82.653, Residuals: 0.144\n",
      "Loss: 81.478, Residuals: 0.106\n",
      "Loss: 81.468, Residuals: 0.115\n",
      "Loss: 81.081, Residuals: 0.112\n",
      "Loss: 80.977, Residuals: 0.137\n",
      "Loss: 80.783, Residuals: 0.138\n",
      "Loss: 80.429, Residuals: 0.135\n",
      "Loss: 80.422, Residuals: 0.131\n",
      "Loss: 80.367, Residuals: 0.133\n",
      "Loss: 79.854, Residuals: 0.129\n",
      "Loss: 79.001, Residuals: 0.112\n",
      "Loss: 78.896, Residuals: 0.126\n",
      "Loss: 78.889, Residuals: 0.131\n",
      "Loss: 77.957, Residuals: 0.127\n",
      "Loss: 77.948, Residuals: 0.105\n",
      "Loss: 77.637, Residuals: 0.087\n",
      "Loss: 77.633, Residuals: 0.092\n",
      "Loss: 77.117, Residuals: 0.088\n",
      "Loss: 76.991, Residuals: 0.101\n",
      "Loss: 76.751, Residuals: 0.103\n",
      "Loss: 76.748, Residuals: 0.106\n",
      "Loss: 75.602, Residuals: 0.072\n",
      "Loss: 75.542, Residuals: 0.079\n",
      "Loss: 75.436, Residuals: 0.093\n",
      "Loss: 75.281, Residuals: 0.124\n",
      "Loss: 75.170, Residuals: 0.145\n",
      "Loss: 74.968, Residuals: 0.142\n",
      "Loss: 74.960, Residuals: 0.143\n",
      "Loss: 74.631, Residuals: 0.139\n",
      "Loss: 74.624, Residuals: 0.137\n",
      "Loss: 74.329, Residuals: 0.133\n",
      "Loss: 74.327, Residuals: 0.133\n",
      "Loss: 74.232, Residuals: 0.136\n",
      "Loss: 74.056, Residuals: 0.141\n",
      "Loss: 74.055, Residuals: 0.144\n",
      "Loss: 74.042, Residuals: 0.146\n",
      "Loss: 73.537, Residuals: 0.134\n",
      "Loss: 73.529, Residuals: 0.135\n",
      "Loss: 73.514, Residuals: 0.135\n",
      "Loss: 72.938, Residuals: 0.130\n",
      "Loss: 72.936, Residuals: 0.133\n",
      "Loss: 72.920, Residuals: 0.136\n",
      "Loss: 72.780, Residuals: 0.131\n",
      "Loss: 72.776, Residuals: 0.132\n",
      "Loss: 72.193, Residuals: 0.117\n",
      "Loss: 72.191, Residuals: 0.121\n",
      "Loss: 72.177, Residuals: 0.121\n",
      "Loss: 72.051, Residuals: 0.121\n",
      "Loss: 72.049, Residuals: 0.121\n",
      "Loss: 71.733, Residuals: 0.117\n",
      "Loss: 71.731, Residuals: 0.120\n",
      "Loss: 71.622, Residuals: 0.120\n",
      "Loss: 71.621, Residuals: 0.120\n",
      "Loss: 71.504, Residuals: 0.122\n",
      "Loss: 71.476, Residuals: 0.122\n",
      "Loss: 70.539, Residuals: 0.112\n",
      "Loss: 70.527, Residuals: 0.120\n",
      "Loss: 70.521, Residuals: 0.116\n",
      "Loss: 70.516, Residuals: 0.106\n",
      "Loss: 70.311, Residuals: 0.103\n",
      "Loss: 70.305, Residuals: 0.107\n",
      "Loss: 70.048, Residuals: 0.104\n",
      "Loss: 70.047, Residuals: 0.105\n",
      "Loss: 69.837, Residuals: 0.091\n",
      "Loss: 69.793, Residuals: 0.105\n",
      "Loss: 69.785, Residuals: 0.107\n",
      "Loss: 69.723, Residuals: 0.108\n",
      "Loss: 69.723, Residuals: 0.107\n",
      "Loss: 69.299, Residuals: 0.102\n",
      "Loss: 69.298, Residuals: 0.103\n",
      "Evidence -549.655\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 2.66e-02\n",
      "Loss: 107.354, Residuals: 0.135\n",
      "Loss: 106.997, Residuals: 0.131\n",
      "Loss: 106.504, Residuals: 0.103\n",
      "Loss: 106.494, Residuals: 0.105\n",
      "Loss: 105.014, Residuals: 0.077\n",
      "Loss: 104.531, Residuals: 0.094\n",
      "Loss: 104.292, Residuals: 0.100\n",
      "Loss: 103.836, Residuals: 0.106\n",
      "Loss: 103.772, Residuals: 0.076\n",
      "Loss: 103.180, Residuals: 0.091\n",
      "Loss: 103.179, Residuals: 0.089\n",
      "Evidence -194.948\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 4.97e-02\n",
      "Loss: 134.621, Residuals: 0.096\n",
      "Loss: 134.418, Residuals: 0.101\n",
      "Loss: 134.041, Residuals: 0.084\n",
      "Loss: 133.382, Residuals: 0.049\n",
      "Loss: 133.375, Residuals: 0.053\n",
      "Loss: 132.180, Residuals: 0.062\n",
      "Loss: 130.404, Residuals: 0.054\n",
      "Loss: 130.397, Residuals: 0.049\n",
      "Loss: 130.383, Residuals: 0.047\n",
      "Loss: 130.364, Residuals: 0.041\n",
      "Loss: 129.615, Residuals: 0.060\n",
      "Loss: 129.611, Residuals: 0.061\n",
      "Loss: 127.370, Residuals: 0.169\n",
      "Loss: 127.277, Residuals: 0.167\n",
      "Loss: 127.248, Residuals: 0.142\n",
      "Loss: 127.196, Residuals: 0.136\n",
      "Loss: 127.166, Residuals: 0.119\n",
      "Loss: 126.013, Residuals: 0.131\n",
      "Loss: 126.007, Residuals: 0.137\n",
      "Loss: 125.959, Residuals: 0.135\n",
      "Loss: 124.363, Residuals: 0.158\n",
      "Loss: 124.330, Residuals: 0.173\n",
      "Loss: 124.322, Residuals: 0.175\n",
      "Loss: 124.306, Residuals: 0.177\n",
      "Loss: 124.279, Residuals: 0.181\n",
      "Loss: 124.228, Residuals: 0.185\n",
      "Loss: 124.140, Residuals: 0.187\n",
      "Loss: 124.139, Residuals: 0.185\n",
      "Loss: 123.954, Residuals: 0.186\n",
      "Loss: 123.952, Residuals: 0.185\n",
      "Loss: 123.648, Residuals: 0.195\n",
      "Loss: 123.647, Residuals: 0.194\n",
      "Loss: 123.493, Residuals: 0.200\n",
      "Loss: 123.457, Residuals: 0.204\n",
      "Loss: 123.456, Residuals: 0.203\n",
      "Loss: 123.155, Residuals: 0.215\n",
      "Loss: 123.154, Residuals: 0.213\n",
      "Loss: 123.151, Residuals: 0.214\n",
      "Loss: 123.123, Residuals: 0.215\n",
      "Loss: 123.073, Residuals: 0.216\n",
      "Loss: 123.070, Residuals: 0.212\n",
      "Loss: 122.978, Residuals: 0.218\n",
      "Loss: 122.978, Residuals: 0.217\n",
      "Loss: 122.943, Residuals: 0.219\n",
      "Loss: 122.941, Residuals: 0.221\n",
      "Loss: 122.859, Residuals: 0.226\n",
      "Loss: 122.859, Residuals: 0.225\n",
      "Loss: 122.825, Residuals: 0.227\n",
      "Loss: 122.825, Residuals: 0.226\n",
      "Loss: 122.822, Residuals: 0.224\n",
      "Loss: 122.817, Residuals: 0.222\n",
      "Loss: 122.817, Residuals: 0.222\n",
      "Evidence -170.164\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.45e-01\n",
      "Loss: 142.113, Residuals: 0.193\n",
      "Loss: 142.034, Residuals: 0.203\n",
      "Loss: 141.335, Residuals: 0.175\n",
      "Loss: 140.305, Residuals: 0.115\n",
      "Loss: 140.297, Residuals: 0.118\n",
      "Loss: 139.026, Residuals: 0.162\n",
      "Loss: 138.956, Residuals: 0.152\n",
      "Loss: 136.482, Residuals: 0.214\n",
      "Loss: 136.475, Residuals: 0.210\n",
      "Loss: 136.464, Residuals: 0.212\n",
      "Loss: 136.452, Residuals: 0.217\n",
      "Loss: 135.960, Residuals: 0.233\n",
      "Loss: 135.281, Residuals: 0.276\n",
      "Loss: 135.274, Residuals: 0.264\n",
      "Loss: 135.260, Residuals: 0.264\n",
      "Loss: 135.234, Residuals: 0.262\n",
      "Loss: 134.987, Residuals: 0.272\n",
      "Loss: 134.937, Residuals: 0.276\n",
      "Loss: 134.514, Residuals: 0.289\n",
      "Loss: 134.513, Residuals: 0.287\n",
      "Loss: 134.499, Residuals: 0.286\n",
      "Loss: 134.475, Residuals: 0.285\n",
      "Loss: 134.434, Residuals: 0.281\n",
      "Loss: 134.433, Residuals: 0.277\n",
      "Loss: 134.177, Residuals: 0.294\n",
      "Loss: 134.176, Residuals: 0.296\n",
      "Loss: 134.044, Residuals: 0.312\n",
      "Loss: 134.038, Residuals: 0.312\n",
      "Loss: 134.028, Residuals: 0.314\n",
      "Loss: 134.026, Residuals: 0.312\n",
      "Loss: 133.952, Residuals: 0.321\n",
      "Loss: 133.952, Residuals: 0.321\n",
      "Evidence -148.343\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.21e-01\n",
      "Loss: 144.740, Residuals: 0.291\n",
      "Loss: 144.337, Residuals: 0.271\n",
      "Loss: 143.693, Residuals: 0.263\n",
      "Loss: 142.839, Residuals: 0.240\n",
      "Loss: 142.818, Residuals: 0.264\n",
      "Loss: 142.009, Residuals: 0.285\n",
      "Loss: 140.730, Residuals: 0.345\n",
      "Loss: 140.627, Residuals: 0.359\n",
      "Loss: 140.430, Residuals: 0.368\n",
      "Loss: 140.081, Residuals: 0.386\n",
      "Loss: 140.032, Residuals: 0.378\n",
      "Loss: 140.015, Residuals: 0.385\n",
      "Loss: 139.852, Residuals: 0.398\n",
      "Loss: 139.595, Residuals: 0.423\n",
      "Loss: 139.594, Residuals: 0.422\n",
      "Loss: 139.586, Residuals: 0.425\n",
      "Loss: 139.571, Residuals: 0.427\n",
      "Loss: 139.557, Residuals: 0.432\n",
      "Loss: 139.535, Residuals: 0.434\n",
      "Loss: 139.535, Residuals: 0.431\n",
      "Loss: 139.472, Residuals: 0.439\n",
      "Loss: 139.465, Residuals: 0.440\n",
      "Loss: 139.463, Residuals: 0.438\n",
      "Loss: 139.411, Residuals: 0.447\n",
      "Loss: 139.411, Residuals: 0.447\n",
      "Loss: 139.379, Residuals: 0.453\n",
      "Loss: 139.374, Residuals: 0.456\n",
      "Loss: 139.373, Residuals: 0.453\n",
      "Loss: 139.364, Residuals: 0.455\n",
      "Loss: 139.363, Residuals: 0.453\n",
      "Loss: 139.342, Residuals: 0.459\n",
      "Loss: 139.341, Residuals: 0.460\n",
      "Loss: 139.339, Residuals: 0.460\n",
      "Loss: 139.339, Residuals: 0.460\n",
      "Evidence -131.026\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.59e-01\n",
      "Loss: 146.974, Residuals: 0.446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 146.754, Residuals: 0.434\n",
      "Loss: 146.390, Residuals: 0.431\n",
      "Loss: 145.805, Residuals: 0.441\n",
      "Loss: 144.888, Residuals: 0.508\n",
      "Loss: 144.672, Residuals: 0.480\n",
      "Loss: 144.623, Residuals: 0.475\n",
      "Loss: 144.200, Residuals: 0.504\n",
      "Loss: 144.191, Residuals: 0.503\n",
      "Loss: 144.107, Residuals: 0.506\n",
      "Loss: 144.021, Residuals: 0.499\n",
      "Loss: 144.012, Residuals: 0.491\n",
      "Loss: 143.665, Residuals: 0.518\n",
      "Loss: 143.659, Residuals: 0.510\n",
      "Loss: 143.419, Residuals: 0.530\n",
      "Loss: 143.394, Residuals: 0.514\n",
      "Loss: 143.346, Residuals: 0.511\n",
      "Loss: 143.260, Residuals: 0.503\n",
      "Loss: 143.105, Residuals: 0.490\n",
      "Loss: 143.096, Residuals: 0.476\n",
      "Loss: 142.747, Residuals: 0.493\n",
      "Loss: 142.721, Residuals: 0.477\n",
      "Loss: 142.486, Residuals: 0.468\n",
      "Loss: 142.476, Residuals: 0.453\n",
      "Loss: 142.088, Residuals: 0.473\n",
      "Loss: 141.950, Residuals: 0.447\n",
      "Loss: 141.747, Residuals: 0.420\n",
      "Loss: 141.395, Residuals: 0.400\n",
      "Loss: 141.382, Residuals: 0.381\n",
      "Loss: 140.890, Residuals: 0.413\n",
      "Loss: 140.758, Residuals: 0.419\n",
      "Loss: 140.516, Residuals: 0.427\n",
      "Loss: 140.364, Residuals: 0.386\n",
      "Loss: 140.100, Residuals: 0.408\n",
      "Loss: 140.075, Residuals: 0.429\n",
      "Loss: 139.854, Residuals: 0.439\n",
      "Loss: 139.602, Residuals: 0.442\n",
      "Loss: 139.571, Residuals: 0.454\n",
      "Loss: 139.533, Residuals: 0.445\n",
      "Loss: 139.501, Residuals: 0.438\n",
      "Loss: 139.497, Residuals: 0.435\n",
      "Loss: 139.460, Residuals: 0.441\n",
      "Loss: 139.413, Residuals: 0.459\n",
      "Loss: 139.409, Residuals: 0.462\n",
      "Loss: 139.404, Residuals: 0.456\n",
      "Loss: 139.401, Residuals: 0.453\n",
      "Loss: 139.396, Residuals: 0.454\n",
      "Loss: 139.396, Residuals: 0.453\n",
      "Loss: 139.393, Residuals: 0.454\n",
      "Loss: 139.390, Residuals: 0.456\n",
      "Loss: 139.390, Residuals: 0.456\n",
      "Loss: 139.390, Residuals: 0.454\n",
      "Loss: 139.390, Residuals: 0.454\n",
      "Loss: 139.390, Residuals: 0.454\n",
      "Loss: 139.390, Residuals: 0.454\n",
      "Evidence -118.743\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.19e+00\n",
      "Loss: 144.287, Residuals: 0.381\n",
      "Loss: 143.773, Residuals: 0.327\n",
      "Loss: 142.828, Residuals: 0.334\n",
      "Loss: 141.376, Residuals: 0.373\n",
      "Loss: 141.231, Residuals: 0.397\n",
      "Loss: 140.968, Residuals: 0.409\n",
      "Loss: 140.567, Residuals: 0.431\n",
      "Loss: 140.484, Residuals: 0.417\n",
      "Loss: 140.347, Residuals: 0.429\n",
      "Loss: 140.257, Residuals: 0.461\n",
      "Loss: 140.245, Residuals: 0.467\n",
      "Loss: 140.242, Residuals: 0.463\n",
      "Loss: 140.238, Residuals: 0.461\n",
      "Loss: 140.233, Residuals: 0.460\n",
      "Loss: 140.233, Residuals: 0.461\n",
      "Loss: 140.232, Residuals: 0.461\n",
      "Loss: 140.232, Residuals: 0.461\n",
      "Loss: 140.232, Residuals: 0.461\n",
      "Loss: 140.231, Residuals: 0.461\n",
      "Loss: 140.231, Residuals: 0.460\n",
      "Loss: 140.231, Residuals: 0.460\n",
      "Evidence -103.587\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.60e+00\n",
      "Loss: 146.053, Residuals: 0.403\n",
      "Loss: 145.562, Residuals: 0.414\n",
      "Loss: 145.211, Residuals: 0.420\n",
      "Loss: 145.156, Residuals: 0.431\n",
      "Loss: 145.072, Residuals: 0.457\n",
      "Loss: 145.006, Residuals: 0.492\n",
      "Loss: 145.000, Residuals: 0.492\n",
      "Loss: 144.992, Residuals: 0.488\n",
      "Loss: 144.988, Residuals: 0.478\n",
      "Loss: 144.988, Residuals: 0.478\n",
      "Loss: 144.988, Residuals: 0.478\n",
      "Loss: 144.987, Residuals: 0.479\n",
      "Loss: 144.987, Residuals: 0.482\n",
      "Loss: 144.987, Residuals: 0.483\n",
      "Loss: 144.986, Residuals: 0.482\n",
      "Loss: 144.986, Residuals: 0.482\n",
      "Evidence -97.710\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.78e+00\n",
      "Loss: 148.250, Residuals: 0.450\n",
      "Loss: 148.048, Residuals: 0.469\n",
      "Loss: 147.925, Residuals: 0.505\n",
      "Loss: 147.914, Residuals: 0.493\n",
      "Loss: 147.896, Residuals: 0.496\n",
      "Loss: 147.872, Residuals: 0.499\n",
      "Loss: 147.871, Residuals: 0.501\n",
      "Loss: 147.869, Residuals: 0.501\n",
      "Loss: 147.865, Residuals: 0.499\n",
      "Loss: 147.863, Residuals: 0.497\n",
      "Loss: 147.862, Residuals: 0.498\n",
      "Loss: 147.862, Residuals: 0.497\n",
      "Loss: 147.862, Residuals: 0.497\n",
      "Loss: 147.861, Residuals: 0.496\n",
      "Loss: 147.861, Residuals: 0.496\n",
      "Loss: 147.861, Residuals: 0.496\n",
      "Loss: 147.861, Residuals: 0.496\n",
      "Loss: 147.861, Residuals: 0.496\n",
      "Loss: 147.860, Residuals: 0.495\n",
      "Loss: 147.860, Residuals: 0.495\n",
      "Loss: 147.860, Residuals: 0.495\n",
      "Loss: 147.860, Residuals: 0.495\n",
      "Loss: 147.860, Residuals: 0.495\n",
      "Evidence -95.473\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.87e+00\n",
      "Loss: 149.416, Residuals: 0.461\n",
      "Loss: 149.311, Residuals: 0.519\n",
      "Loss: 149.249, Residuals: 0.498\n",
      "Loss: 149.232, Residuals: 0.502\n",
      "Loss: 149.212, Residuals: 0.495\n",
      "Loss: 149.208, Residuals: 0.495\n",
      "Loss: 149.203, Residuals: 0.493\n",
      "Loss: 149.202, Residuals: 0.494\n",
      "Loss: 149.200, Residuals: 0.493\n",
      "Loss: 149.198, Residuals: 0.490\n",
      "Loss: 149.198, Residuals: 0.490\n",
      "Loss: 149.198, Residuals: 0.490\n",
      "Loss: 149.197, Residuals: 0.490\n",
      "Loss: 149.197, Residuals: 0.490\n",
      "Loss: 149.197, Residuals: 0.490\n",
      "Loss: 149.197, Residuals: 0.489\n",
      "Loss: 149.197, Residuals: 0.489\n",
      "Evidence -94.516\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.90e+00\n",
      "Loss: 150.071, Residuals: 0.470\n",
      "Loss: 149.988, Residuals: 0.495\n",
      "Loss: 149.900, Residuals: 0.472\n",
      "Loss: 149.897, Residuals: 0.477\n",
      "Loss: 149.893, Residuals: 0.475\n",
      "Loss: 149.887, Residuals: 0.474\n",
      "Loss: 149.881, Residuals: 0.474\n",
      "Loss: 149.880, Residuals: 0.474\n",
      "Loss: 149.879, Residuals: 0.472\n",
      "Loss: 149.879, Residuals: 0.472\n",
      "Loss: 149.879, Residuals: 0.472\n",
      "Loss: 149.879, Residuals: 0.472\n",
      "Loss: 149.879, Residuals: 0.472\n",
      "Loss: 149.879, Residuals: 0.472\n",
      "Evidence -93.886\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.89e+00\n",
      "Loss: 150.538, Residuals: 0.468\n",
      "Loss: 150.495, Residuals: 0.471\n",
      "Loss: 150.443, Residuals: 0.463\n",
      "Loss: 150.437, Residuals: 0.463\n",
      "Loss: 150.430, Residuals: 0.460\n",
      "Loss: 150.428, Residuals: 0.458\n",
      "Loss: 150.427, Residuals: 0.456\n",
      "Loss: 150.426, Residuals: 0.455\n",
      "Loss: 150.426, Residuals: 0.455\n",
      "Loss: 150.426, Residuals: 0.455\n",
      "Evidence -93.400\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.88e+00\n",
      "Loss: 150.883, Residuals: 0.452\n",
      "Loss: 150.865, Residuals: 0.453\n",
      "Loss: 150.844, Residuals: 0.446\n",
      "Loss: 150.842, Residuals: 0.447\n",
      "Loss: 150.839, Residuals: 0.445\n",
      "Loss: 150.839, Residuals: 0.444\n",
      "Loss: 150.839, Residuals: 0.443\n",
      "Loss: 150.838, Residuals: 0.443\n",
      "Loss: 150.838, Residuals: 0.444\n",
      "Evidence -93.080\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.88e+00\n",
      "Loss: 151.130, Residuals: 0.443\n",
      "Loss: 151.120, Residuals: 0.440\n",
      "Loss: 151.114, Residuals: 0.437\n",
      "Loss: 151.113, Residuals: 0.439\n",
      "Loss: 151.112, Residuals: 0.438\n",
      "Loss: 151.112, Residuals: 0.438\n",
      "Loss: 151.112, Residuals: 0.438\n",
      "Loss: 151.112, Residuals: 0.437\n",
      "Loss: 151.111, Residuals: 0.437\n",
      "Loss: 151.111, Residuals: 0.437\n",
      "Loss: 151.111, Residuals: 0.437\n",
      "Evidence -92.855\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.87e+00\n",
      "Loss: 151.301, Residuals: 0.436\n",
      "Loss: 151.296, Residuals: 0.435\n",
      "Loss: 151.296, Residuals: 0.435\n",
      "Loss: 151.295, Residuals: 0.434\n",
      "Loss: 151.295, Residuals: 0.434\n",
      "Loss: 151.294, Residuals: 0.434\n",
      "Loss: 151.293, Residuals: 0.434\n",
      "Loss: 151.293, Residuals: 0.434\n",
      "Loss: 151.293, Residuals: 0.433\n",
      "Loss: 151.293, Residuals: 0.433\n",
      "Loss: 151.292, Residuals: 0.433\n",
      "Evidence -92.685\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.88e+00\n",
      "Loss: 151.425, Residuals: 0.433\n",
      "Loss: 151.422, Residuals: 0.434\n",
      "Loss: 151.420, Residuals: 0.430\n",
      "Loss: 151.420, Residuals: 0.432\n",
      "Loss: 151.420, Residuals: 0.431\n",
      "Loss: 151.420, Residuals: 0.431\n",
      "Loss: 151.420, Residuals: 0.431\n",
      "Loss: 151.419, Residuals: 0.431\n",
      "Loss: 151.419, Residuals: 0.431\n",
      "Loss: 151.419, Residuals: 0.431\n",
      "Evidence -92.555\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.88e+00\n",
      "Loss: 151.519, Residuals: 0.432\n",
      "Loss: 151.517, Residuals: 0.432\n",
      "Loss: 151.517, Residuals: 0.431\n",
      "Loss: 151.517, Residuals: 0.430\n",
      "Loss: 151.516, Residuals: 0.430\n",
      "Loss: 151.516, Residuals: 0.430\n",
      "Loss: 151.516, Residuals: 0.430\n",
      "Loss: 151.516, Residuals: 0.430\n",
      "Loss: 151.516, Residuals: 0.430\n",
      "Loss: 151.516, Residuals: 0.430\n",
      "Evidence -92.452\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.88e+00\n",
      "Loss: 151.593, Residuals: 0.432\n",
      "Loss: 151.591, Residuals: 0.431\n",
      "Loss: 151.591, Residuals: 0.428\n",
      "Loss: 151.591, Residuals: 0.429\n",
      "Loss: 151.590, Residuals: 0.429\n",
      "Loss: 151.590, Residuals: 0.429\n",
      "Evidence -92.368\n",
      "Pass count  1\n",
      "Total samples: 38, Updated regularization: 1.00e-05\n",
      "Loss: 408.290, Residuals: -1.904\n",
      "Loss: 302.186, Residuals: -1.461\n",
      "Loss: 277.899, Residuals: -0.910\n",
      "Loss: 251.375, Residuals: 0.123\n",
      "Loss: 247.627, Residuals: 0.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 189.207, Residuals: 0.051\n",
      "Loss: 174.417, Residuals: -0.034\n",
      "Loss: 170.982, Residuals: 0.275\n",
      "Loss: 143.163, Residuals: 0.085\n",
      "Loss: 137.661, Residuals: 0.133\n",
      "Loss: 127.317, Residuals: 0.217\n",
      "Loss: 121.879, Residuals: 0.261\n",
      "Loss: 113.380, Residuals: 0.140\n",
      "Loss: 112.542, Residuals: 0.291\n",
      "Loss: 105.964, Residuals: 0.306\n",
      "Loss: 105.544, Residuals: 0.276\n",
      "Loss: 104.774, Residuals: 0.333\n",
      "Loss: 98.766, Residuals: 0.263\n",
      "Loss: 98.721, Residuals: 0.237\n",
      "Loss: 98.308, Residuals: 0.276\n",
      "Loss: 94.775, Residuals: 0.213\n",
      "Loss: 94.011, Residuals: 0.327\n",
      "Loss: 93.965, Residuals: 0.358\n",
      "Loss: 92.215, Residuals: 0.317\n",
      "Loss: 92.196, Residuals: 0.276\n",
      "Loss: 89.512, Residuals: 0.234\n",
      "Loss: 89.484, Residuals: 0.213\n",
      "Loss: 89.237, Residuals: 0.221\n",
      "Loss: 88.772, Residuals: 0.222\n",
      "Loss: 87.887, Residuals: 0.204\n",
      "Loss: 87.450, Residuals: 0.210\n",
      "Loss: 86.747, Residuals: 0.220\n",
      "Loss: 85.455, Residuals: 0.200\n",
      "Loss: 85.452, Residuals: 0.193\n",
      "Loss: 85.423, Residuals: 0.200\n",
      "Loss: 84.279, Residuals: 0.195\n",
      "Loss: 84.267, Residuals: 0.183\n",
      "Loss: 82.438, Residuals: 0.176\n",
      "Loss: 82.433, Residuals: 0.166\n",
      "Loss: 82.394, Residuals: 0.165\n",
      "Loss: 82.085, Residuals: 0.147\n",
      "Loss: 82.061, Residuals: 0.174\n",
      "Loss: 81.134, Residuals: 0.163\n",
      "Loss: 81.130, Residuals: 0.160\n",
      "Loss: 80.562, Residuals: 0.167\n",
      "Loss: 80.554, Residuals: 0.167\n",
      "Loss: 79.391, Residuals: 0.153\n",
      "Loss: 79.385, Residuals: 0.147\n",
      "Loss: 79.329, Residuals: 0.141\n",
      "Loss: 79.226, Residuals: 0.141\n",
      "Loss: 78.279, Residuals: 0.131\n",
      "Loss: 78.268, Residuals: 0.120\n",
      "Loss: 77.877, Residuals: 0.116\n",
      "Loss: 77.863, Residuals: 0.129\n",
      "Loss: 77.837, Residuals: 0.128\n",
      "Loss: 77.618, Residuals: 0.135\n",
      "Loss: 77.540, Residuals: 0.134\n",
      "Loss: 77.504, Residuals: 0.157\n",
      "Loss: 77.460, Residuals: 0.149\n",
      "Loss: 75.911, Residuals: 0.138\n",
      "Loss: 75.880, Residuals: 0.114\n",
      "Loss: 75.826, Residuals: 0.110\n",
      "Loss: 75.315, Residuals: 0.112\n",
      "Loss: 75.300, Residuals: 0.104\n",
      "Loss: 75.183, Residuals: 0.115\n",
      "Loss: 74.387, Residuals: 0.176\n",
      "Loss: 74.339, Residuals: 0.145\n",
      "Loss: 74.318, Residuals: 0.145\n",
      "Loss: 74.147, Residuals: 0.155\n",
      "Loss: 73.819, Residuals: 0.153\n",
      "Loss: 73.206, Residuals: 0.147\n",
      "Loss: 73.200, Residuals: 0.141\n",
      "Loss: 73.135, Residuals: 0.142\n",
      "Loss: 72.519, Residuals: 0.143\n",
      "Loss: 72.483, Residuals: 0.171\n",
      "Loss: 72.199, Residuals: 0.163\n",
      "Loss: 72.196, Residuals: 0.158\n",
      "Loss: 70.550, Residuals: 0.115\n",
      "Loss: 70.523, Residuals: 0.138\n",
      "Loss: 70.493, Residuals: 0.139\n",
      "Loss: 70.469, Residuals: 0.150\n",
      "Loss: 70.263, Residuals: 0.149\n",
      "Loss: 70.225, Residuals: 0.148\n",
      "Loss: 69.914, Residuals: 0.160\n",
      "Loss: 69.913, Residuals: 0.157\n",
      "Loss: 69.902, Residuals: 0.160\n",
      "Loss: 69.454, Residuals: 0.146\n",
      "Loss: 69.427, Residuals: 0.163\n",
      "Loss: 69.180, Residuals: 0.162\n",
      "Loss: 69.180, Residuals: 0.161\n",
      "Loss: 68.384, Residuals: 0.142\n",
      "Loss: 68.381, Residuals: 0.137\n",
      "Loss: 67.936, Residuals: 0.127\n",
      "Loss: 67.936, Residuals: 0.128\n",
      "Loss: 67.419, Residuals: 0.126\n",
      "Loss: 67.419, Residuals: 0.123\n",
      "Evidence -531.010\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 2.57e-02\n",
      "Loss: 106.660, Residuals: 0.115\n",
      "Loss: 106.240, Residuals: 0.109\n",
      "Loss: 105.883, Residuals: 0.163\n",
      "Loss: 105.336, Residuals: 0.203\n",
      "Loss: 105.275, Residuals: 0.217\n",
      "Loss: 104.692, Residuals: 0.213\n",
      "Loss: 104.260, Residuals: 0.255\n",
      "Loss: 104.105, Residuals: 0.233\n",
      "Loss: 104.104, Residuals: 0.235\n",
      "Evidence -176.882\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 4.80e-02\n",
      "Loss: 134.096, Residuals: 0.228\n",
      "Loss: 133.947, Residuals: 0.221\n",
      "Loss: 133.676, Residuals: 0.233\n",
      "Loss: 133.247, Residuals: 0.255\n",
      "Loss: 129.903, Residuals: 0.277\n",
      "Loss: 129.887, Residuals: 0.267\n",
      "Loss: 129.868, Residuals: 0.272\n",
      "Loss: 129.835, Residuals: 0.272\n",
      "Loss: 129.574, Residuals: 0.266\n",
      "Loss: 129.076, Residuals: 0.272\n",
      "Loss: 128.200, Residuals: 0.287\n",
      "Loss: 128.192, Residuals: 0.283\n",
      "Loss: 127.889, Residuals: 0.289\n",
      "Loss: 127.347, Residuals: 0.306\n",
      "Loss: 127.328, Residuals: 0.312\n",
      "Loss: 126.667, Residuals: 0.338\n",
      "Loss: 126.664, Residuals: 0.334\n",
      "Evidence -150.564\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 6.16e-02\n",
      "Loss: 142.676, Residuals: 0.326\n",
      "Loss: 142.615, Residuals: 0.327\n",
      "Loss: 142.089, Residuals: 0.318\n",
      "Loss: 141.285, Residuals: 0.296\n",
      "Loss: 141.281, Residuals: 0.291\n",
      "Evidence -137.334\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 6.50e-02\n",
      "Loss: 147.967, Residuals: 0.283\n",
      "Loss: 147.817, Residuals: 0.284\n",
      "Loss: 146.640, Residuals: 0.285\n",
      "Loss: 146.630, Residuals: 0.278\n",
      "Loss: 146.542, Residuals: 0.276\n",
      "Loss: 146.409, Residuals: 0.270\n",
      "Loss: 146.155, Residuals: 0.273\n",
      "Loss: 145.673, Residuals: 0.283\n",
      "Loss: 144.955, Residuals: 0.302\n",
      "Loss: 144.954, Residuals: 0.299\n",
      "Evidence -132.111\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 6.88e-02\n",
      "Loss: 148.530, Residuals: 0.264\n",
      "Loss: 148.506, Residuals: 0.263\n",
      "Loss: 147.648, Residuals: 0.279\n",
      "Loss: 147.624, Residuals: 0.290\n",
      "Loss: 146.753, Residuals: 0.318\n",
      "Loss: 146.752, Residuals: 0.316\n",
      "Evidence -127.626\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 7.31e-02\n",
      "Loss: 149.370, Residuals: 0.286\n",
      "Loss: 149.269, Residuals: 0.299\n",
      "Loss: 149.082, Residuals: 0.297\n",
      "Loss: 148.761, Residuals: 0.297\n",
      "Loss: 148.760, Residuals: 0.295\n",
      "Evidence -124.404\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 7.44e-02\n",
      "Loss: 150.155, Residuals: 0.275\n",
      "Loss: 150.141, Residuals: 0.269\n",
      "Loss: 149.576, Residuals: 0.284\n",
      "Loss: 148.580, Residuals: 0.322\n",
      "Loss: 148.576, Residuals: 0.327\n",
      "Loss: 147.969, Residuals: 0.338\n",
      "Loss: 147.729, Residuals: 0.340\n",
      "Loss: 147.688, Residuals: 0.335\n",
      "Loss: 146.189, Residuals: 0.343\n",
      "Loss: 146.160, Residuals: 0.348\n",
      "Loss: 146.127, Residuals: 0.336\n",
      "Loss: 144.841, Residuals: 0.350\n",
      "Loss: 144.791, Residuals: 0.336\n",
      "Loss: 144.696, Residuals: 0.344\n",
      "Loss: 144.520, Residuals: 0.353\n",
      "Loss: 144.458, Residuals: 0.350\n",
      "Loss: 144.382, Residuals: 0.355\n",
      "Loss: 144.361, Residuals: 0.357\n",
      "Loss: 142.808, Residuals: 0.373\n",
      "Loss: 142.744, Residuals: 0.372\n",
      "Loss: 142.724, Residuals: 0.347\n",
      "Loss: 142.551, Residuals: 0.352\n",
      "Loss: 142.261, Residuals: 0.350\n",
      "Loss: 142.260, Residuals: 0.348\n",
      "Evidence -119.565\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.54e-01\n",
      "Loss: 147.249, Residuals: 0.294\n",
      "Loss: 147.215, Residuals: 0.278\n",
      "Loss: 146.013, Residuals: 0.295\n",
      "Loss: 143.963, Residuals: 0.349\n",
      "Loss: 143.945, Residuals: 0.339\n",
      "Loss: 143.913, Residuals: 0.346\n",
      "Loss: 143.856, Residuals: 0.359\n",
      "Loss: 141.908, Residuals: 0.419\n",
      "Loss: 141.892, Residuals: 0.409\n",
      "Loss: 141.865, Residuals: 0.403\n",
      "Loss: 141.817, Residuals: 0.409\n",
      "Loss: 141.742, Residuals: 0.416\n",
      "Loss: 141.599, Residuals: 0.405\n",
      "Loss: 140.434, Residuals: 0.434\n",
      "Loss: 140.407, Residuals: 0.422\n",
      "Loss: 140.361, Residuals: 0.423\n",
      "Loss: 140.284, Residuals: 0.413\n",
      "Loss: 140.175, Residuals: 0.386\n",
      "Loss: 140.161, Residuals: 0.387\n",
      "Loss: 139.687, Residuals: 0.402\n",
      "Loss: 139.683, Residuals: 0.399\n",
      "Loss: 139.676, Residuals: 0.400\n",
      "Loss: 139.664, Residuals: 0.399\n",
      "Loss: 139.640, Residuals: 0.398\n",
      "Loss: 139.447, Residuals: 0.412\n",
      "Loss: 139.445, Residuals: 0.413\n",
      "Loss: 139.426, Residuals: 0.408\n",
      "Loss: 139.399, Residuals: 0.403\n",
      "Loss: 139.397, Residuals: 0.402\n",
      "Loss: 139.327, Residuals: 0.407\n",
      "Loss: 139.324, Residuals: 0.408\n",
      "Loss: 139.303, Residuals: 0.411\n",
      "Loss: 139.294, Residuals: 0.409\n",
      "Loss: 139.293, Residuals: 0.412\n",
      "Loss: 139.280, Residuals: 0.413\n",
      "Loss: 139.280, Residuals: 0.414\n",
      "Loss: 139.274, Residuals: 0.414\n",
      "Loss: 139.264, Residuals: 0.414\n",
      "Loss: 139.264, Residuals: 0.413\n",
      "Loss: 139.258, Residuals: 0.413\n",
      "Loss: 139.257, Residuals: 0.414\n",
      "Loss: 139.256, Residuals: 0.413\n",
      "Loss: 139.255, Residuals: 0.413\n",
      "Loss: 139.255, Residuals: 0.413\n",
      "Loss: 139.253, Residuals: 0.413\n",
      "Loss: 139.253, Residuals: 0.413\n",
      "Loss: 139.252, Residuals: 0.413\n",
      "Loss: 139.251, Residuals: 0.413\n",
      "Loss: 139.251, Residuals: 0.413\n",
      "Loss: 139.251, Residuals: 0.413\n",
      "Loss: 139.251, Residuals: 0.412\n",
      "Loss: 139.250, Residuals: 0.412\n",
      "Loss: 139.250, Residuals: 0.413\n",
      "Loss: 139.250, Residuals: 0.412\n",
      "Loss: 139.250, Residuals: 0.412\n",
      "Loss: 139.250, Residuals: 0.413\n",
      "Evidence -112.714\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 2.93e-01\n",
      "Loss: 145.645, Residuals: 0.369\n",
      "Loss: 145.400, Residuals: 0.383\n",
      "Loss: 144.947, Residuals: 0.397\n",
      "Loss: 144.186, Residuals: 0.430\n",
      "Loss: 144.176, Residuals: 0.426\n",
      "Loss: 143.804, Residuals: 0.447\n",
      "Loss: 143.339, Residuals: 0.487\n",
      "Loss: 143.307, Residuals: 0.477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 143.261, Residuals: 0.471\n",
      "Loss: 142.881, Residuals: 0.478\n",
      "Loss: 142.876, Residuals: 0.485\n",
      "Loss: 142.711, Residuals: 0.487\n",
      "Loss: 142.503, Residuals: 0.499\n",
      "Loss: 142.501, Residuals: 0.504\n",
      "Loss: 142.496, Residuals: 0.501\n",
      "Loss: 142.489, Residuals: 0.496\n",
      "Loss: 142.477, Residuals: 0.491\n",
      "Loss: 142.467, Residuals: 0.488\n",
      "Loss: 142.466, Residuals: 0.488\n",
      "Loss: 142.457, Residuals: 0.489\n",
      "Loss: 142.445, Residuals: 0.492\n",
      "Loss: 142.444, Residuals: 0.492\n",
      "Loss: 142.443, Residuals: 0.492\n",
      "Loss: 142.442, Residuals: 0.492\n",
      "Loss: 142.439, Residuals: 0.493\n",
      "Loss: 142.439, Residuals: 0.493\n",
      "Loss: 142.439, Residuals: 0.492\n",
      "Loss: 142.438, Residuals: 0.492\n",
      "Loss: 142.437, Residuals: 0.492\n",
      "Loss: 142.437, Residuals: 0.492\n",
      "Evidence -101.889\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.69e-01\n",
      "Loss: 146.514, Residuals: 0.432\n",
      "Loss: 146.290, Residuals: 0.448\n",
      "Loss: 145.897, Residuals: 0.476\n",
      "Loss: 145.398, Residuals: 0.534\n",
      "Loss: 145.388, Residuals: 0.533\n",
      "Loss: 145.370, Residuals: 0.530\n",
      "Loss: 145.338, Residuals: 0.524\n",
      "Loss: 145.059, Residuals: 0.527\n",
      "Loss: 145.055, Residuals: 0.541\n",
      "Loss: 144.884, Residuals: 0.544\n",
      "Loss: 144.863, Residuals: 0.520\n",
      "Loss: 144.822, Residuals: 0.523\n",
      "Loss: 144.752, Residuals: 0.530\n",
      "Loss: 144.749, Residuals: 0.532\n",
      "Loss: 144.729, Residuals: 0.534\n",
      "Loss: 144.693, Residuals: 0.537\n",
      "Loss: 144.649, Residuals: 0.548\n",
      "Loss: 144.647, Residuals: 0.546\n",
      "Loss: 144.646, Residuals: 0.543\n",
      "Loss: 144.645, Residuals: 0.544\n",
      "Loss: 144.644, Residuals: 0.544\n",
      "Loss: 144.640, Residuals: 0.544\n",
      "Loss: 144.640, Residuals: 0.544\n",
      "Loss: 144.639, Residuals: 0.544\n",
      "Loss: 144.638, Residuals: 0.544\n",
      "Loss: 144.638, Residuals: 0.543\n",
      "Loss: 144.638, Residuals: 0.543\n",
      "Loss: 144.638, Residuals: 0.544\n",
      "Loss: 144.637, Residuals: 0.543\n",
      "Evidence -96.680\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 4.27e-01\n",
      "Loss: 147.847, Residuals: 0.512\n",
      "Loss: 147.658, Residuals: 0.515\n",
      "Loss: 147.368, Residuals: 0.522\n",
      "Loss: 147.004, Residuals: 0.578\n",
      "Loss: 146.967, Residuals: 0.572\n",
      "Loss: 146.903, Residuals: 0.570\n",
      "Loss: 146.813, Residuals: 0.569\n",
      "Loss: 146.811, Residuals: 0.571\n",
      "Loss: 146.808, Residuals: 0.570\n",
      "Loss: 146.784, Residuals: 0.574\n",
      "Loss: 146.783, Residuals: 0.572\n",
      "Loss: 146.776, Residuals: 0.573\n",
      "Loss: 146.770, Residuals: 0.577\n",
      "Loss: 146.770, Residuals: 0.576\n",
      "Loss: 146.770, Residuals: 0.576\n",
      "Loss: 146.770, Residuals: 0.576\n",
      "Loss: 146.770, Residuals: 0.576\n",
      "Loss: 146.769, Residuals: 0.576\n",
      "Loss: 146.769, Residuals: 0.576\n",
      "Loss: 146.769, Residuals: 0.576\n",
      "Loss: 146.769, Residuals: 0.576\n",
      "Loss: 146.769, Residuals: 0.576\n",
      "Evidence -93.529\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 4.71e-01\n",
      "Loss: 148.728, Residuals: 0.542\n",
      "Loss: 148.609, Residuals: 0.543\n",
      "Loss: 148.437, Residuals: 0.558\n",
      "Loss: 148.257, Residuals: 0.590\n",
      "Loss: 148.231, Residuals: 0.588\n",
      "Loss: 148.194, Residuals: 0.581\n",
      "Loss: 148.147, Residuals: 0.580\n",
      "Loss: 148.146, Residuals: 0.579\n",
      "Loss: 148.145, Residuals: 0.578\n",
      "Loss: 148.142, Residuals: 0.578\n",
      "Loss: 148.138, Residuals: 0.579\n",
      "Loss: 148.136, Residuals: 0.577\n",
      "Loss: 148.133, Residuals: 0.577\n",
      "Loss: 148.129, Residuals: 0.578\n",
      "Loss: 148.129, Residuals: 0.577\n",
      "Loss: 148.129, Residuals: 0.577\n",
      "Loss: 148.128, Residuals: 0.578\n",
      "Loss: 148.127, Residuals: 0.578\n",
      "Loss: 148.127, Residuals: 0.578\n",
      "Loss: 148.127, Residuals: 0.578\n",
      "Loss: 148.127, Residuals: 0.579\n",
      "Loss: 148.127, Residuals: 0.579\n",
      "Loss: 148.127, Residuals: 0.579\n",
      "Loss: 148.126, Residuals: 0.580\n",
      "Loss: 148.126, Residuals: 0.580\n",
      "Evidence -91.732\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.12e-01\n",
      "Loss: 149.391, Residuals: 0.547\n",
      "Loss: 149.294, Residuals: 0.551\n",
      "Loss: 149.137, Residuals: 0.556\n",
      "Loss: 148.928, Residuals: 0.567\n",
      "Loss: 148.913, Residuals: 0.557\n",
      "Loss: 148.889, Residuals: 0.560\n",
      "Loss: 148.847, Residuals: 0.562\n",
      "Loss: 148.790, Residuals: 0.560\n",
      "Loss: 148.786, Residuals: 0.560\n",
      "Loss: 148.782, Residuals: 0.561\n",
      "Loss: 148.776, Residuals: 0.561\n",
      "Loss: 148.776, Residuals: 0.560\n",
      "Loss: 148.775, Residuals: 0.560\n",
      "Loss: 148.774, Residuals: 0.560\n",
      "Loss: 148.773, Residuals: 0.559\n",
      "Loss: 148.773, Residuals: 0.559\n",
      "Loss: 148.773, Residuals: 0.559\n",
      "Loss: 148.773, Residuals: 0.559\n",
      "Evidence -90.625\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.34e-01\n",
      "Loss: 149.831, Residuals: 0.528\n",
      "Loss: 149.741, Residuals: 0.530\n",
      "Loss: 149.587, Residuals: 0.533\n",
      "Loss: 149.418, Residuals: 0.530\n",
      "Loss: 149.228, Residuals: 0.533\n",
      "Loss: 149.227, Residuals: 0.534\n",
      "Evidence -89.548\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.44e-01\n",
      "Loss: 150.167, Residuals: 0.507\n",
      "Loss: 150.103, Residuals: 0.508\n",
      "Loss: 150.010, Residuals: 0.515\n",
      "Loss: 149.870, Residuals: 0.519\n",
      "Loss: 149.860, Residuals: 0.514\n",
      "Loss: 149.843, Residuals: 0.515\n",
      "Loss: 149.815, Residuals: 0.515\n",
      "Loss: 149.790, Residuals: 0.516\n",
      "Loss: 149.788, Residuals: 0.514\n",
      "Loss: 149.787, Residuals: 0.516\n",
      "Loss: 149.786, Residuals: 0.516\n",
      "Loss: 149.785, Residuals: 0.515\n",
      "Loss: 149.784, Residuals: 0.515\n",
      "Loss: 149.784, Residuals: 0.515\n",
      "Loss: 149.783, Residuals: 0.515\n",
      "Loss: 149.783, Residuals: 0.515\n",
      "Loss: 149.783, Residuals: 0.515\n",
      "Loss: 149.783, Residuals: 0.515\n",
      "Loss: 149.783, Residuals: 0.515\n",
      "Loss: 149.783, Residuals: 0.516\n",
      "Loss: 149.783, Residuals: 0.516\n",
      "Evidence -88.747\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.54e-01\n",
      "Loss: 150.511, Residuals: 0.499\n",
      "Loss: 150.472, Residuals: 0.502\n",
      "Loss: 150.424, Residuals: 0.508\n",
      "Loss: 150.369, Residuals: 0.509\n",
      "Loss: 150.366, Residuals: 0.502\n",
      "Loss: 150.361, Residuals: 0.502\n",
      "Loss: 150.353, Residuals: 0.503\n",
      "Loss: 150.353, Residuals: 0.502\n",
      "Loss: 150.349, Residuals: 0.502\n",
      "Loss: 150.347, Residuals: 0.502\n",
      "Loss: 150.346, Residuals: 0.501\n",
      "Loss: 150.346, Residuals: 0.503\n",
      "Loss: 150.346, Residuals: 0.503\n",
      "Loss: 150.346, Residuals: 0.502\n",
      "Loss: 150.346, Residuals: 0.502\n",
      "Evidence -88.150\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.60e-01\n",
      "Loss: 150.782, Residuals: 0.492\n",
      "Loss: 150.765, Residuals: 0.494\n",
      "Loss: 150.737, Residuals: 0.496\n",
      "Loss: 150.711, Residuals: 0.497\n",
      "Loss: 150.709, Residuals: 0.494\n",
      "Loss: 150.707, Residuals: 0.494\n",
      "Loss: 150.704, Residuals: 0.494\n",
      "Loss: 150.704, Residuals: 0.494\n",
      "Evidence -87.734\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.65e-01\n",
      "Loss: 150.996, Residuals: 0.487\n",
      "Loss: 150.987, Residuals: 0.489\n",
      "Loss: 150.972, Residuals: 0.490\n",
      "Loss: 150.959, Residuals: 0.491\n",
      "Loss: 150.958, Residuals: 0.489\n",
      "Loss: 150.957, Residuals: 0.489\n",
      "Loss: 150.955, Residuals: 0.490\n",
      "Evidence -87.461\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.68e-01\n",
      "Loss: 151.163, Residuals: 0.480\n",
      "Loss: 151.157, Residuals: 0.486\n",
      "Loss: 151.151, Residuals: 0.487\n",
      "Loss: 151.145, Residuals: 0.488\n",
      "Loss: 151.144, Residuals: 0.485\n",
      "Loss: 151.143, Residuals: 0.485\n",
      "Loss: 151.142, Residuals: 0.486\n",
      "Loss: 151.142, Residuals: 0.487\n",
      "Loss: 151.142, Residuals: 0.487\n",
      "Loss: 151.141, Residuals: 0.487\n",
      "Loss: 151.141, Residuals: 0.486\n",
      "Loss: 151.141, Residuals: 0.486\n",
      "Evidence -87.253\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.64e-01\n",
      "Loss: 151.301, Residuals: 0.478\n",
      "Loss: 151.298, Residuals: 0.482\n",
      "Loss: 151.293, Residuals: 0.485\n",
      "Loss: 151.290, Residuals: 0.484\n",
      "Loss: 151.290, Residuals: 0.482\n",
      "Loss: 151.290, Residuals: 0.484\n",
      "Loss: 151.289, Residuals: 0.483\n",
      "Loss: 151.288, Residuals: 0.483\n",
      "Loss: 151.288, Residuals: 0.483\n",
      "Loss: 151.288, Residuals: 0.483\n",
      "Loss: 151.288, Residuals: 0.483\n",
      "Loss: 151.288, Residuals: 0.483\n",
      "Evidence -87.091\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.60e-01\n",
      "Loss: 151.411, Residuals: 0.479\n",
      "Loss: 151.408, Residuals: 0.481\n",
      "Loss: 151.406, Residuals: 0.483\n",
      "Loss: 151.405, Residuals: 0.482\n",
      "Loss: 151.404, Residuals: 0.482\n",
      "Loss: 151.403, Residuals: 0.482\n",
      "Loss: 151.403, Residuals: 0.482\n",
      "Loss: 151.403, Residuals: 0.481\n",
      "Loss: 151.402, Residuals: 0.480\n",
      "Loss: 151.402, Residuals: 0.480\n",
      "Loss: 151.402, Residuals: 0.480\n",
      "Loss: 151.402, Residuals: 0.480\n",
      "Loss: 151.402, Residuals: 0.480\n",
      "Loss: 151.402, Residuals: 0.479\n",
      "Loss: 151.402, Residuals: 0.479\n",
      "Loss: 151.402, Residuals: 0.479\n",
      "Evidence -86.955\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.53e-01\n",
      "Loss: 151.499, Residuals: 0.478\n",
      "Loss: 151.499, Residuals: 0.478\n",
      "Loss: 151.497, Residuals: 0.479\n",
      "Loss: 151.496, Residuals: 0.479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 151.494, Residuals: 0.479\n",
      "Loss: 151.494, Residuals: 0.478\n",
      "Loss: 151.494, Residuals: 0.478\n",
      "Loss: 151.494, Residuals: 0.478\n",
      "Loss: 151.493, Residuals: 0.477\n",
      "Loss: 151.493, Residuals: 0.477\n",
      "Evidence -86.850\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.50e-01\n",
      "Loss: 151.571, Residuals: 0.477\n",
      "Loss: 151.571, Residuals: 0.475\n",
      "Loss: 151.570, Residuals: 0.477\n",
      "Loss: 151.568, Residuals: 0.477\n",
      "Loss: 151.567, Residuals: 0.477\n",
      "Loss: 151.567, Residuals: 0.477\n",
      "Evidence -86.777\n",
      "Pass count  1\n",
      "Total samples: 40, Updated regularization: 1.00e-05\n",
      "Loss: 408.233, Residuals: -1.839\n",
      "Loss: 303.425, Residuals: -1.947\n",
      "Loss: 262.178, Residuals: -0.207\n",
      "Loss: 254.220, Residuals: 0.451\n",
      "Loss: 187.481, Residuals: -0.394\n",
      "Loss: 179.401, Residuals: -0.031\n",
      "Loss: 165.393, Residuals: 0.156\n",
      "Loss: 143.518, Residuals: 0.279\n",
      "Loss: 138.150, Residuals: 0.245\n",
      "Loss: 128.456, Residuals: 0.201\n",
      "Loss: 116.898, Residuals: 0.295\n",
      "Loss: 116.087, Residuals: 0.388\n",
      "Loss: 109.478, Residuals: 0.357\n",
      "Loss: 107.957, Residuals: 0.442\n",
      "Loss: 105.137, Residuals: 0.413\n",
      "Loss: 100.387, Residuals: 0.355\n",
      "Loss: 99.802, Residuals: 0.400\n",
      "Loss: 95.200, Residuals: 0.348\n",
      "Loss: 95.068, Residuals: 0.314\n",
      "Loss: 94.991, Residuals: 0.316\n",
      "Loss: 92.176, Residuals: 0.271\n",
      "Loss: 92.115, Residuals: 0.245\n",
      "Loss: 91.571, Residuals: 0.213\n",
      "Loss: 90.538, Residuals: 0.194\n",
      "Loss: 88.989, Residuals: 0.143\n",
      "Loss: 88.971, Residuals: 0.144\n",
      "Loss: 86.545, Residuals: 0.112\n",
      "Loss: 86.507, Residuals: 0.093\n",
      "Loss: 86.457, Residuals: 0.077\n",
      "Loss: 86.362, Residuals: 0.093\n",
      "Loss: 86.195, Residuals: 0.126\n",
      "Loss: 84.725, Residuals: 0.103\n",
      "Loss: 84.703, Residuals: 0.101\n",
      "Loss: 83.807, Residuals: 0.082\n",
      "Loss: 82.952, Residuals: 0.071\n",
      "Loss: 82.835, Residuals: 0.142\n",
      "Loss: 81.781, Residuals: 0.127\n",
      "Loss: 81.768, Residuals: 0.117\n",
      "Loss: 81.667, Residuals: 0.149\n",
      "Loss: 80.760, Residuals: 0.131\n",
      "Loss: 80.752, Residuals: 0.113\n",
      "Loss: 79.554, Residuals: 0.097\n",
      "Loss: 79.546, Residuals: 0.086\n",
      "Loss: 79.535, Residuals: 0.079\n",
      "Loss: 79.143, Residuals: 0.075\n",
      "Loss: 78.652, Residuals: 0.079\n",
      "Loss: 78.537, Residuals: 0.140\n",
      "Loss: 78.535, Residuals: 0.139\n",
      "Loss: 78.247, Residuals: 0.131\n",
      "Loss: 77.725, Residuals: 0.115\n",
      "Loss: 77.718, Residuals: 0.104\n",
      "Loss: 77.452, Residuals: 0.096\n",
      "Loss: 76.977, Residuals: 0.083\n",
      "Loss: 76.971, Residuals: 0.071\n",
      "Loss: 76.754, Residuals: 0.063\n",
      "Loss: 76.608, Residuals: 0.078\n",
      "Loss: 76.607, Residuals: 0.080\n",
      "Loss: 76.382, Residuals: 0.071\n",
      "Loss: 76.135, Residuals: 0.052\n",
      "Loss: 76.133, Residuals: 0.058\n",
      "Loss: 76.131, Residuals: 0.055\n",
      "Loss: 76.115, Residuals: 0.058\n",
      "Loss: 76.089, Residuals: 0.064\n",
      "Loss: 75.852, Residuals: 0.055\n",
      "Loss: 75.851, Residuals: 0.057\n",
      "Evidence -549.567\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 8.48e-02\n",
      "Loss: 107.066, Residuals: 0.026\n",
      "Loss: 106.332, Residuals: 0.064\n",
      "Loss: 106.180, Residuals: 0.056\n",
      "Loss: 104.794, Residuals: 0.066\n",
      "Loss: 103.664, Residuals: 0.005\n",
      "Loss: 103.340, Residuals: 0.093\n",
      "Loss: 100.731, Residuals: 0.126\n",
      "Loss: 100.723, Residuals: 0.116\n",
      "Loss: 100.708, Residuals: 0.112\n",
      "Loss: 100.681, Residuals: 0.105\n",
      "Loss: 100.629, Residuals: 0.109\n",
      "Loss: 100.135, Residuals: 0.112\n",
      "Loss: 99.933, Residuals: 0.159\n",
      "Loss: 99.928, Residuals: 0.160\n",
      "Loss: 99.170, Residuals: 0.166\n",
      "Loss: 98.120, Residuals: 0.172\n",
      "Loss: 98.102, Residuals: 0.178\n",
      "Loss: 98.075, Residuals: 0.180\n",
      "Loss: 97.824, Residuals: 0.180\n",
      "Loss: 97.423, Residuals: 0.178\n",
      "Loss: 97.416, Residuals: 0.179\n",
      "Loss: 97.360, Residuals: 0.178\n",
      "Loss: 97.257, Residuals: 0.180\n",
      "Loss: 97.246, Residuals: 0.182\n",
      "Loss: 97.229, Residuals: 0.181\n",
      "Loss: 97.085, Residuals: 0.182\n",
      "Loss: 97.084, Residuals: 0.183\n",
      "Loss: 97.074, Residuals: 0.184\n",
      "Loss: 96.986, Residuals: 0.186\n",
      "Loss: 96.981, Residuals: 0.195\n",
      "Loss: 96.973, Residuals: 0.194\n",
      "Loss: 96.957, Residuals: 0.193\n",
      "Loss: 96.943, Residuals: 0.190\n",
      "Loss: 96.917, Residuals: 0.191\n",
      "Loss: 96.915, Residuals: 0.197\n",
      "Loss: 96.914, Residuals: 0.193\n",
      "Loss: 96.879, Residuals: 0.194\n",
      "Loss: 96.819, Residuals: 0.194\n",
      "Loss: 96.818, Residuals: 0.196\n",
      "Loss: 96.812, Residuals: 0.197\n",
      "Loss: 96.802, Residuals: 0.196\n",
      "Loss: 96.720, Residuals: 0.198\n",
      "Loss: 96.720, Residuals: 0.200\n",
      "Loss: 96.712, Residuals: 0.200\n",
      "Loss: 96.700, Residuals: 0.200\n",
      "Loss: 96.682, Residuals: 0.201\n",
      "Loss: 96.679, Residuals: 0.207\n",
      "Loss: 96.678, Residuals: 0.204\n",
      "Loss: 96.645, Residuals: 0.203\n",
      "Loss: 96.643, Residuals: 0.210\n",
      "Loss: 96.638, Residuals: 0.209\n",
      "Loss: 96.631, Residuals: 0.209\n",
      "Loss: 96.619, Residuals: 0.207\n",
      "Loss: 96.618, Residuals: 0.208\n",
      "Loss: 96.608, Residuals: 0.208\n",
      "Loss: 96.590, Residuals: 0.209\n",
      "Loss: 96.588, Residuals: 0.213\n",
      "Loss: 96.569, Residuals: 0.213\n",
      "Loss: 96.566, Residuals: 0.214\n",
      "Loss: 96.561, Residuals: 0.213\n",
      "Loss: 96.552, Residuals: 0.213\n",
      "Loss: 96.549, Residuals: 0.213\n",
      "Loss: 96.527, Residuals: 0.214\n",
      "Loss: 96.527, Residuals: 0.217\n",
      "Loss: 96.526, Residuals: 0.217\n",
      "Loss: 96.524, Residuals: 0.217\n",
      "Loss: 96.521, Residuals: 0.217\n",
      "Loss: 96.521, Residuals: 0.219\n",
      "Loss: 96.515, Residuals: 0.219\n",
      "Loss: 96.512, Residuals: 0.217\n",
      "Loss: 96.511, Residuals: 0.220\n",
      "Loss: 96.508, Residuals: 0.220\n",
      "Loss: 96.501, Residuals: 0.219\n",
      "Loss: 96.500, Residuals: 0.220\n",
      "Loss: 96.500, Residuals: 0.220\n",
      "Loss: 96.493, Residuals: 0.220\n",
      "Loss: 96.492, Residuals: 0.221\n",
      "Loss: 96.492, Residuals: 0.221\n",
      "Loss: 96.490, Residuals: 0.220\n",
      "Loss: 96.490, Residuals: 0.222\n",
      "Loss: 96.490, Residuals: 0.221\n",
      "Loss: 96.488, Residuals: 0.221\n",
      "Loss: 96.485, Residuals: 0.221\n",
      "Loss: 96.485, Residuals: 0.221\n",
      "Loss: 96.485, Residuals: 0.221\n",
      "Loss: 96.484, Residuals: 0.222\n",
      "Loss: 96.483, Residuals: 0.222\n",
      "Loss: 96.482, Residuals: 0.223\n",
      "Loss: 96.482, Residuals: 0.223\n",
      "Loss: 96.482, Residuals: 0.223\n",
      "Loss: 96.480, Residuals: 0.223\n",
      "Loss: 96.479, Residuals: 0.223\n",
      "Evidence -160.615\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.84e-01\n",
      "Loss: 127.476, Residuals: 0.216\n",
      "Loss: 127.118, Residuals: 0.238\n",
      "Loss: 126.455, Residuals: 0.229\n",
      "Loss: 126.124, Residuals: 0.137\n",
      "Loss: 126.111, Residuals: 0.148\n",
      "Loss: 125.588, Residuals: 0.164\n",
      "Loss: 124.711, Residuals: 0.198\n",
      "Loss: 123.774, Residuals: 0.291\n",
      "Loss: 123.749, Residuals: 0.296\n",
      "Loss: 123.714, Residuals: 0.298\n",
      "Loss: 123.408, Residuals: 0.293\n",
      "Loss: 123.038, Residuals: 0.296\n",
      "Loss: 123.024, Residuals: 0.292\n",
      "Loss: 123.001, Residuals: 0.290\n",
      "Loss: 122.974, Residuals: 0.288\n",
      "Loss: 122.931, Residuals: 0.293\n",
      "Loss: 122.929, Residuals: 0.292\n",
      "Loss: 122.916, Residuals: 0.292\n",
      "Loss: 122.897, Residuals: 0.293\n",
      "Loss: 122.896, Residuals: 0.294\n",
      "Loss: 122.896, Residuals: 0.294\n",
      "Loss: 122.896, Residuals: 0.293\n",
      "Loss: 122.890, Residuals: 0.293\n",
      "Loss: 122.890, Residuals: 0.291\n",
      "Loss: 122.888, Residuals: 0.291\n",
      "Loss: 122.888, Residuals: 0.291\n",
      "Loss: 122.887, Residuals: 0.291\n",
      "Loss: 122.887, Residuals: 0.290\n",
      "Loss: 122.887, Residuals: 0.291\n",
      "Loss: 122.887, Residuals: 0.291\n",
      "Loss: 122.886, Residuals: 0.291\n",
      "Loss: 122.886, Residuals: 0.291\n",
      "Evidence -108.258\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.19e-01\n",
      "Loss: 141.926, Residuals: 0.271\n",
      "Loss: 141.391, Residuals: 0.254\n",
      "Loss: 140.527, Residuals: 0.248\n",
      "Loss: 140.499, Residuals: 0.251\n",
      "Loss: 140.233, Residuals: 0.259\n",
      "Loss: 139.781, Residuals: 0.272\n",
      "Loss: 139.689, Residuals: 0.283\n",
      "Loss: 139.520, Residuals: 0.286\n",
      "Loss: 139.215, Residuals: 0.292\n",
      "Loss: 138.834, Residuals: 0.326\n",
      "Loss: 138.829, Residuals: 0.331\n",
      "Loss: 138.781, Residuals: 0.323\n",
      "Loss: 138.764, Residuals: 0.316\n",
      "Loss: 138.730, Residuals: 0.316\n",
      "Loss: 138.694, Residuals: 0.303\n",
      "Loss: 138.683, Residuals: 0.312\n",
      "Loss: 138.672, Residuals: 0.310\n",
      "Loss: 138.595, Residuals: 0.313\n",
      "Loss: 138.595, Residuals: 0.313\n",
      "Loss: 138.581, Residuals: 0.313\n",
      "Loss: 138.559, Residuals: 0.314\n",
      "Loss: 138.557, Residuals: 0.312\n",
      "Loss: 138.555, Residuals: 0.311\n",
      "Loss: 138.555, Residuals: 0.310\n",
      "Loss: 138.549, Residuals: 0.311\n",
      "Loss: 138.543, Residuals: 0.313\n",
      "Loss: 138.543, Residuals: 0.312\n",
      "Loss: 138.543, Residuals: 0.312\n",
      "Loss: 138.543, Residuals: 0.312\n",
      "Loss: 138.542, Residuals: 0.312\n",
      "Loss: 138.542, Residuals: 0.312\n",
      "Evidence -91.181\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.44e-01\n",
      "Loss: 148.212, Residuals: 0.279\n",
      "Loss: 147.801, Residuals: 0.280\n",
      "Loss: 147.096, Residuals: 0.278\n",
      "Loss: 146.411, Residuals: 0.274\n",
      "Loss: 146.331, Residuals: 0.290\n",
      "Loss: 146.187, Residuals: 0.298\n",
      "Loss: 145.939, Residuals: 0.300\n",
      "Loss: 145.930, Residuals: 0.296\n",
      "Loss: 145.845, Residuals: 0.298\n",
      "Loss: 145.707, Residuals: 0.305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 145.690, Residuals: 0.295\n",
      "Loss: 145.661, Residuals: 0.299\n",
      "Loss: 145.617, Residuals: 0.307\n",
      "Loss: 145.612, Residuals: 0.307\n",
      "Loss: 145.610, Residuals: 0.304\n",
      "Loss: 145.596, Residuals: 0.306\n",
      "Loss: 145.575, Residuals: 0.310\n",
      "Loss: 145.575, Residuals: 0.310\n",
      "Loss: 145.574, Residuals: 0.309\n",
      "Loss: 145.573, Residuals: 0.308\n",
      "Loss: 145.573, Residuals: 0.307\n",
      "Loss: 145.567, Residuals: 0.309\n",
      "Loss: 145.557, Residuals: 0.313\n",
      "Loss: 145.557, Residuals: 0.313\n",
      "Loss: 145.551, Residuals: 0.315\n",
      "Loss: 145.551, Residuals: 0.315\n",
      "Loss: 145.548, Residuals: 0.317\n",
      "Loss: 145.547, Residuals: 0.317\n",
      "Loss: 145.547, Residuals: 0.317\n",
      "Evidence -82.093\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.78e-01\n",
      "Loss: 150.774, Residuals: 0.275\n",
      "Loss: 150.416, Residuals: 0.284\n",
      "Loss: 150.162, Residuals: 0.307\n",
      "Loss: 150.154, Residuals: 0.309\n",
      "Loss: 150.080, Residuals: 0.312\n",
      "Loss: 149.969, Residuals: 0.322\n",
      "Loss: 149.962, Residuals: 0.316\n",
      "Loss: 149.948, Residuals: 0.317\n",
      "Loss: 149.924, Residuals: 0.319\n",
      "Loss: 149.924, Residuals: 0.318\n",
      "Loss: 149.898, Residuals: 0.323\n",
      "Loss: 149.872, Residuals: 0.330\n",
      "Loss: 149.872, Residuals: 0.330\n",
      "Loss: 149.870, Residuals: 0.328\n",
      "Loss: 149.851, Residuals: 0.332\n",
      "Loss: 149.825, Residuals: 0.340\n",
      "Loss: 149.825, Residuals: 0.341\n",
      "Loss: 149.825, Residuals: 0.340\n",
      "Loss: 149.824, Residuals: 0.339\n",
      "Loss: 149.823, Residuals: 0.340\n",
      "Loss: 149.823, Residuals: 0.340\n",
      "Evidence -76.946\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 3.18e-01\n",
      "Loss: 152.680, Residuals: 0.330\n",
      "Loss: 152.646, Residuals: 0.342\n",
      "Loss: 152.590, Residuals: 0.343\n",
      "Loss: 152.524, Residuals: 0.344\n",
      "Loss: 152.519, Residuals: 0.344\n",
      "Loss: 152.514, Residuals: 0.348\n",
      "Loss: 152.476, Residuals: 0.354\n",
      "Loss: 152.475, Residuals: 0.353\n",
      "Loss: 152.468, Residuals: 0.354\n",
      "Loss: 152.455, Residuals: 0.358\n",
      "Loss: 152.455, Residuals: 0.358\n",
      "Loss: 152.452, Residuals: 0.359\n",
      "Loss: 152.446, Residuals: 0.361\n",
      "Loss: 152.445, Residuals: 0.361\n",
      "Loss: 152.445, Residuals: 0.361\n",
      "Evidence -74.964\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 3.51e-01\n",
      "Loss: 153.879, Residuals: 0.357\n",
      "Loss: 153.871, Residuals: 0.348\n",
      "Loss: 153.854, Residuals: 0.350\n",
      "Loss: 153.826, Residuals: 0.352\n",
      "Loss: 153.789, Residuals: 0.358\n",
      "Loss: 153.788, Residuals: 0.358\n",
      "Loss: 153.782, Residuals: 0.359\n",
      "Loss: 153.773, Residuals: 0.362\n",
      "Loss: 153.773, Residuals: 0.362\n",
      "Loss: 153.761, Residuals: 0.365\n",
      "Loss: 153.748, Residuals: 0.373\n",
      "Loss: 153.748, Residuals: 0.374\n",
      "Loss: 153.748, Residuals: 0.373\n",
      "Loss: 153.747, Residuals: 0.373\n",
      "Loss: 153.747, Residuals: 0.373\n",
      "Loss: 153.747, Residuals: 0.373\n",
      "Evidence -74.027\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 3.80e-01\n",
      "Loss: 154.566, Residuals: 0.362\n",
      "Loss: 154.534, Residuals: 0.369\n",
      "Loss: 154.501, Residuals: 0.371\n",
      "Loss: 154.500, Residuals: 0.369\n",
      "Loss: 154.497, Residuals: 0.369\n",
      "Loss: 154.493, Residuals: 0.369\n",
      "Loss: 154.492, Residuals: 0.368\n",
      "Loss: 154.485, Residuals: 0.371\n",
      "Loss: 154.479, Residuals: 0.378\n",
      "Loss: 154.479, Residuals: 0.378\n",
      "Loss: 154.479, Residuals: 0.378\n",
      "Loss: 154.479, Residuals: 0.378\n",
      "Loss: 154.479, Residuals: 0.378\n",
      "Evidence -73.467\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.03e-01\n",
      "Loss: 155.012, Residuals: 0.372\n",
      "Loss: 154.982, Residuals: 0.373\n",
      "Loss: 154.974, Residuals: 0.375\n",
      "Loss: 154.969, Residuals: 0.373\n",
      "Loss: 154.967, Residuals: 0.375\n",
      "Loss: 154.963, Residuals: 0.374\n",
      "Loss: 154.959, Residuals: 0.374\n",
      "Loss: 154.959, Residuals: 0.374\n",
      "Evidence -73.008\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.09e-01\n",
      "Loss: 155.321, Residuals: 0.370\n",
      "Loss: 155.301, Residuals: 0.369\n",
      "Loss: 155.288, Residuals: 0.370\n",
      "Loss: 155.286, Residuals: 0.369\n",
      "Loss: 155.285, Residuals: 0.370\n",
      "Loss: 155.280, Residuals: 0.372\n",
      "Loss: 155.279, Residuals: 0.372\n",
      "Loss: 155.278, Residuals: 0.372\n",
      "Loss: 155.277, Residuals: 0.373\n",
      "Loss: 155.277, Residuals: 0.372\n",
      "Loss: 155.276, Residuals: 0.374\n",
      "Loss: 155.276, Residuals: 0.374\n",
      "Loss: 155.276, Residuals: 0.375\n",
      "Loss: 155.275, Residuals: 0.377\n",
      "Evidence -72.764\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.25e-01\n",
      "Loss: 155.551, Residuals: 0.372\n",
      "Loss: 155.537, Residuals: 0.372\n",
      "Loss: 155.524, Residuals: 0.373\n",
      "Loss: 155.524, Residuals: 0.372\n",
      "Loss: 155.523, Residuals: 0.372\n",
      "Loss: 155.522, Residuals: 0.372\n",
      "Loss: 155.521, Residuals: 0.372\n",
      "Loss: 155.521, Residuals: 0.372\n",
      "Loss: 155.520, Residuals: 0.372\n",
      "Loss: 155.520, Residuals: 0.372\n",
      "Loss: 155.519, Residuals: 0.373\n",
      "Loss: 155.518, Residuals: 0.374\n",
      "Loss: 155.518, Residuals: 0.374\n",
      "Evidence -72.552\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.33e-01\n",
      "Loss: 155.701, Residuals: 0.373\n",
      "Loss: 155.699, Residuals: 0.370\n",
      "Loss: 155.696, Residuals: 0.371\n",
      "Loss: 155.694, Residuals: 0.370\n",
      "Loss: 155.694, Residuals: 0.370\n",
      "Evidence -72.347\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.35e-01\n",
      "Loss: 155.836, Residuals: 0.370\n",
      "Loss: 155.833, Residuals: 0.367\n",
      "Loss: 155.830, Residuals: 0.368\n",
      "Loss: 155.828, Residuals: 0.368\n",
      "Loss: 155.828, Residuals: 0.368\n",
      "Loss: 155.827, Residuals: 0.368\n",
      "Loss: 155.826, Residuals: 0.370\n",
      "Loss: 155.826, Residuals: 0.370\n",
      "Loss: 155.825, Residuals: 0.370\n",
      "Loss: 155.825, Residuals: 0.370\n",
      "Evidence -72.242\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.44e-01\n",
      "Loss: 155.941, Residuals: 0.368\n",
      "Loss: 155.939, Residuals: 0.368\n",
      "Loss: 155.936, Residuals: 0.369\n",
      "Loss: 155.936, Residuals: 0.368\n",
      "Loss: 155.935, Residuals: 0.368\n",
      "Loss: 155.934, Residuals: 0.368\n",
      "Loss: 155.934, Residuals: 0.367\n",
      "Loss: 155.934, Residuals: 0.367\n",
      "Loss: 155.933, Residuals: 0.368\n",
      "Loss: 155.933, Residuals: 0.370\n",
      "Evidence -72.161\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.52e-01\n",
      "Loss: 156.027, Residuals: 0.368\n",
      "Loss: 156.025, Residuals: 0.367\n",
      "Loss: 156.023, Residuals: 0.368\n",
      "Loss: 156.023, Residuals: 0.367\n",
      "Loss: 156.022, Residuals: 0.367\n",
      "Loss: 156.022, Residuals: 0.367\n",
      "Evidence -72.047\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.53e-01\n",
      "Loss: 156.101, Residuals: 0.365\n",
      "Loss: 156.100, Residuals: 0.364\n",
      "Loss: 156.098, Residuals: 0.364\n",
      "Loss: 156.098, Residuals: 0.364\n",
      "Loss: 156.098, Residuals: 0.364\n",
      "Loss: 156.098, Residuals: 0.364\n",
      "Loss: 156.097, Residuals: 0.364\n",
      "Loss: 156.097, Residuals: 0.364\n",
      "Evidence -71.948\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.54e-01\n",
      "Loss: 156.162, Residuals: 0.362\n",
      "Loss: 156.161, Residuals: 0.362\n",
      "Loss: 156.159, Residuals: 0.362\n",
      "Loss: 156.159, Residuals: 0.361\n",
      "Loss: 156.159, Residuals: 0.361\n",
      "Loss: 156.159, Residuals: 0.361\n",
      "Evidence -71.864\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 4.56e-01\n",
      "Loss: 156.212, Residuals: 0.359\n",
      "Loss: 156.211, Residuals: 0.359\n",
      "Loss: 156.210, Residuals: 0.359\n",
      "Loss: 156.209, Residuals: 0.359\n",
      "Loss: 156.209, Residuals: 0.358\n",
      "Loss: 156.209, Residuals: 0.359\n",
      "Loss: 156.208, Residuals: 0.359\n",
      "Loss: 156.208, Residuals: 0.360\n",
      "Loss: 156.208, Residuals: 0.360\n",
      "Loss: 156.208, Residuals: 0.360\n",
      "Loss: 156.208, Residuals: 0.361\n",
      "Loss: 156.208, Residuals: 0.361\n",
      "Evidence -71.833\n",
      "Pass count  1\n",
      "Total samples: 40, Updated regularization: 1.00e-05\n",
      "Loss: 404.765, Residuals: -2.007\n",
      "Loss: 302.427, Residuals: -1.951\n",
      "Loss: 259.774, Residuals: -0.420\n",
      "Loss: 248.235, Residuals: 0.403\n",
      "Loss: 187.907, Residuals: -0.248\n",
      "Loss: 176.948, Residuals: 0.017\n",
      "Loss: 161.491, Residuals: -0.107\n",
      "Loss: 144.894, Residuals: 0.424\n",
      "Loss: 142.246, Residuals: 0.581\n",
      "Loss: 137.295, Residuals: 0.527\n",
      "Loss: 128.554, Residuals: 0.402\n",
      "Loss: 125.580, Residuals: 0.344\n",
      "Loss: 120.083, Residuals: 0.278\n",
      "Loss: 117.516, Residuals: 0.323\n",
      "Loss: 112.817, Residuals: 0.298\n",
      "Loss: 106.940, Residuals: 0.443\n",
      "Loss: 106.230, Residuals: 0.493\n",
      "Loss: 104.873, Residuals: 0.463\n",
      "Loss: 102.426, Residuals: 0.423\n",
      "Loss: 98.477, Residuals: 0.335\n",
      "Loss: 98.156, Residuals: 0.352\n",
      "Loss: 95.405, Residuals: 0.266\n",
      "Loss: 94.827, Residuals: 0.227\n",
      "Loss: 93.737, Residuals: 0.206\n",
      "Loss: 91.812, Residuals: 0.170\n",
      "Loss: 91.748, Residuals: 0.177\n",
      "Loss: 89.484, Residuals: 0.109\n",
      "Loss: 89.462, Residuals: 0.106\n",
      "Loss: 88.583, Residuals: 0.086\n",
      "Loss: 88.107, Residuals: 0.104\n",
      "Loss: 87.979, Residuals: 0.167\n",
      "Loss: 86.835, Residuals: 0.136\n",
      "Loss: 86.823, Residuals: 0.123\n",
      "Loss: 85.156, Residuals: 0.093\n",
      "Loss: 85.131, Residuals: 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 84.899, Residuals: 0.075\n",
      "Loss: 84.490, Residuals: 0.078\n",
      "Loss: 83.806, Residuals: 0.065\n",
      "Loss: 83.773, Residuals: 0.082\n",
      "Loss: 83.715, Residuals: 0.091\n",
      "Loss: 83.176, Residuals: 0.083\n",
      "Loss: 83.168, Residuals: 0.063\n",
      "Loss: 81.972, Residuals: 0.060\n",
      "Loss: 81.959, Residuals: 0.048\n",
      "Loss: 81.942, Residuals: 0.032\n",
      "Loss: 81.796, Residuals: 0.050\n",
      "Loss: 80.646, Residuals: 0.056\n",
      "Loss: 80.631, Residuals: 0.042\n",
      "Loss: 80.609, Residuals: 0.026\n",
      "Loss: 80.571, Residuals: 0.033\n",
      "Loss: 80.509, Residuals: 0.035\n",
      "Loss: 80.392, Residuals: 0.035\n",
      "Loss: 80.227, Residuals: 0.064\n",
      "Loss: 80.206, Residuals: 0.076\n",
      "Loss: 79.465, Residuals: 0.076\n",
      "Loss: 79.463, Residuals: 0.071\n",
      "Loss: 79.460, Residuals: 0.066\n",
      "Loss: 79.456, Residuals: 0.056\n",
      "Loss: 78.915, Residuals: 0.056\n",
      "Loss: 78.912, Residuals: 0.047\n",
      "Loss: 78.887, Residuals: 0.047\n",
      "Loss: 78.840, Residuals: 0.045\n",
      "Loss: 78.751, Residuals: 0.043\n",
      "Loss: 78.684, Residuals: 0.056\n",
      "Loss: 78.565, Residuals: 0.050\n",
      "Loss: 78.556, Residuals: 0.052\n",
      "Loss: 78.212, Residuals: 0.052\n",
      "Loss: 78.210, Residuals: 0.043\n",
      "Loss: 77.948, Residuals: 0.042\n",
      "Loss: 77.896, Residuals: 0.039\n",
      "Loss: 77.888, Residuals: 0.039\n",
      "Loss: 77.818, Residuals: 0.042\n",
      "Loss: 77.783, Residuals: 0.039\n",
      "Loss: 77.771, Residuals: 0.038\n",
      "Loss: 77.661, Residuals: 0.035\n",
      "Loss: 77.565, Residuals: 0.042\n",
      "Loss: 77.562, Residuals: 0.039\n",
      "Loss: 77.555, Residuals: 0.040\n",
      "Loss: 77.493, Residuals: 0.042\n",
      "Loss: 77.479, Residuals: 0.046\n",
      "Loss: 77.354, Residuals: 0.047\n",
      "Loss: 77.351, Residuals: 0.043\n",
      "Loss: 77.346, Residuals: 0.043\n",
      "Loss: 77.338, Residuals: 0.045\n",
      "Loss: 77.326, Residuals: 0.046\n",
      "Loss: 77.326, Residuals: 0.045\n",
      "Loss: 77.072, Residuals: 0.044\n",
      "Loss: 76.785, Residuals: 0.013\n",
      "Loss: 76.776, Residuals: 0.024\n",
      "Loss: 76.763, Residuals: 0.025\n",
      "Loss: 76.743, Residuals: 0.024\n",
      "Loss: 76.708, Residuals: 0.026\n",
      "Loss: 76.689, Residuals: 0.030\n",
      "Loss: 76.654, Residuals: 0.030\n",
      "Loss: 76.653, Residuals: 0.030\n",
      "Loss: 76.618, Residuals: 0.031\n",
      "Loss: 76.566, Residuals: 0.036\n",
      "Loss: 76.564, Residuals: 0.033\n",
      "Loss: 76.561, Residuals: 0.034\n",
      "Loss: 76.556, Residuals: 0.035\n",
      "Loss: 76.550, Residuals: 0.036\n",
      "Loss: 76.550, Residuals: 0.034\n",
      "Loss: 76.372, Residuals: 0.034\n",
      "Loss: 76.111, Residuals: 0.035\n",
      "Loss: 76.104, Residuals: 0.029\n",
      "Loss: 76.092, Residuals: 0.028\n",
      "Loss: 76.070, Residuals: 0.026\n",
      "Loss: 76.064, Residuals: 0.027\n",
      "Loss: 76.059, Residuals: 0.029\n",
      "Loss: 76.056, Residuals: 0.029\n",
      "Loss: 76.031, Residuals: 0.030\n",
      "Loss: 76.000, Residuals: 0.024\n",
      "Loss: 75.998, Residuals: 0.021\n",
      "Loss: 75.994, Residuals: 0.023\n",
      "Loss: 75.990, Residuals: 0.024\n",
      "Loss: 75.990, Residuals: 0.024\n",
      "Loss: 75.516, Residuals: 0.030\n",
      "Loss: 75.515, Residuals: 0.026\n",
      "Loss: 75.513, Residuals: 0.023\n",
      "Loss: 75.511, Residuals: 0.016\n",
      "Loss: 75.491, Residuals: 0.016\n",
      "Loss: 75.487, Residuals: 0.016\n",
      "Loss: 75.486, Residuals: 0.016\n",
      "Loss: 75.453, Residuals: 0.017\n",
      "Loss: 75.453, Residuals: 0.014\n",
      "Evidence -545.815\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.49e-02\n",
      "Loss: 109.995, Residuals: -0.058\n",
      "Loss: 108.708, Residuals: -0.062\n",
      "Loss: 107.420, Residuals: -0.035\n",
      "Loss: 107.080, Residuals: -0.034\n",
      "Loss: 106.432, Residuals: -0.029\n",
      "Loss: 106.279, Residuals: -0.035\n",
      "Loss: 104.970, Residuals: -0.006\n",
      "Loss: 104.952, Residuals: -0.002\n",
      "Loss: 104.923, Residuals: 0.002\n",
      "Loss: 103.962, Residuals: 0.019\n",
      "Loss: 103.943, Residuals: 0.022\n",
      "Loss: 103.919, Residuals: 0.022\n",
      "Loss: 103.734, Residuals: 0.030\n",
      "Loss: 103.721, Residuals: 0.032\n",
      "Loss: 101.960, Residuals: 0.029\n",
      "Loss: 101.908, Residuals: 0.064\n",
      "Loss: 101.897, Residuals: 0.055\n",
      "Loss: 101.880, Residuals: 0.058\n",
      "Loss: 101.723, Residuals: 0.059\n",
      "Loss: 101.454, Residuals: 0.065\n",
      "Loss: 101.453, Residuals: 0.066\n",
      "Loss: 101.439, Residuals: 0.070\n",
      "Loss: 101.311, Residuals: 0.075\n",
      "Loss: 101.294, Residuals: 0.081\n",
      "Loss: 101.288, Residuals: 0.082\n",
      "Loss: 101.055, Residuals: 0.086\n",
      "Loss: 101.054, Residuals: 0.086\n",
      "Loss: 100.594, Residuals: 0.098\n",
      "Loss: 100.594, Residuals: 0.096\n",
      "Evidence -210.341\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 6.90e-02\n",
      "Loss: 134.950, Residuals: 0.066\n",
      "Loss: 134.767, Residuals: 0.065\n",
      "Loss: 134.427, Residuals: 0.053\n",
      "Loss: 133.861, Residuals: 0.027\n",
      "Loss: 132.797, Residuals: 0.036\n",
      "Loss: 131.279, Residuals: 0.030\n",
      "Loss: 131.260, Residuals: 0.028\n",
      "Loss: 130.540, Residuals: 0.045\n",
      "Loss: 130.455, Residuals: 0.056\n",
      "Loss: 127.278, Residuals: 0.090\n",
      "Loss: 127.275, Residuals: 0.086\n",
      "Loss: 127.255, Residuals: 0.085\n",
      "Loss: 126.452, Residuals: 0.105\n",
      "Loss: 126.159, Residuals: 0.136\n",
      "Loss: 126.075, Residuals: 0.113\n",
      "Loss: 126.061, Residuals: 0.116\n",
      "Loss: 126.051, Residuals: 0.123\n",
      "Loss: 124.618, Residuals: 0.138\n",
      "Loss: 124.613, Residuals: 0.127\n",
      "Loss: 123.788, Residuals: 0.155\n",
      "Loss: 123.785, Residuals: 0.152\n",
      "Loss: 123.765, Residuals: 0.153\n",
      "Loss: 123.607, Residuals: 0.170\n",
      "Loss: 123.580, Residuals: 0.166\n",
      "Loss: 123.328, Residuals: 0.169\n",
      "Loss: 123.070, Residuals: 0.169\n",
      "Loss: 123.066, Residuals: 0.174\n",
      "Loss: 123.030, Residuals: 0.173\n",
      "Loss: 122.970, Residuals: 0.170\n",
      "Loss: 122.956, Residuals: 0.165\n",
      "Loss: 122.929, Residuals: 0.166\n",
      "Loss: 122.687, Residuals: 0.169\n",
      "Loss: 122.645, Residuals: 0.172\n",
      "Loss: 122.634, Residuals: 0.168\n",
      "Loss: 122.613, Residuals: 0.167\n",
      "Loss: 122.438, Residuals: 0.171\n",
      "Loss: 122.414, Residuals: 0.176\n",
      "Loss: 122.408, Residuals: 0.171\n",
      "Loss: 122.398, Residuals: 0.169\n",
      "Loss: 122.396, Residuals: 0.170\n",
      "Loss: 122.393, Residuals: 0.170\n",
      "Loss: 122.306, Residuals: 0.172\n",
      "Loss: 122.280, Residuals: 0.171\n",
      "Loss: 122.278, Residuals: 0.168\n",
      "Loss: 122.274, Residuals: 0.171\n",
      "Loss: 122.237, Residuals: 0.172\n",
      "Loss: 122.188, Residuals: 0.175\n",
      "Loss: 122.182, Residuals: 0.177\n",
      "Loss: 122.181, Residuals: 0.174\n",
      "Loss: 122.178, Residuals: 0.174\n",
      "Loss: 122.174, Residuals: 0.175\n",
      "Loss: 122.170, Residuals: 0.172\n",
      "Loss: 122.170, Residuals: 0.172\n",
      "Loss: 122.170, Residuals: 0.172\n",
      "Loss: 122.160, Residuals: 0.173\n",
      "Loss: 122.159, Residuals: 0.174\n",
      "Loss: 122.159, Residuals: 0.174\n",
      "Loss: 122.157, Residuals: 0.174\n",
      "Loss: 122.157, Residuals: 0.174\n",
      "Loss: 122.156, Residuals: 0.174\n",
      "Loss: 122.153, Residuals: 0.174\n",
      "Loss: 122.153, Residuals: 0.174\n",
      "Loss: 122.153, Residuals: 0.173\n",
      "Loss: 122.151, Residuals: 0.173\n",
      "Loss: 122.151, Residuals: 0.174\n",
      "Loss: 122.150, Residuals: 0.173\n",
      "Loss: 122.150, Residuals: 0.173\n",
      "Loss: 122.150, Residuals: 0.173\n",
      "Loss: 122.149, Residuals: 0.173\n",
      "Loss: 122.149, Residuals: 0.173\n",
      "Loss: 122.149, Residuals: 0.173\n",
      "Loss: 122.149, Residuals: 0.173\n",
      "Loss: 122.149, Residuals: 0.173\n",
      "Loss: 122.149, Residuals: 0.173\n",
      "Loss: 122.149, Residuals: 0.173\n",
      "Evidence -167.038\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 3.25e-01\n",
      "Loss: 142.116, Residuals: 0.150\n",
      "Loss: 141.167, Residuals: 0.087\n",
      "Loss: 140.956, Residuals: 0.120\n",
      "Loss: 139.160, Residuals: 0.154\n",
      "Loss: 138.470, Residuals: 0.153\n",
      "Loss: 137.318, Residuals: 0.181\n",
      "Loss: 137.256, Residuals: 0.188\n",
      "Loss: 136.669, Residuals: 0.195\n",
      "Loss: 135.667, Residuals: 0.222\n",
      "Loss: 135.659, Residuals: 0.225\n",
      "Loss: 135.381, Residuals: 0.231\n",
      "Loss: 134.990, Residuals: 0.247\n",
      "Loss: 134.986, Residuals: 0.240\n",
      "Loss: 134.950, Residuals: 0.237\n",
      "Loss: 134.901, Residuals: 0.230\n",
      "Loss: 134.811, Residuals: 0.230\n",
      "Loss: 134.661, Residuals: 0.234\n",
      "Loss: 134.639, Residuals: 0.235\n",
      "Loss: 134.635, Residuals: 0.228\n",
      "Loss: 134.595, Residuals: 0.230\n",
      "Loss: 134.593, Residuals: 0.232\n",
      "Loss: 134.539, Residuals: 0.234\n",
      "Loss: 134.530, Residuals: 0.231\n",
      "Loss: 134.528, Residuals: 0.231\n",
      "Loss: 134.526, Residuals: 0.232\n",
      "Loss: 134.509, Residuals: 0.233\n",
      "Loss: 134.486, Residuals: 0.235\n",
      "Loss: 134.483, Residuals: 0.231\n",
      "Loss: 134.481, Residuals: 0.231\n",
      "Loss: 134.480, Residuals: 0.232\n",
      "Loss: 134.476, Residuals: 0.233\n",
      "Loss: 134.475, Residuals: 0.231\n",
      "Loss: 134.475, Residuals: 0.231\n",
      "Loss: 134.475, Residuals: 0.231\n",
      "Loss: 134.473, Residuals: 0.232\n",
      "Loss: 134.470, Residuals: 0.232\n",
      "Loss: 134.470, Residuals: 0.232\n",
      "Loss: 134.470, Residuals: 0.231\n",
      "Loss: 134.469, Residuals: 0.231\n",
      "Loss: 134.469, Residuals: 0.231\n",
      "Evidence -129.925\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 7.48e-01\n",
      "Loss: 147.695, Residuals: 0.231\n",
      "Loss: 146.874, Residuals: 0.221\n",
      "Loss: 145.496, Residuals: 0.241\n",
      "Loss: 145.341, Residuals: 0.195\n",
      "Loss: 143.961, Residuals: 0.219\n",
      "Loss: 142.087, Residuals: 0.295\n",
      "Loss: 142.074, Residuals: 0.286\n",
      "Loss: 141.970, Residuals: 0.283\n",
      "Loss: 141.810, Residuals: 0.265\n",
      "Loss: 141.669, Residuals: 0.273\n",
      "Loss: 141.495, Residuals: 0.280\n",
      "Loss: 141.462, Residuals: 0.270\n",
      "Loss: 141.451, Residuals: 0.274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 141.445, Residuals: 0.273\n",
      "Loss: 141.441, Residuals: 0.274\n",
      "Loss: 141.438, Residuals: 0.275\n",
      "Loss: 141.409, Residuals: 0.278\n",
      "Loss: 141.390, Residuals: 0.285\n",
      "Loss: 141.388, Residuals: 0.285\n",
      "Loss: 141.387, Residuals: 0.285\n",
      "Loss: 141.387, Residuals: 0.284\n",
      "Loss: 141.386, Residuals: 0.285\n",
      "Loss: 141.386, Residuals: 0.285\n",
      "Loss: 141.386, Residuals: 0.286\n",
      "Loss: 141.386, Residuals: 0.286\n",
      "Loss: 141.386, Residuals: 0.287\n",
      "Loss: 141.386, Residuals: 0.287\n",
      "Evidence -113.191\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.28e+00\n",
      "Loss: 149.615, Residuals: 0.175\n",
      "Loss: 148.591, Residuals: 0.206\n",
      "Loss: 147.407, Residuals: 0.256\n",
      "Loss: 147.345, Residuals: 0.258\n",
      "Loss: 147.230, Residuals: 0.259\n",
      "Loss: 147.042, Residuals: 0.263\n",
      "Loss: 147.013, Residuals: 0.264\n",
      "Loss: 146.959, Residuals: 0.267\n",
      "Loss: 146.881, Residuals: 0.275\n",
      "Loss: 146.876, Residuals: 0.279\n",
      "Loss: 146.867, Residuals: 0.280\n",
      "Loss: 146.851, Residuals: 0.283\n",
      "Loss: 146.850, Residuals: 0.282\n",
      "Loss: 146.842, Residuals: 0.284\n",
      "Loss: 146.831, Residuals: 0.292\n",
      "Loss: 146.831, Residuals: 0.292\n",
      "Loss: 146.831, Residuals: 0.292\n",
      "Loss: 146.830, Residuals: 0.292\n",
      "Loss: 146.830, Residuals: 0.292\n",
      "Loss: 146.829, Residuals: 0.292\n",
      "Loss: 146.829, Residuals: 0.292\n",
      "Loss: 146.829, Residuals: 0.293\n",
      "Loss: 146.829, Residuals: 0.293\n",
      "Loss: 146.829, Residuals: 0.293\n",
      "Loss: 146.829, Residuals: 0.293\n",
      "Loss: 146.829, Residuals: 0.294\n",
      "Evidence -99.562\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.62e+00\n",
      "Loss: 151.984, Residuals: 0.233\n",
      "Loss: 151.719, Residuals: 0.223\n",
      "Loss: 151.432, Residuals: 0.256\n",
      "Loss: 151.285, Residuals: 0.260\n",
      "Loss: 151.247, Residuals: 0.259\n",
      "Loss: 151.229, Residuals: 0.258\n",
      "Loss: 151.220, Residuals: 0.258\n",
      "Loss: 151.208, Residuals: 0.258\n",
      "Loss: 151.203, Residuals: 0.258\n",
      "Loss: 151.202, Residuals: 0.258\n",
      "Loss: 151.201, Residuals: 0.260\n",
      "Loss: 151.201, Residuals: 0.259\n",
      "Loss: 151.201, Residuals: 0.259\n",
      "Loss: 151.200, Residuals: 0.260\n",
      "Loss: 151.200, Residuals: 0.260\n",
      "Loss: 151.200, Residuals: 0.260\n",
      "Loss: 151.199, Residuals: 0.261\n",
      "Loss: 151.199, Residuals: 0.261\n",
      "Loss: 151.199, Residuals: 0.261\n",
      "Evidence -93.534\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.74e+00\n",
      "Loss: 153.774, Residuals: 0.247\n",
      "Loss: 153.658, Residuals: 0.223\n",
      "Loss: 153.567, Residuals: 0.243\n",
      "Loss: 153.555, Residuals: 0.242\n",
      "Loss: 153.544, Residuals: 0.241\n",
      "Loss: 153.542, Residuals: 0.239\n",
      "Loss: 153.540, Residuals: 0.240\n",
      "Loss: 153.540, Residuals: 0.239\n",
      "Loss: 153.539, Residuals: 0.239\n",
      "Loss: 153.539, Residuals: 0.239\n",
      "Loss: 153.539, Residuals: 0.239\n",
      "Loss: 153.539, Residuals: 0.240\n",
      "Loss: 153.539, Residuals: 0.240\n",
      "Loss: 153.539, Residuals: 0.240\n",
      "Loss: 153.539, Residuals: 0.240\n",
      "Evidence -90.824\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.79e+00\n",
      "Loss: 154.847, Residuals: 0.225\n",
      "Loss: 154.795, Residuals: 0.230\n",
      "Loss: 154.769, Residuals: 0.227\n",
      "Loss: 154.761, Residuals: 0.234\n",
      "Loss: 154.753, Residuals: 0.230\n",
      "Loss: 154.752, Residuals: 0.230\n",
      "Loss: 154.751, Residuals: 0.230\n",
      "Loss: 154.751, Residuals: 0.230\n",
      "Loss: 154.750, Residuals: 0.230\n",
      "Loss: 154.749, Residuals: 0.229\n",
      "Loss: 154.749, Residuals: 0.229\n",
      "Evidence -89.418\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.81e+00\n",
      "Loss: 155.475, Residuals: 0.219\n",
      "Loss: 155.446, Residuals: 0.227\n",
      "Loss: 155.429, Residuals: 0.221\n",
      "Loss: 155.427, Residuals: 0.226\n",
      "Loss: 155.426, Residuals: 0.223\n",
      "Loss: 155.425, Residuals: 0.223\n",
      "Loss: 155.423, Residuals: 0.223\n",
      "Loss: 155.423, Residuals: 0.223\n",
      "Loss: 155.423, Residuals: 0.223\n",
      "Loss: 155.423, Residuals: 0.223\n",
      "Loss: 155.423, Residuals: 0.223\n",
      "Evidence -88.607\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.82e+00\n",
      "Loss: 155.855, Residuals: 0.217\n",
      "Loss: 155.836, Residuals: 0.219\n",
      "Loss: 155.834, Residuals: 0.218\n",
      "Loss: 155.831, Residuals: 0.219\n",
      "Loss: 155.831, Residuals: 0.219\n",
      "Loss: 155.830, Residuals: 0.219\n",
      "Loss: 155.830, Residuals: 0.220\n",
      "Loss: 155.829, Residuals: 0.220\n",
      "Loss: 155.829, Residuals: 0.220\n",
      "Loss: 155.829, Residuals: 0.219\n",
      "Loss: 155.829, Residuals: 0.219\n",
      "Loss: 155.829, Residuals: 0.219\n",
      "Loss: 155.829, Residuals: 0.219\n",
      "Evidence -88.086\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.83e+00\n",
      "Loss: 156.107, Residuals: 0.216\n",
      "Loss: 156.095, Residuals: 0.216\n",
      "Loss: 156.093, Residuals: 0.217\n",
      "Loss: 156.090, Residuals: 0.216\n",
      "Loss: 156.090, Residuals: 0.217\n",
      "Loss: 156.090, Residuals: 0.216\n",
      "Loss: 156.089, Residuals: 0.217\n",
      "Loss: 156.089, Residuals: 0.217\n",
      "Evidence -87.727\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.83e+00\n",
      "Loss: 156.281, Residuals: 0.216\n",
      "Loss: 156.273, Residuals: 0.213\n",
      "Loss: 156.272, Residuals: 0.214\n",
      "Loss: 156.270, Residuals: 0.214\n",
      "Loss: 156.269, Residuals: 0.215\n",
      "Loss: 156.269, Residuals: 0.215\n",
      "Loss: 156.269, Residuals: 0.215\n",
      "Loss: 156.269, Residuals: 0.215\n",
      "Evidence -87.458\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.84e+00\n",
      "Loss: 156.406, Residuals: 0.214\n",
      "Loss: 156.404, Residuals: 0.212\n",
      "Loss: 156.401, Residuals: 0.213\n",
      "Loss: 156.399, Residuals: 0.214\n",
      "Loss: 156.399, Residuals: 0.214\n",
      "Loss: 156.399, Residuals: 0.213\n",
      "Loss: 156.399, Residuals: 0.213\n",
      "Evidence -87.256\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.84e+00\n",
      "Loss: 156.505, Residuals: 0.212\n",
      "Loss: 156.501, Residuals: 0.212\n",
      "Loss: 156.501, Residuals: 0.211\n",
      "Loss: 156.500, Residuals: 0.212\n",
      "Loss: 156.500, Residuals: 0.212\n",
      "Loss: 156.500, Residuals: 0.212\n",
      "Loss: 156.500, Residuals: 0.212\n",
      "Loss: 156.500, Residuals: 0.212\n",
      "Evidence -87.098\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.85e+00\n",
      "Loss: 156.585, Residuals: 0.212\n",
      "Loss: 156.582, Residuals: 0.211\n",
      "Loss: 156.581, Residuals: 0.212\n",
      "Loss: 156.580, Residuals: 0.211\n",
      "Loss: 156.580, Residuals: 0.211\n",
      "Loss: 156.580, Residuals: 0.211\n",
      "Loss: 156.580, Residuals: 0.211\n",
      "Loss: 156.580, Residuals: 0.211\n",
      "Evidence -86.972\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.85e+00\n",
      "Loss: 156.648, Residuals: 0.211\n",
      "Loss: 156.647, Residuals: 0.209\n",
      "Loss: 156.645, Residuals: 0.210\n",
      "Loss: 156.645, Residuals: 0.210\n",
      "Loss: 156.645, Residuals: 0.210\n",
      "Loss: 156.645, Residuals: 0.210\n",
      "Loss: 156.645, Residuals: 0.210\n",
      "Evidence -86.870\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.85e+00\n",
      "Loss: 156.702, Residuals: 0.210\n",
      "Loss: 156.700, Residuals: 0.209\n",
      "Loss: 156.700, Residuals: 0.209\n",
      "Loss: 156.699, Residuals: 0.209\n",
      "Loss: 156.699, Residuals: 0.209\n",
      "Loss: 156.699, Residuals: 0.209\n",
      "Loss: 156.699, Residuals: 0.209\n",
      "Loss: 156.699, Residuals: 0.209\n",
      "Evidence -86.785\n",
      "Pass count  1\n",
      "Total samples: 40, Updated regularization: 1.00e-05\n",
      "Loss: 410.345, Residuals: -1.824\n",
      "Loss: 314.993, Residuals: -1.436\n",
      "Loss: 274.900, Residuals: -0.633\n",
      "Loss: 252.886, Residuals: 0.455\n",
      "Loss: 183.165, Residuals: -0.111\n",
      "Loss: 170.839, Residuals: 0.073\n",
      "Loss: 167.408, Residuals: -0.000\n",
      "Loss: 142.670, Residuals: -0.172\n",
      "Loss: 139.723, Residuals: -0.029\n",
      "Loss: 134.066, Residuals: 0.012\n",
      "Loss: 123.289, Residuals: 0.084\n",
      "Loss: 115.642, Residuals: 0.247\n",
      "Loss: 115.458, Residuals: 0.298\n",
      "Loss: 113.703, Residuals: 0.287\n",
      "Loss: 110.524, Residuals: 0.267\n",
      "Loss: 107.558, Residuals: 0.500\n",
      "Loss: 107.439, Residuals: 0.471\n",
      "Loss: 103.318, Residuals: 0.396\n",
      "Loss: 103.186, Residuals: 0.402\n",
      "Loss: 101.922, Residuals: 0.363\n",
      "Loss: 99.619, Residuals: 0.316\n",
      "Loss: 95.797, Residuals: 0.248\n",
      "Loss: 95.578, Residuals: 0.228\n",
      "Loss: 93.722, Residuals: 0.239\n",
      "Loss: 90.582, Residuals: 0.137\n",
      "Loss: 90.157, Residuals: 0.236\n",
      "Loss: 90.020, Residuals: 0.193\n",
      "Loss: 88.724, Residuals: 0.167\n",
      "Loss: 86.670, Residuals: 0.164\n",
      "Loss: 86.581, Residuals: 0.183\n",
      "Loss: 85.753, Residuals: 0.188\n",
      "Loss: 84.277, Residuals: 0.200\n",
      "Loss: 83.867, Residuals: 0.196\n",
      "Loss: 83.864, Residuals: 0.192\n",
      "Loss: 82.129, Residuals: 0.169\n",
      "Loss: 82.121, Residuals: 0.174\n",
      "Loss: 80.977, Residuals: 0.167\n",
      "Loss: 80.744, Residuals: 0.183\n",
      "Loss: 78.849, Residuals: 0.167\n",
      "Loss: 78.844, Residuals: 0.158\n",
      "Loss: 78.835, Residuals: 0.151\n",
      "Loss: 78.536, Residuals: 0.145\n",
      "Loss: 78.532, Residuals: 0.142\n",
      "Loss: 77.812, Residuals: 0.127\n",
      "Loss: 77.806, Residuals: 0.113\n",
      "Loss: 76.910, Residuals: 0.109\n",
      "Loss: 76.904, Residuals: 0.096\n",
      "Loss: 76.857, Residuals: 0.097\n",
      "Loss: 76.420, Residuals: 0.092\n",
      "Loss: 76.419, Residuals: 0.091\n",
      "Loss: 75.856, Residuals: 0.085\n",
      "Loss: 75.850, Residuals: 0.070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 75.795, Residuals: 0.073\n",
      "Loss: 75.696, Residuals: 0.078\n",
      "Loss: 75.539, Residuals: 0.087\n",
      "Loss: 75.537, Residuals: 0.089\n",
      "Loss: 75.142, Residuals: 0.085\n",
      "Loss: 75.141, Residuals: 0.083\n",
      "Loss: 74.472, Residuals: 0.085\n",
      "Loss: 74.470, Residuals: 0.080\n",
      "Loss: 74.467, Residuals: 0.075\n",
      "Loss: 74.443, Residuals: 0.076\n",
      "Loss: 74.422, Residuals: 0.074\n",
      "Loss: 74.407, Residuals: 0.078\n",
      "Loss: 74.276, Residuals: 0.069\n",
      "Loss: 74.269, Residuals: 0.077\n",
      "Loss: 74.000, Residuals: 0.076\n",
      "Loss: 73.999, Residuals: 0.070\n",
      "Loss: 73.566, Residuals: 0.073\n",
      "Loss: 73.566, Residuals: 0.069\n",
      "Evidence -542.066\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.54e-02\n",
      "Loss: 107.333, Residuals: 0.011\n",
      "Loss: 107.088, Residuals: -0.019\n",
      "Loss: 105.101, Residuals: -0.013\n",
      "Loss: 105.079, Residuals: -0.002\n",
      "Loss: 104.245, Residuals: 0.006\n",
      "Loss: 102.853, Residuals: 0.012\n",
      "Loss: 102.682, Residuals: 0.005\n",
      "Loss: 101.414, Residuals: 0.028\n",
      "Loss: 101.387, Residuals: 0.027\n",
      "Loss: 101.343, Residuals: 0.034\n",
      "Loss: 100.939, Residuals: 0.050\n",
      "Loss: 100.846, Residuals: 0.069\n",
      "Loss: 100.669, Residuals: 0.074\n",
      "Loss: 100.629, Residuals: 0.075\n",
      "Loss: 100.284, Residuals: 0.090\n",
      "Loss: 99.828, Residuals: 0.121\n",
      "Loss: 99.827, Residuals: 0.118\n",
      "Evidence -171.373\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 6.54e-02\n",
      "Loss: 133.125, Residuals: 0.095\n",
      "Loss: 132.978, Residuals: 0.058\n",
      "Loss: 131.802, Residuals: 0.028\n",
      "Loss: 131.788, Residuals: 0.043\n",
      "Loss: 131.281, Residuals: 0.038\n",
      "Loss: 130.504, Residuals: 0.061\n",
      "Loss: 130.422, Residuals: 0.039\n",
      "Loss: 129.687, Residuals: 0.072\n",
      "Loss: 129.681, Residuals: 0.075\n",
      "Loss: 129.669, Residuals: 0.076\n",
      "Loss: 129.564, Residuals: 0.083\n",
      "Loss: 128.584, Residuals: 0.101\n",
      "Loss: 128.583, Residuals: 0.101\n",
      "Loss: 128.404, Residuals: 0.121\n",
      "Loss: 128.395, Residuals: 0.119\n",
      "Loss: 127.206, Residuals: 0.163\n",
      "Loss: 127.195, Residuals: 0.154\n",
      "Loss: 127.184, Residuals: 0.142\n",
      "Loss: 127.182, Residuals: 0.145\n",
      "Loss: 127.171, Residuals: 0.146\n",
      "Loss: 127.062, Residuals: 0.151\n",
      "Loss: 127.048, Residuals: 0.161\n",
      "Loss: 127.043, Residuals: 0.154\n",
      "Loss: 127.041, Residuals: 0.156\n",
      "Loss: 126.842, Residuals: 0.166\n",
      "Loss: 126.842, Residuals: 0.165\n",
      "Evidence -136.221\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.58e-01\n",
      "Loss: 144.682, Residuals: 0.153\n",
      "Loss: 144.593, Residuals: 0.127\n",
      "Loss: 143.870, Residuals: 0.105\n",
      "Loss: 143.038, Residuals: 0.083\n",
      "Loss: 143.029, Residuals: 0.076\n",
      "Loss: 141.861, Residuals: 0.138\n",
      "Loss: 141.851, Residuals: 0.137\n",
      "Loss: 141.841, Residuals: 0.140\n",
      "Loss: 141.475, Residuals: 0.157\n",
      "Loss: 141.442, Residuals: 0.160\n",
      "Loss: 141.123, Residuals: 0.173\n",
      "Loss: 140.573, Residuals: 0.200\n",
      "Loss: 140.572, Residuals: 0.198\n",
      "Evidence -118.825\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.45e-01\n",
      "Loss: 149.732, Residuals: 0.201\n",
      "Loss: 149.556, Residuals: 0.192\n",
      "Loss: 149.266, Residuals: 0.179\n",
      "Loss: 148.731, Residuals: 0.200\n",
      "Loss: 148.419, Residuals: 0.181\n",
      "Loss: 148.389, Residuals: 0.194\n",
      "Loss: 148.337, Residuals: 0.198\n",
      "Loss: 147.860, Residuals: 0.222\n",
      "Loss: 147.122, Residuals: 0.266\n",
      "Loss: 147.116, Residuals: 0.264\n",
      "Loss: 147.105, Residuals: 0.264\n",
      "Loss: 147.084, Residuals: 0.269\n",
      "Loss: 146.892, Residuals: 0.272\n",
      "Loss: 146.878, Residuals: 0.275\n",
      "Loss: 146.747, Residuals: 0.277\n",
      "Loss: 146.528, Residuals: 0.285\n",
      "Loss: 146.525, Residuals: 0.278\n",
      "Loss: 146.520, Residuals: 0.279\n",
      "Loss: 146.509, Residuals: 0.280\n",
      "Loss: 146.492, Residuals: 0.282\n",
      "Loss: 146.492, Residuals: 0.281\n",
      "Evidence -111.162\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 3.89e-01\n",
      "Loss: 152.324, Residuals: 0.291\n",
      "Loss: 152.220, Residuals: 0.287\n",
      "Loss: 152.054, Residuals: 0.269\n",
      "Loss: 151.798, Residuals: 0.267\n",
      "Loss: 151.348, Residuals: 0.297\n",
      "Loss: 151.332, Residuals: 0.299\n",
      "Loss: 150.782, Residuals: 0.332\n",
      "Loss: 150.682, Residuals: 0.338\n",
      "Loss: 150.676, Residuals: 0.334\n",
      "Loss: 150.446, Residuals: 0.352\n",
      "Loss: 150.442, Residuals: 0.346\n",
      "Loss: 150.404, Residuals: 0.350\n",
      "Loss: 150.369, Residuals: 0.354\n",
      "Loss: 150.365, Residuals: 0.347\n",
      "Loss: 150.323, Residuals: 0.350\n",
      "Loss: 150.321, Residuals: 0.351\n",
      "Loss: 150.260, Residuals: 0.355\n",
      "Loss: 150.219, Residuals: 0.360\n",
      "Loss: 150.214, Residuals: 0.356\n",
      "Loss: 150.209, Residuals: 0.357\n",
      "Loss: 150.207, Residuals: 0.354\n",
      "Loss: 150.181, Residuals: 0.356\n",
      "Loss: 150.151, Residuals: 0.362\n",
      "Loss: 150.147, Residuals: 0.362\n",
      "Loss: 150.142, Residuals: 0.359\n",
      "Loss: 150.133, Residuals: 0.360\n",
      "Loss: 150.116, Residuals: 0.362\n",
      "Loss: 150.116, Residuals: 0.360\n",
      "Loss: 150.098, Residuals: 0.364\n",
      "Loss: 150.096, Residuals: 0.363\n",
      "Loss: 150.092, Residuals: 0.361\n",
      "Loss: 150.092, Residuals: 0.360\n",
      "Loss: 150.086, Residuals: 0.362\n",
      "Loss: 150.083, Residuals: 0.363\n",
      "Loss: 150.082, Residuals: 0.361\n",
      "Loss: 150.079, Residuals: 0.362\n",
      "Loss: 150.079, Residuals: 0.363\n",
      "Loss: 150.073, Residuals: 0.364\n",
      "Loss: 150.072, Residuals: 0.363\n",
      "Loss: 150.071, Residuals: 0.364\n",
      "Loss: 150.069, Residuals: 0.364\n",
      "Loss: 150.069, Residuals: 0.364\n",
      "Loss: 150.067, Residuals: 0.364\n",
      "Loss: 150.066, Residuals: 0.364\n",
      "Loss: 150.066, Residuals: 0.364\n",
      "Evidence -107.064\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 6.07e-01\n",
      "Loss: 153.803, Residuals: 0.370\n",
      "Loss: 153.361, Residuals: 0.338\n",
      "Loss: 152.620, Residuals: 0.358\n",
      "Loss: 152.348, Residuals: 0.332\n",
      "Loss: 151.858, Residuals: 0.363\n",
      "Loss: 151.819, Residuals: 0.368\n",
      "Loss: 151.444, Residuals: 0.378\n",
      "Loss: 150.790, Residuals: 0.407\n",
      "Loss: 150.725, Residuals: 0.394\n",
      "Loss: 150.606, Residuals: 0.399\n",
      "Loss: 150.396, Residuals: 0.404\n",
      "Loss: 150.290, Residuals: 0.387\n",
      "Loss: 150.099, Residuals: 0.403\n",
      "Loss: 149.996, Residuals: 0.406\n",
      "Loss: 149.983, Residuals: 0.406\n",
      "Loss: 149.868, Residuals: 0.415\n",
      "Loss: 149.757, Residuals: 0.434\n",
      "Loss: 149.744, Residuals: 0.436\n",
      "Loss: 149.717, Residuals: 0.436\n",
      "Loss: 149.671, Residuals: 0.437\n",
      "Loss: 149.663, Residuals: 0.426\n",
      "Loss: 149.647, Residuals: 0.428\n",
      "Loss: 149.622, Residuals: 0.432\n",
      "Loss: 149.616, Residuals: 0.430\n",
      "Loss: 149.605, Residuals: 0.433\n",
      "Loss: 149.593, Residuals: 0.442\n",
      "Loss: 149.592, Residuals: 0.442\n",
      "Loss: 149.591, Residuals: 0.439\n",
      "Loss: 149.591, Residuals: 0.439\n",
      "Loss: 149.590, Residuals: 0.439\n",
      "Loss: 149.590, Residuals: 0.439\n",
      "Loss: 149.589, Residuals: 0.439\n",
      "Loss: 149.589, Residuals: 0.440\n",
      "Evidence -104.921\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 9.10e-01\n",
      "Loss: 152.000, Residuals: 0.425\n",
      "Loss: 151.104, Residuals: 0.410\n",
      "Loss: 149.818, Residuals: 0.413\n",
      "Loss: 149.735, Residuals: 0.425\n",
      "Loss: 149.579, Residuals: 0.434\n",
      "Loss: 149.299, Residuals: 0.444\n",
      "Loss: 148.897, Residuals: 0.462\n",
      "Loss: 148.846, Residuals: 0.460\n",
      "Loss: 148.765, Residuals: 0.463\n",
      "Loss: 148.745, Residuals: 0.473\n",
      "Loss: 148.713, Residuals: 0.474\n",
      "Loss: 148.702, Residuals: 0.479\n",
      "Loss: 148.687, Residuals: 0.479\n",
      "Loss: 148.684, Residuals: 0.483\n",
      "Loss: 148.679, Residuals: 0.485\n",
      "Loss: 148.676, Residuals: 0.487\n",
      "Loss: 148.674, Residuals: 0.486\n",
      "Loss: 148.674, Residuals: 0.488\n",
      "Loss: 148.674, Residuals: 0.488\n",
      "Loss: 148.674, Residuals: 0.488\n",
      "Loss: 148.673, Residuals: 0.489\n",
      "Loss: 148.673, Residuals: 0.490\n",
      "Loss: 148.672, Residuals: 0.491\n",
      "Loss: 148.672, Residuals: 0.491\n",
      "Loss: 148.672, Residuals: 0.492\n",
      "Loss: 148.672, Residuals: 0.492\n",
      "Evidence -96.896\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.09e+00\n",
      "Loss: 151.516, Residuals: 0.483\n",
      "Loss: 150.937, Residuals: 0.449\n",
      "Loss: 150.513, Residuals: 0.517\n",
      "Loss: 150.506, Residuals: 0.533\n",
      "Loss: 150.449, Residuals: 0.528\n",
      "Loss: 150.374, Residuals: 0.518\n",
      "Loss: 150.371, Residuals: 0.524\n",
      "Loss: 150.364, Residuals: 0.525\n",
      "Loss: 150.351, Residuals: 0.526\n",
      "Loss: 150.337, Residuals: 0.529\n",
      "Loss: 150.336, Residuals: 0.530\n",
      "Loss: 150.335, Residuals: 0.529\n",
      "Loss: 150.334, Residuals: 0.529\n",
      "Loss: 150.332, Residuals: 0.530\n",
      "Loss: 150.332, Residuals: 0.530\n",
      "Evidence -91.647\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.16e+00\n",
      "Loss: 152.561, Residuals: 0.511\n",
      "Loss: 152.305, Residuals: 0.525\n",
      "Loss: 152.157, Residuals: 0.572\n",
      "Loss: 152.150, Residuals: 0.564\n",
      "Loss: 152.137, Residuals: 0.564\n",
      "Loss: 152.115, Residuals: 0.564\n",
      "Loss: 152.099, Residuals: 0.566\n",
      "Loss: 152.098, Residuals: 0.567\n",
      "Loss: 152.098, Residuals: 0.565\n",
      "Loss: 152.097, Residuals: 0.565\n",
      "Loss: 152.095, Residuals: 0.565\n",
      "Loss: 152.094, Residuals: 0.566\n",
      "Loss: 152.094, Residuals: 0.566\n",
      "Evidence -88.899\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.20e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 153.661, Residuals: 0.557\n",
      "Loss: 153.563, Residuals: 0.567\n",
      "Loss: 153.479, Residuals: 0.599\n",
      "Loss: 153.474, Residuals: 0.599\n",
      "Loss: 153.468, Residuals: 0.595\n",
      "Loss: 153.458, Residuals: 0.595\n",
      "Loss: 153.451, Residuals: 0.594\n",
      "Loss: 153.451, Residuals: 0.593\n",
      "Loss: 153.450, Residuals: 0.592\n",
      "Loss: 153.449, Residuals: 0.592\n",
      "Loss: 153.448, Residuals: 0.591\n",
      "Loss: 153.448, Residuals: 0.591\n",
      "Loss: 153.448, Residuals: 0.591\n",
      "Evidence -87.419\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.21e+00\n",
      "Loss: 154.474, Residuals: 0.593\n",
      "Loss: 154.424, Residuals: 0.605\n",
      "Loss: 154.402, Residuals: 0.613\n",
      "Loss: 154.396, Residuals: 0.609\n",
      "Loss: 154.392, Residuals: 0.609\n",
      "Loss: 154.392, Residuals: 0.609\n",
      "Loss: 154.392, Residuals: 0.610\n",
      "Loss: 154.391, Residuals: 0.610\n",
      "Loss: 154.391, Residuals: 0.609\n",
      "Loss: 154.391, Residuals: 0.607\n",
      "Loss: 154.391, Residuals: 0.607\n",
      "Evidence -86.588\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.21e+00\n",
      "Loss: 155.074, Residuals: 0.606\n",
      "Loss: 155.052, Residuals: 0.611\n",
      "Loss: 155.031, Residuals: 0.625\n",
      "Loss: 155.027, Residuals: 0.620\n",
      "Loss: 155.024, Residuals: 0.620\n",
      "Loss: 155.023, Residuals: 0.620\n",
      "Loss: 155.023, Residuals: 0.620\n",
      "Loss: 155.023, Residuals: 0.620\n",
      "Loss: 155.022, Residuals: 0.619\n",
      "Loss: 155.022, Residuals: 0.618\n",
      "Loss: 155.022, Residuals: 0.618\n",
      "Loss: 155.022, Residuals: 0.617\n",
      "Loss: 155.022, Residuals: 0.617\n",
      "Evidence -86.081\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.21e+00\n",
      "Loss: 155.477, Residuals: 0.617\n",
      "Loss: 155.466, Residuals: 0.620\n",
      "Loss: 155.452, Residuals: 0.627\n",
      "Loss: 155.448, Residuals: 0.626\n",
      "Loss: 155.448, Residuals: 0.627\n",
      "Loss: 155.448, Residuals: 0.627\n",
      "Loss: 155.447, Residuals: 0.626\n",
      "Loss: 155.446, Residuals: 0.625\n",
      "Loss: 155.446, Residuals: 0.624\n",
      "Loss: 155.446, Residuals: 0.624\n",
      "Loss: 155.446, Residuals: 0.624\n",
      "Loss: 155.446, Residuals: 0.622\n",
      "Loss: 155.446, Residuals: 0.622\n",
      "Evidence -85.735\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.20e+00\n",
      "Loss: 155.756, Residuals: 0.623\n",
      "Loss: 155.750, Residuals: 0.624\n",
      "Loss: 155.741, Residuals: 0.629\n",
      "Loss: 155.737, Residuals: 0.629\n",
      "Loss: 155.737, Residuals: 0.630\n",
      "Loss: 155.736, Residuals: 0.629\n",
      "Loss: 155.736, Residuals: 0.628\n",
      "Loss: 155.736, Residuals: 0.627\n",
      "Loss: 155.736, Residuals: 0.627\n",
      "Evidence -85.502\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.20e+00\n",
      "Loss: 155.954, Residuals: 0.629\n",
      "Loss: 155.949, Residuals: 0.634\n",
      "Loss: 155.945, Residuals: 0.631\n",
      "Loss: 155.945, Residuals: 0.631\n",
      "Loss: 155.944, Residuals: 0.630\n",
      "Loss: 155.943, Residuals: 0.627\n",
      "Evidence -85.321\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.19e+00\n",
      "Loss: 156.095, Residuals: 0.631\n",
      "Loss: 156.091, Residuals: 0.631\n",
      "Loss: 156.090, Residuals: 0.631\n",
      "Loss: 156.090, Residuals: 0.631\n",
      "Loss: 156.088, Residuals: 0.631\n",
      "Loss: 156.087, Residuals: 0.630\n",
      "Loss: 156.087, Residuals: 0.630\n",
      "Loss: 156.087, Residuals: 0.630\n",
      "Loss: 156.086, Residuals: 0.629\n",
      "Loss: 156.086, Residuals: 0.628\n",
      "Loss: 156.086, Residuals: 0.628\n",
      "Loss: 156.086, Residuals: 0.627\n",
      "Loss: 156.086, Residuals: 0.627\n",
      "Loss: 156.086, Residuals: 0.627\n",
      "Evidence -85.165\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.19e+00\n",
      "Loss: 156.202, Residuals: 0.629\n",
      "Loss: 156.197, Residuals: 0.632\n",
      "Loss: 156.197, Residuals: 0.631\n",
      "Loss: 156.197, Residuals: 0.631\n",
      "Loss: 156.196, Residuals: 0.630\n",
      "Loss: 156.196, Residuals: 0.628\n",
      "Loss: 156.196, Residuals: 0.628\n",
      "Loss: 156.195, Residuals: 0.628\n",
      "Loss: 156.195, Residuals: 0.628\n",
      "Loss: 156.195, Residuals: 0.626\n",
      "Loss: 156.195, Residuals: 0.626\n",
      "Loss: 156.195, Residuals: 0.626\n",
      "Loss: 156.195, Residuals: 0.626\n",
      "Evidence -85.044\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.18e+00\n",
      "Loss: 156.283, Residuals: 0.627\n",
      "Loss: 156.280, Residuals: 0.629\n",
      "Loss: 156.279, Residuals: 0.630\n",
      "Loss: 156.279, Residuals: 0.629\n",
      "Loss: 156.278, Residuals: 0.629\n",
      "Loss: 156.278, Residuals: 0.627\n",
      "Loss: 156.278, Residuals: 0.627\n",
      "Loss: 156.278, Residuals: 0.627\n",
      "Loss: 156.278, Residuals: 0.626\n",
      "Loss: 156.278, Residuals: 0.626\n",
      "Loss: 156.278, Residuals: 0.625\n",
      "Evidence -84.945\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.17e+00\n",
      "Loss: 156.349, Residuals: 0.628\n",
      "Loss: 156.346, Residuals: 0.628\n",
      "Loss: 156.345, Residuals: 0.627\n",
      "Loss: 156.344, Residuals: 0.627\n",
      "Loss: 156.344, Residuals: 0.627\n",
      "Loss: 156.344, Residuals: 0.626\n",
      "Loss: 156.344, Residuals: 0.625\n",
      "Evidence -84.866\n",
      "Pass count  1\n",
      "Total samples: 37, Updated regularization: 1.00e-05\n",
      "Loss: 397.640, Residuals: -1.900\n",
      "Loss: 302.690, Residuals: -1.634\n",
      "Loss: 258.798, Residuals: -0.542\n",
      "Loss: 239.828, Residuals: 0.467\n",
      "Loss: 168.673, Residuals: -0.161\n",
      "Loss: 164.031, Residuals: 0.216\n",
      "Loss: 156.031, Residuals: 0.288\n",
      "Loss: 142.614, Residuals: 0.504\n",
      "Loss: 127.279, Residuals: 0.547\n",
      "Loss: 124.897, Residuals: 0.739\n",
      "Loss: 120.455, Residuals: 0.639\n",
      "Loss: 112.906, Residuals: 0.480\n",
      "Loss: 112.404, Residuals: 0.545\n",
      "Loss: 107.957, Residuals: 0.477\n",
      "Loss: 101.418, Residuals: 0.360\n",
      "Loss: 100.187, Residuals: 0.280\n",
      "Loss: 100.082, Residuals: 0.331\n",
      "Loss: 99.083, Residuals: 0.301\n",
      "Loss: 97.262, Residuals: 0.243\n",
      "Loss: 96.429, Residuals: 0.233\n",
      "Loss: 96.294, Residuals: 0.282\n",
      "Loss: 95.105, Residuals: 0.197\n",
      "Loss: 95.063, Residuals: 0.215\n",
      "Loss: 93.443, Residuals: 0.198\n",
      "Loss: 90.928, Residuals: 0.207\n",
      "Loss: 90.763, Residuals: 0.243\n",
      "Loss: 89.372, Residuals: 0.224\n",
      "Loss: 86.913, Residuals: 0.204\n",
      "Loss: 85.379, Residuals: 0.277\n",
      "Loss: 85.296, Residuals: 0.296\n",
      "Loss: 82.197, Residuals: 0.264\n",
      "Loss: 79.477, Residuals: 0.202\n",
      "Loss: 79.248, Residuals: 0.268\n",
      "Loss: 77.230, Residuals: 0.258\n",
      "Loss: 76.849, Residuals: 0.302\n",
      "Loss: 73.850, Residuals: 0.271\n",
      "Loss: 73.814, Residuals: 0.254\n",
      "Loss: 72.488, Residuals: 0.217\n",
      "Loss: 72.462, Residuals: 0.212\n",
      "Loss: 71.468, Residuals: 0.199\n",
      "Loss: 71.461, Residuals: 0.184\n",
      "Loss: 70.393, Residuals: 0.172\n",
      "Loss: 70.388, Residuals: 0.170\n",
      "Loss: 70.163, Residuals: 0.163\n",
      "Loss: 69.738, Residuals: 0.157\n",
      "Loss: 68.986, Residuals: 0.145\n",
      "Loss: 68.976, Residuals: 0.132\n",
      "Loss: 68.880, Residuals: 0.130\n",
      "Loss: 68.708, Residuals: 0.128\n",
      "Loss: 68.644, Residuals: 0.124\n",
      "Loss: 68.071, Residuals: 0.117\n",
      "Loss: 68.038, Residuals: 0.124\n",
      "Loss: 67.724, Residuals: 0.110\n",
      "Loss: 67.503, Residuals: 0.107\n",
      "Loss: 67.494, Residuals: 0.116\n",
      "Loss: 67.485, Residuals: 0.132\n",
      "Loss: 67.404, Residuals: 0.133\n",
      "Loss: 66.720, Residuals: 0.125\n",
      "Loss: 66.716, Residuals: 0.115\n",
      "Loss: 66.122, Residuals: 0.078\n",
      "Loss: 66.115, Residuals: 0.093\n",
      "Loss: 66.104, Residuals: 0.092\n",
      "Loss: 66.086, Residuals: 0.091\n",
      "Loss: 65.480, Residuals: 0.091\n",
      "Loss: 65.471, Residuals: 0.075\n",
      "Loss: 65.456, Residuals: 0.073\n",
      "Loss: 65.336, Residuals: 0.073\n",
      "Loss: 65.335, Residuals: 0.077\n",
      "Loss: 65.184, Residuals: 0.070\n",
      "Loss: 65.182, Residuals: 0.076\n",
      "Loss: 64.920, Residuals: 0.074\n",
      "Loss: 64.914, Residuals: 0.085\n",
      "Loss: 64.861, Residuals: 0.088\n",
      "Loss: 64.833, Residuals: 0.095\n",
      "Loss: 64.831, Residuals: 0.096\n",
      "Loss: 64.523, Residuals: 0.096\n",
      "Loss: 64.520, Residuals: 0.088\n",
      "Loss: 64.520, Residuals: 0.086\n",
      "Loss: 64.050, Residuals: 0.065\n",
      "Loss: 64.043, Residuals: 0.075\n",
      "Loss: 64.041, Residuals: 0.081\n",
      "Loss: 64.037, Residuals: 0.084\n",
      "Loss: 64.032, Residuals: 0.089\n",
      "Evidence -497.096\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.06e-02\n",
      "Loss: 92.330, Residuals: -0.006\n",
      "Loss: 92.086, Residuals: 0.044\n",
      "Loss: 91.646, Residuals: 0.044\n",
      "Loss: 90.903, Residuals: 0.012\n",
      "Loss: 90.901, Residuals: 0.015\n",
      "Loss: 89.965, Residuals: 0.011\n",
      "Loss: 89.961, Residuals: 0.016\n",
      "Loss: 89.355, Residuals: 0.024\n",
      "Loss: 89.354, Residuals: 0.026\n",
      "Loss: 88.754, Residuals: 0.049\n",
      "Loss: 88.748, Residuals: 0.044\n",
      "Loss: 87.907, Residuals: 0.089\n",
      "Loss: 87.894, Residuals: 0.083\n",
      "Loss: 87.875, Residuals: 0.076\n",
      "Loss: 87.856, Residuals: 0.077\n",
      "Loss: 87.172, Residuals: 0.096\n",
      "Loss: 87.171, Residuals: 0.097\n",
      "Loss: 86.761, Residuals: 0.100\n",
      "Loss: 86.703, Residuals: 0.082\n",
      "Loss: 86.702, Residuals: 0.083\n",
      "Evidence -160.009\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 3.52e-02\n",
      "Loss: 122.621, Residuals: 0.061\n",
      "Loss: 122.405, Residuals: 0.051\n",
      "Loss: 120.774, Residuals: 0.013\n",
      "Loss: 120.754, Residuals: 0.023\n",
      "Loss: 120.715, Residuals: 0.022\n",
      "Loss: 119.313, Residuals: 0.056\n",
      "Loss: 119.267, Residuals: 0.043\n",
      "Loss: 117.481, Residuals: 0.072\n",
      "Loss: 117.479, Residuals: 0.075\n",
      "Loss: 116.664, Residuals: 0.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 116.661, Residuals: 0.086\n",
      "Loss: 114.970, Residuals: 0.127\n",
      "Loss: 114.966, Residuals: 0.128\n",
      "Loss: 114.958, Residuals: 0.130\n",
      "Loss: 114.945, Residuals: 0.132\n",
      "Loss: 113.182, Residuals: 0.149\n",
      "Loss: 113.120, Residuals: 0.129\n",
      "Loss: 113.093, Residuals: 0.138\n",
      "Loss: 113.055, Residuals: 0.147\n",
      "Loss: 112.982, Residuals: 0.148\n",
      "Loss: 112.846, Residuals: 0.146\n",
      "Loss: 112.834, Residuals: 0.150\n",
      "Loss: 112.382, Residuals: 0.156\n",
      "Loss: 112.380, Residuals: 0.153\n",
      "Loss: 112.361, Residuals: 0.157\n",
      "Loss: 112.326, Residuals: 0.157\n",
      "Loss: 112.008, Residuals: 0.161\n",
      "Loss: 112.007, Residuals: 0.159\n",
      "Evidence -128.497\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.23e-01\n",
      "Loss: 134.084, Residuals: 0.132\n",
      "Loss: 133.867, Residuals: 0.127\n",
      "Loss: 133.455, Residuals: 0.122\n",
      "Loss: 132.712, Residuals: 0.121\n",
      "Loss: 131.556, Residuals: 0.128\n",
      "Loss: 131.551, Residuals: 0.127\n",
      "Loss: 130.708, Residuals: 0.140\n",
      "Loss: 129.270, Residuals: 0.179\n",
      "Loss: 129.261, Residuals: 0.174\n",
      "Loss: 129.252, Residuals: 0.175\n",
      "Loss: 127.973, Residuals: 0.208\n",
      "Loss: 127.968, Residuals: 0.205\n",
      "Loss: 127.921, Residuals: 0.206\n",
      "Loss: 127.845, Residuals: 0.211\n",
      "Loss: 127.753, Residuals: 0.222\n",
      "Loss: 126.964, Residuals: 0.235\n",
      "Loss: 126.960, Residuals: 0.229\n",
      "Loss: 126.922, Residuals: 0.230\n",
      "Loss: 126.862, Residuals: 0.230\n",
      "Loss: 126.862, Residuals: 0.231\n",
      "Loss: 126.541, Residuals: 0.238\n",
      "Loss: 126.527, Residuals: 0.241\n",
      "Loss: 126.503, Residuals: 0.239\n",
      "Loss: 126.459, Residuals: 0.239\n",
      "Loss: 126.380, Residuals: 0.241\n",
      "Loss: 126.354, Residuals: 0.233\n",
      "Loss: 126.348, Residuals: 0.236\n",
      "Loss: 126.346, Residuals: 0.235\n",
      "Loss: 126.254, Residuals: 0.239\n",
      "Loss: 126.252, Residuals: 0.242\n",
      "Loss: 126.232, Residuals: 0.242\n",
      "Loss: 126.199, Residuals: 0.243\n",
      "Loss: 126.198, Residuals: 0.242\n",
      "Loss: 126.184, Residuals: 0.243\n",
      "Loss: 126.180, Residuals: 0.242\n",
      "Loss: 126.180, Residuals: 0.242\n",
      "Loss: 126.180, Residuals: 0.241\n",
      "Loss: 126.174, Residuals: 0.242\n",
      "Loss: 126.166, Residuals: 0.242\n",
      "Loss: 126.166, Residuals: 0.243\n",
      "Loss: 126.166, Residuals: 0.243\n",
      "Loss: 126.166, Residuals: 0.243\n",
      "Loss: 126.166, Residuals: 0.244\n",
      "Loss: 126.165, Residuals: 0.243\n",
      "Loss: 126.164, Residuals: 0.243\n",
      "Loss: 126.164, Residuals: 0.244\n",
      "Loss: 126.164, Residuals: 0.244\n",
      "Evidence -112.100\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 3.77e-01\n",
      "Loss: 138.418, Residuals: 0.202\n",
      "Loss: 138.227, Residuals: 0.193\n",
      "Loss: 137.866, Residuals: 0.190\n",
      "Loss: 137.233, Residuals: 0.188\n",
      "Loss: 136.152, Residuals: 0.222\n",
      "Loss: 135.947, Residuals: 0.198\n",
      "Loss: 135.575, Residuals: 0.214\n",
      "Loss: 135.006, Residuals: 0.252\n",
      "Loss: 134.985, Residuals: 0.268\n",
      "Loss: 134.795, Residuals: 0.282\n",
      "Loss: 134.770, Residuals: 0.297\n",
      "Loss: 134.760, Residuals: 0.282\n",
      "Loss: 134.663, Residuals: 0.289\n",
      "Loss: 134.505, Residuals: 0.303\n",
      "Loss: 134.498, Residuals: 0.314\n",
      "Loss: 134.496, Residuals: 0.310\n",
      "Loss: 134.429, Residuals: 0.313\n",
      "Loss: 134.428, Residuals: 0.315\n",
      "Loss: 134.396, Residuals: 0.315\n",
      "Loss: 134.375, Residuals: 0.314\n",
      "Loss: 134.375, Residuals: 0.315\n",
      "Loss: 134.374, Residuals: 0.315\n",
      "Loss: 134.373, Residuals: 0.314\n",
      "Loss: 134.373, Residuals: 0.314\n",
      "Loss: 134.373, Residuals: 0.314\n",
      "Loss: 134.373, Residuals: 0.314\n",
      "Loss: 134.373, Residuals: 0.314\n",
      "Loss: 134.373, Residuals: 0.314\n",
      "Evidence -97.838\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 6.55e-01\n",
      "Loss: 141.011, Residuals: 0.220\n",
      "Loss: 140.865, Residuals: 0.249\n",
      "Loss: 140.786, Residuals: 0.247\n",
      "Loss: 140.184, Residuals: 0.292\n",
      "Loss: 139.666, Residuals: 0.397\n",
      "Loss: 139.655, Residuals: 0.399\n",
      "Loss: 139.641, Residuals: 0.413\n",
      "Loss: 139.615, Residuals: 0.408\n",
      "Loss: 139.570, Residuals: 0.400\n",
      "Loss: 139.532, Residuals: 0.375\n",
      "Loss: 139.527, Residuals: 0.383\n",
      "Loss: 139.519, Residuals: 0.383\n",
      "Loss: 139.504, Residuals: 0.379\n",
      "Loss: 139.500, Residuals: 0.377\n",
      "Loss: 139.469, Residuals: 0.378\n",
      "Loss: 139.446, Residuals: 0.380\n",
      "Loss: 139.444, Residuals: 0.376\n",
      "Loss: 139.443, Residuals: 0.377\n",
      "Loss: 139.443, Residuals: 0.379\n",
      "Loss: 139.443, Residuals: 0.379\n",
      "Loss: 139.442, Residuals: 0.380\n",
      "Loss: 139.442, Residuals: 0.380\n",
      "Loss: 139.442, Residuals: 0.381\n",
      "Loss: 139.442, Residuals: 0.380\n",
      "Loss: 139.442, Residuals: 0.380\n",
      "Evidence -91.827\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 8.81e-01\n",
      "Loss: 142.529, Residuals: 0.297\n",
      "Loss: 141.923, Residuals: 0.336\n",
      "Loss: 141.921, Residuals: 0.333\n",
      "Loss: 141.630, Residuals: 0.370\n",
      "Loss: 141.590, Residuals: 0.388\n",
      "Loss: 141.517, Residuals: 0.395\n",
      "Loss: 141.401, Residuals: 0.410\n",
      "Loss: 141.397, Residuals: 0.409\n",
      "Loss: 141.361, Residuals: 0.413\n",
      "Loss: 141.319, Residuals: 0.422\n",
      "Loss: 141.317, Residuals: 0.419\n",
      "Loss: 141.315, Residuals: 0.419\n",
      "Loss: 141.314, Residuals: 0.417\n",
      "Loss: 141.314, Residuals: 0.416\n",
      "Loss: 141.313, Residuals: 0.417\n",
      "Loss: 141.312, Residuals: 0.417\n",
      "Loss: 141.312, Residuals: 0.416\n",
      "Loss: 141.312, Residuals: 0.417\n",
      "Loss: 141.312, Residuals: 0.418\n",
      "Loss: 141.312, Residuals: 0.418\n",
      "Evidence -89.547\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.12e+00\n",
      "Loss: 143.234, Residuals: 0.345\n",
      "Loss: 142.765, Residuals: 0.367\n",
      "Loss: 142.761, Residuals: 0.364\n",
      "Loss: 142.628, Residuals: 0.391\n",
      "Loss: 142.448, Residuals: 0.436\n",
      "Loss: 142.438, Residuals: 0.450\n",
      "Loss: 142.420, Residuals: 0.451\n",
      "Loss: 142.391, Residuals: 0.451\n",
      "Loss: 142.387, Residuals: 0.446\n",
      "Loss: 142.381, Residuals: 0.449\n",
      "Loss: 142.374, Residuals: 0.453\n",
      "Loss: 142.374, Residuals: 0.454\n",
      "Loss: 142.373, Residuals: 0.454\n",
      "Loss: 142.373, Residuals: 0.454\n",
      "Loss: 142.373, Residuals: 0.454\n",
      "Loss: 142.373, Residuals: 0.455\n",
      "Loss: 142.372, Residuals: 0.456\n",
      "Loss: 142.372, Residuals: 0.457\n",
      "Loss: 142.372, Residuals: 0.457\n",
      "Evidence -88.110\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.34e+00\n",
      "Loss: 143.746, Residuals: 0.414\n",
      "Loss: 143.694, Residuals: 0.418\n",
      "Loss: 143.611, Residuals: 0.430\n",
      "Loss: 143.485, Residuals: 0.456\n",
      "Loss: 143.381, Residuals: 0.489\n",
      "Loss: 143.371, Residuals: 0.494\n",
      "Loss: 143.364, Residuals: 0.488\n",
      "Loss: 143.352, Residuals: 0.491\n",
      "Loss: 143.352, Residuals: 0.491\n",
      "Loss: 143.350, Residuals: 0.491\n",
      "Loss: 143.349, Residuals: 0.489\n",
      "Loss: 143.348, Residuals: 0.489\n",
      "Loss: 143.348, Residuals: 0.492\n",
      "Loss: 143.348, Residuals: 0.492\n",
      "Loss: 143.348, Residuals: 0.492\n",
      "Evidence -87.358\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.51e+00\n",
      "Loss: 144.461, Residuals: 0.446\n",
      "Loss: 144.397, Residuals: 0.480\n",
      "Loss: 144.300, Residuals: 0.494\n",
      "Loss: 144.228, Residuals: 0.519\n",
      "Loss: 144.217, Residuals: 0.520\n",
      "Loss: 144.208, Residuals: 0.513\n",
      "Loss: 144.201, Residuals: 0.524\n",
      "Loss: 144.201, Residuals: 0.523\n",
      "Evidence -86.814\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.63e+00\n",
      "Loss: 145.082, Residuals: 0.502\n",
      "Loss: 145.056, Residuals: 0.518\n",
      "Loss: 145.018, Residuals: 0.529\n",
      "Loss: 144.975, Residuals: 0.545\n",
      "Loss: 144.974, Residuals: 0.544\n",
      "Evidence -86.382\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.71e+00\n",
      "Loss: 145.538, Residuals: 0.541\n",
      "Loss: 145.519, Residuals: 0.551\n",
      "Loss: 145.493, Residuals: 0.562\n",
      "Loss: 145.490, Residuals: 0.560\n",
      "Loss: 145.485, Residuals: 0.562\n",
      "Loss: 145.478, Residuals: 0.568\n",
      "Loss: 145.478, Residuals: 0.569\n",
      "Loss: 145.477, Residuals: 0.569\n",
      "Loss: 145.477, Residuals: 0.570\n",
      "Loss: 145.476, Residuals: 0.570\n",
      "Loss: 145.476, Residuals: 0.570\n",
      "Loss: 145.476, Residuals: 0.571\n",
      "Loss: 145.476, Residuals: 0.571\n",
      "Evidence -86.253\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.77e+00\n",
      "Loss: 145.866, Residuals: 0.568\n",
      "Loss: 145.838, Residuals: 0.589\n",
      "Loss: 145.837, Residuals: 0.592\n",
      "Loss: 145.835, Residuals: 0.591\n",
      "Loss: 145.832, Residuals: 0.590\n",
      "Loss: 145.830, Residuals: 0.585\n",
      "Loss: 145.829, Residuals: 0.591\n",
      "Loss: 145.828, Residuals: 0.591\n",
      "Loss: 145.828, Residuals: 0.592\n",
      "Loss: 145.828, Residuals: 0.592\n",
      "Loss: 145.828, Residuals: 0.592\n",
      "Loss: 145.828, Residuals: 0.592\n",
      "Loss: 145.828, Residuals: 0.592\n",
      "Evidence -86.210\n",
      "Pass count  1\n",
      "Total samples: 40, Updated regularization: 1.00e-05\n",
      "Loss: 404.110, Residuals: -1.660\n",
      "Loss: 294.382, Residuals: -1.899\n",
      "Loss: 253.182, Residuals: -1.220\n",
      "Loss: 231.243, Residuals: -0.133\n",
      "Loss: 192.089, Residuals: -0.269\n",
      "Loss: 155.757, Residuals: -0.357\n",
      "Loss: 146.191, Residuals: 0.255\n",
      "Loss: 133.192, Residuals: 0.223\n",
      "Loss: 129.366, Residuals: 0.162\n",
      "Loss: 122.393, Residuals: 0.151\n",
      "Loss: 110.684, Residuals: 0.368\n",
      "Loss: 108.286, Residuals: 0.319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 104.257, Residuals: 0.325\n",
      "Loss: 103.804, Residuals: 0.401\n",
      "Loss: 100.034, Residuals: 0.346\n",
      "Loss: 99.926, Residuals: 0.321\n",
      "Loss: 96.219, Residuals: 0.262\n",
      "Loss: 96.186, Residuals: 0.242\n",
      "Loss: 95.897, Residuals: 0.265\n",
      "Loss: 93.517, Residuals: 0.190\n",
      "Loss: 93.074, Residuals: 0.286\n",
      "Loss: 92.232, Residuals: 0.262\n",
      "Loss: 92.222, Residuals: 0.264\n",
      "Loss: 90.809, Residuals: 0.225\n",
      "Loss: 90.805, Residuals: 0.218\n",
      "Loss: 88.573, Residuals: 0.165\n",
      "Loss: 88.547, Residuals: 0.147\n",
      "Loss: 88.508, Residuals: 0.126\n",
      "Loss: 88.212, Residuals: 0.127\n",
      "Loss: 85.860, Residuals: 0.103\n",
      "Loss: 85.818, Residuals: 0.079\n",
      "Loss: 85.762, Residuals: 0.050\n",
      "Loss: 85.659, Residuals: 0.063\n",
      "Loss: 85.465, Residuals: 0.069\n",
      "Loss: 85.112, Residuals: 0.074\n",
      "Loss: 84.945, Residuals: 0.141\n",
      "Loss: 84.943, Residuals: 0.142\n",
      "Loss: 83.494, Residuals: 0.121\n",
      "Loss: 83.429, Residuals: 0.125\n",
      "Loss: 82.925, Residuals: 0.118\n",
      "Loss: 82.788, Residuals: 0.113\n",
      "Loss: 81.641, Residuals: 0.100\n",
      "Loss: 81.548, Residuals: 0.124\n",
      "Loss: 80.666, Residuals: 0.117\n",
      "Loss: 80.662, Residuals: 0.114\n",
      "Loss: 80.084, Residuals: 0.103\n",
      "Loss: 79.867, Residuals: 0.085\n",
      "Loss: 79.854, Residuals: 0.100\n",
      "Loss: 79.406, Residuals: 0.118\n",
      "Loss: 79.383, Residuals: 0.133\n",
      "Loss: 78.539, Residuals: 0.109\n",
      "Loss: 78.536, Residuals: 0.109\n",
      "Loss: 78.419, Residuals: 0.114\n",
      "Loss: 78.200, Residuals: 0.118\n",
      "Loss: 78.128, Residuals: 0.116\n",
      "Loss: 78.103, Residuals: 0.121\n",
      "Loss: 77.232, Residuals: 0.107\n",
      "Loss: 77.226, Residuals: 0.110\n",
      "Loss: 77.014, Residuals: 0.111\n",
      "Loss: 76.976, Residuals: 0.117\n",
      "Loss: 76.953, Residuals: 0.112\n",
      "Loss: 76.929, Residuals: 0.115\n",
      "Loss: 76.723, Residuals: 0.102\n",
      "Loss: 76.346, Residuals: 0.100\n",
      "Loss: 76.343, Residuals: 0.103\n",
      "Loss: 76.243, Residuals: 0.100\n",
      "Loss: 75.458, Residuals: 0.099\n",
      "Loss: 75.446, Residuals: 0.095\n",
      "Loss: 75.423, Residuals: 0.092\n",
      "Loss: 75.383, Residuals: 0.088\n",
      "Loss: 75.309, Residuals: 0.088\n",
      "Loss: 75.195, Residuals: 0.096\n",
      "Loss: 75.184, Residuals: 0.101\n",
      "Loss: 74.754, Residuals: 0.092\n",
      "Loss: 74.751, Residuals: 0.093\n",
      "Loss: 74.644, Residuals: 0.092\n",
      "Loss: 74.556, Residuals: 0.095\n",
      "Loss: 74.537, Residuals: 0.094\n",
      "Loss: 74.396, Residuals: 0.097\n",
      "Loss: 74.393, Residuals: 0.105\n",
      "Loss: 74.388, Residuals: 0.105\n",
      "Loss: 74.380, Residuals: 0.103\n",
      "Loss: 74.379, Residuals: 0.099\n",
      "Loss: 74.163, Residuals: 0.098\n",
      "Loss: 74.162, Residuals: 0.097\n",
      "Evidence -538.512\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 3.89e-02\n",
      "Loss: 107.303, Residuals: 0.055\n",
      "Loss: 107.113, Residuals: 0.064\n",
      "Loss: 106.820, Residuals: 0.037\n",
      "Loss: 106.340, Residuals: 0.040\n",
      "Loss: 105.512, Residuals: 0.053\n",
      "Loss: 104.094, Residuals: 0.052\n",
      "Loss: 104.086, Residuals: 0.058\n",
      "Loss: 103.017, Residuals: 0.060\n",
      "Loss: 103.008, Residuals: 0.063\n",
      "Loss: 102.650, Residuals: 0.064\n",
      "Loss: 102.013, Residuals: 0.066\n",
      "Loss: 101.962, Residuals: 0.081\n",
      "Loss: 101.477, Residuals: 0.088\n",
      "Loss: 101.456, Residuals: 0.077\n",
      "Loss: 100.603, Residuals: 0.090\n",
      "Loss: 100.490, Residuals: 0.098\n",
      "Loss: 100.475, Residuals: 0.094\n",
      "Loss: 99.901, Residuals: 0.104\n",
      "Loss: 99.895, Residuals: 0.102\n",
      "Loss: 99.663, Residuals: 0.116\n",
      "Loss: 99.655, Residuals: 0.116\n",
      "Loss: 99.356, Residuals: 0.123\n",
      "Loss: 99.354, Residuals: 0.117\n",
      "Loss: 99.270, Residuals: 0.121\n",
      "Loss: 99.120, Residuals: 0.130\n",
      "Loss: 99.120, Residuals: 0.127\n",
      "Loss: 99.103, Residuals: 0.127\n",
      "Loss: 99.072, Residuals: 0.126\n",
      "Loss: 99.059, Residuals: 0.129\n",
      "Loss: 98.941, Residuals: 0.134\n",
      "Loss: 98.940, Residuals: 0.131\n",
      "Loss: 98.923, Residuals: 0.131\n",
      "Loss: 98.916, Residuals: 0.128\n",
      "Loss: 98.914, Residuals: 0.129\n",
      "Loss: 98.913, Residuals: 0.129\n",
      "Loss: 98.862, Residuals: 0.131\n",
      "Loss: 98.862, Residuals: 0.130\n",
      "Loss: 98.795, Residuals: 0.136\n",
      "Loss: 98.795, Residuals: 0.134\n",
      "Loss: 98.793, Residuals: 0.135\n",
      "Loss: 98.790, Residuals: 0.135\n",
      "Loss: 98.790, Residuals: 0.133\n",
      "Loss: 98.754, Residuals: 0.138\n",
      "Loss: 98.754, Residuals: 0.137\n",
      "Loss: 98.753, Residuals: 0.137\n",
      "Loss: 98.749, Residuals: 0.138\n",
      "Loss: 98.749, Residuals: 0.137\n",
      "Evidence -167.138\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.66e-01\n",
      "Loss: 131.990, Residuals: 0.122\n",
      "Loss: 131.827, Residuals: 0.096\n",
      "Loss: 131.526, Residuals: 0.092\n",
      "Loss: 131.026, Residuals: 0.083\n",
      "Loss: 130.772, Residuals: 0.063\n",
      "Loss: 130.623, Residuals: 0.087\n",
      "Loss: 130.341, Residuals: 0.084\n",
      "Loss: 129.844, Residuals: 0.080\n",
      "Loss: 128.906, Residuals: 0.099\n",
      "Loss: 128.139, Residuals: 0.105\n",
      "Loss: 128.131, Residuals: 0.095\n",
      "Loss: 128.053, Residuals: 0.095\n",
      "Loss: 127.974, Residuals: 0.114\n",
      "Loss: 127.245, Residuals: 0.133\n",
      "Loss: 126.110, Residuals: 0.182\n",
      "Loss: 126.096, Residuals: 0.168\n",
      "Loss: 126.071, Residuals: 0.170\n",
      "Loss: 126.032, Residuals: 0.181\n",
      "Loss: 125.680, Residuals: 0.189\n",
      "Loss: 125.640, Residuals: 0.190\n",
      "Loss: 125.626, Residuals: 0.177\n",
      "Loss: 125.618, Residuals: 0.186\n",
      "Loss: 125.302, Residuals: 0.201\n",
      "Loss: 125.300, Residuals: 0.193\n",
      "Loss: 125.282, Residuals: 0.190\n",
      "Loss: 125.252, Residuals: 0.188\n",
      "Loss: 125.210, Residuals: 0.177\n",
      "Loss: 125.205, Residuals: 0.178\n",
      "Loss: 125.198, Residuals: 0.179\n",
      "Loss: 125.131, Residuals: 0.184\n",
      "Loss: 125.020, Residuals: 0.195\n",
      "Loss: 125.019, Residuals: 0.191\n",
      "Loss: 125.012, Residuals: 0.190\n",
      "Loss: 125.001, Residuals: 0.187\n",
      "Loss: 124.999, Residuals: 0.186\n",
      "Loss: 124.980, Residuals: 0.189\n",
      "Loss: 124.946, Residuals: 0.194\n",
      "Loss: 124.945, Residuals: 0.192\n",
      "Loss: 124.942, Residuals: 0.191\n",
      "Loss: 124.937, Residuals: 0.189\n",
      "Loss: 124.929, Residuals: 0.187\n",
      "Loss: 124.928, Residuals: 0.186\n",
      "Loss: 124.922, Residuals: 0.187\n",
      "Loss: 124.922, Residuals: 0.187\n",
      "Loss: 124.916, Residuals: 0.188\n",
      "Loss: 124.906, Residuals: 0.191\n",
      "Loss: 124.905, Residuals: 0.191\n",
      "Evidence -129.216\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 3.82e-01\n",
      "Loss: 143.135, Residuals: 0.200\n",
      "Loss: 142.653, Residuals: 0.145\n",
      "Loss: 142.255, Residuals: 0.108\n",
      "Loss: 142.057, Residuals: 0.165\n",
      "Loss: 141.696, Residuals: 0.170\n",
      "Loss: 141.032, Residuals: 0.192\n",
      "Loss: 139.997, Residuals: 0.239\n",
      "Loss: 139.991, Residuals: 0.232\n",
      "Loss: 139.934, Residuals: 0.231\n",
      "Loss: 139.831, Residuals: 0.227\n",
      "Loss: 139.636, Residuals: 0.236\n",
      "Loss: 139.305, Residuals: 0.249\n",
      "Loss: 139.298, Residuals: 0.236\n",
      "Loss: 139.242, Residuals: 0.234\n",
      "Loss: 139.139, Residuals: 0.242\n",
      "Loss: 139.127, Residuals: 0.242\n",
      "Loss: 139.022, Residuals: 0.251\n",
      "Loss: 138.975, Residuals: 0.235\n",
      "Loss: 138.971, Residuals: 0.237\n",
      "Loss: 138.850, Residuals: 0.253\n",
      "Loss: 138.849, Residuals: 0.253\n",
      "Loss: 138.847, Residuals: 0.252\n",
      "Loss: 138.845, Residuals: 0.251\n",
      "Loss: 138.820, Residuals: 0.254\n",
      "Loss: 138.778, Residuals: 0.261\n",
      "Loss: 138.774, Residuals: 0.254\n",
      "Loss: 138.767, Residuals: 0.251\n",
      "Loss: 138.753, Residuals: 0.253\n",
      "Loss: 138.753, Residuals: 0.253\n",
      "Loss: 138.737, Residuals: 0.255\n",
      "Loss: 138.735, Residuals: 0.251\n",
      "Loss: 138.732, Residuals: 0.250\n",
      "Loss: 138.711, Residuals: 0.254\n",
      "Loss: 138.711, Residuals: 0.254\n",
      "Loss: 138.710, Residuals: 0.253\n",
      "Loss: 138.709, Residuals: 0.252\n",
      "Loss: 138.707, Residuals: 0.252\n",
      "Loss: 138.704, Residuals: 0.251\n",
      "Loss: 138.703, Residuals: 0.250\n",
      "Loss: 138.700, Residuals: 0.251\n",
      "Loss: 138.699, Residuals: 0.251\n",
      "Evidence -110.230\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 6.14e-01\n",
      "Loss: 149.099, Residuals: 0.271\n",
      "Loss: 148.709, Residuals: 0.228\n",
      "Loss: 148.001, Residuals: 0.222\n",
      "Loss: 146.861, Residuals: 0.223\n",
      "Loss: 146.693, Residuals: 0.179\n",
      "Loss: 146.373, Residuals: 0.197\n",
      "Loss: 145.859, Residuals: 0.224\n",
      "Loss: 145.839, Residuals: 0.236\n",
      "Loss: 145.166, Residuals: 0.270\n",
      "Loss: 145.130, Residuals: 0.268\n",
      "Loss: 144.836, Residuals: 0.291\n",
      "Loss: 144.770, Residuals: 0.278\n",
      "Loss: 144.760, Residuals: 0.274\n",
      "Loss: 144.677, Residuals: 0.284\n",
      "Loss: 144.556, Residuals: 0.303\n",
      "Loss: 144.554, Residuals: 0.304\n",
      "Loss: 144.552, Residuals: 0.303\n",
      "Loss: 144.549, Residuals: 0.301\n",
      "Loss: 144.521, Residuals: 0.302\n",
      "Loss: 144.520, Residuals: 0.302\n",
      "Loss: 144.504, Residuals: 0.303\n",
      "Loss: 144.501, Residuals: 0.300\n",
      "Loss: 144.501, Residuals: 0.300\n",
      "Loss: 144.494, Residuals: 0.300\n",
      "Loss: 144.494, Residuals: 0.300\n",
      "Evidence -100.916\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 9.24e-01\n",
      "Loss: 151.379, Residuals: 0.283\n",
      "Loss: 150.753, Residuals: 0.277\n",
      "Loss: 149.834, Residuals: 0.305\n",
      "Loss: 149.682, Residuals: 0.286\n",
      "Loss: 149.413, Residuals: 0.303\n",
      "Loss: 149.123, Residuals: 0.366\n",
      "Loss: 149.103, Residuals: 0.368\n",
      "Loss: 149.072, Residuals: 0.362\n",
      "Loss: 149.015, Residuals: 0.363\n",
      "Loss: 148.926, Residuals: 0.365\n",
      "Loss: 148.924, Residuals: 0.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 148.906, Residuals: 0.367\n",
      "Loss: 148.902, Residuals: 0.362\n",
      "Loss: 148.896, Residuals: 0.362\n",
      "Loss: 148.883, Residuals: 0.364\n",
      "Loss: 148.883, Residuals: 0.364\n",
      "Loss: 148.869, Residuals: 0.367\n",
      "Loss: 148.869, Residuals: 0.366\n",
      "Evidence -93.335\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.19e+00\n",
      "Loss: 153.319, Residuals: 0.355\n",
      "Loss: 153.033, Residuals: 0.344\n",
      "Loss: 152.688, Residuals: 0.384\n",
      "Loss: 152.504, Residuals: 0.389\n",
      "Loss: 152.497, Residuals: 0.402\n",
      "Loss: 152.485, Residuals: 0.401\n",
      "Loss: 152.461, Residuals: 0.401\n",
      "Loss: 152.419, Residuals: 0.401\n",
      "Loss: 152.374, Residuals: 0.398\n",
      "Loss: 152.370, Residuals: 0.396\n",
      "Loss: 152.366, Residuals: 0.398\n",
      "Loss: 152.365, Residuals: 0.398\n",
      "Loss: 152.360, Residuals: 0.398\n",
      "Loss: 152.360, Residuals: 0.399\n",
      "Loss: 152.357, Residuals: 0.399\n",
      "Loss: 152.353, Residuals: 0.399\n",
      "Loss: 152.353, Residuals: 0.398\n",
      "Loss: 152.353, Residuals: 0.398\n",
      "Loss: 152.352, Residuals: 0.399\n",
      "Loss: 152.351, Residuals: 0.399\n",
      "Loss: 152.351, Residuals: 0.399\n",
      "Evidence -89.631\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.36e+00\n",
      "Loss: 154.537, Residuals: 0.375\n",
      "Loss: 154.369, Residuals: 0.392\n",
      "Loss: 154.350, Residuals: 0.386\n",
      "Loss: 154.319, Residuals: 0.391\n",
      "Loss: 154.272, Residuals: 0.398\n",
      "Loss: 154.259, Residuals: 0.403\n",
      "Loss: 154.253, Residuals: 0.401\n",
      "Loss: 154.242, Residuals: 0.403\n",
      "Loss: 154.227, Residuals: 0.406\n",
      "Loss: 154.227, Residuals: 0.407\n",
      "Evidence -87.731\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.45e+00\n",
      "Loss: 155.352, Residuals: 0.390\n",
      "Loss: 155.285, Residuals: 0.396\n",
      "Loss: 155.261, Residuals: 0.408\n",
      "Loss: 155.244, Residuals: 0.411\n",
      "Loss: 155.242, Residuals: 0.408\n",
      "Loss: 155.241, Residuals: 0.407\n",
      "Loss: 155.235, Residuals: 0.406\n",
      "Loss: 155.226, Residuals: 0.406\n",
      "Loss: 155.225, Residuals: 0.406\n",
      "Evidence -86.665\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.48e+00\n",
      "Loss: 155.887, Residuals: 0.399\n",
      "Loss: 155.861, Residuals: 0.401\n",
      "Loss: 155.848, Residuals: 0.398\n",
      "Loss: 155.846, Residuals: 0.403\n",
      "Loss: 155.842, Residuals: 0.403\n",
      "Loss: 155.836, Residuals: 0.403\n",
      "Loss: 155.836, Residuals: 0.403\n",
      "Loss: 155.834, Residuals: 0.403\n",
      "Loss: 155.832, Residuals: 0.402\n",
      "Loss: 155.832, Residuals: 0.401\n",
      "Loss: 155.830, Residuals: 0.402\n",
      "Evidence -86.004\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.50e+00\n",
      "Loss: 156.248, Residuals: 0.396\n",
      "Loss: 156.231, Residuals: 0.402\n",
      "Loss: 156.222, Residuals: 0.399\n",
      "Loss: 156.222, Residuals: 0.401\n",
      "Loss: 156.215, Residuals: 0.400\n",
      "Loss: 156.215, Residuals: 0.399\n",
      "Loss: 156.214, Residuals: 0.399\n",
      "Loss: 156.212, Residuals: 0.398\n",
      "Loss: 156.212, Residuals: 0.398\n",
      "Loss: 156.212, Residuals: 0.399\n",
      "Loss: 156.211, Residuals: 0.398\n",
      "Loss: 156.211, Residuals: 0.398\n",
      "Loss: 156.211, Residuals: 0.398\n",
      "Evidence -85.567\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.51e+00\n",
      "Loss: 156.481, Residuals: 0.395\n",
      "Loss: 156.471, Residuals: 0.402\n",
      "Loss: 156.470, Residuals: 0.399\n",
      "Loss: 156.468, Residuals: 0.398\n",
      "Loss: 156.465, Residuals: 0.397\n",
      "Loss: 156.465, Residuals: 0.397\n",
      "Loss: 156.464, Residuals: 0.396\n",
      "Loss: 156.463, Residuals: 0.396\n",
      "Loss: 156.463, Residuals: 0.396\n",
      "Loss: 156.463, Residuals: 0.396\n",
      "Loss: 156.462, Residuals: 0.396\n",
      "Loss: 156.462, Residuals: 0.396\n",
      "Evidence -85.275\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.52e+00\n",
      "Loss: 156.642, Residuals: 0.393\n",
      "Loss: 156.636, Residuals: 0.399\n",
      "Loss: 156.635, Residuals: 0.397\n",
      "Loss: 156.633, Residuals: 0.396\n",
      "Loss: 156.632, Residuals: 0.395\n",
      "Loss: 156.632, Residuals: 0.395\n",
      "Loss: 156.631, Residuals: 0.395\n",
      "Loss: 156.631, Residuals: 0.394\n",
      "Evidence -85.073\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.52e+00\n",
      "Loss: 156.761, Residuals: 0.392\n",
      "Loss: 156.758, Residuals: 0.396\n",
      "Loss: 156.757, Residuals: 0.395\n",
      "Loss: 156.755, Residuals: 0.394\n",
      "Loss: 156.755, Residuals: 0.393\n",
      "Loss: 156.754, Residuals: 0.394\n",
      "Loss: 156.754, Residuals: 0.394\n",
      "Loss: 156.754, Residuals: 0.394\n",
      "Loss: 156.754, Residuals: 0.393\n",
      "Loss: 156.754, Residuals: 0.393\n",
      "Loss: 156.753, Residuals: 0.393\n",
      "Loss: 156.753, Residuals: 0.393\n",
      "Evidence -84.914\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.52e+00\n",
      "Loss: 156.855, Residuals: 0.392\n",
      "Loss: 156.853, Residuals: 0.393\n",
      "Loss: 156.853, Residuals: 0.393\n",
      "Loss: 156.852, Residuals: 0.393\n",
      "Loss: 156.851, Residuals: 0.393\n",
      "Loss: 156.851, Residuals: 0.393\n",
      "Evidence -84.807\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.52e+00\n",
      "Loss: 156.927, Residuals: 0.394\n",
      "Loss: 156.926, Residuals: 0.392\n",
      "Loss: 156.925, Residuals: 0.392\n",
      "Loss: 156.925, Residuals: 0.392\n",
      "Evidence -84.732\n",
      "Pass count  1\n",
      "Total samples: 38, Updated regularization: 1.00e-05\n",
      "Loss: 384.577, Residuals: -1.822\n",
      "Loss: 275.139, Residuals: -1.695\n",
      "Loss: 231.354, Residuals: -0.472\n",
      "Loss: 218.763, Residuals: 0.484\n",
      "Loss: 149.080, Residuals: -0.020\n",
      "Loss: 136.645, Residuals: 0.011\n",
      "Loss: 132.076, Residuals: 0.085\n",
      "Loss: 123.429, Residuals: 0.068\n",
      "Loss: 109.026, Residuals: 0.054\n",
      "Loss: 103.885, Residuals: 0.293\n",
      "Loss: 94.991, Residuals: 0.267\n",
      "Loss: 94.608, Residuals: 0.354\n",
      "Loss: 91.036, Residuals: 0.312\n",
      "Loss: 84.964, Residuals: 0.220\n",
      "Loss: 82.362, Residuals: 0.200\n",
      "Loss: 82.248, Residuals: 0.181\n",
      "Loss: 78.155, Residuals: 0.103\n",
      "Loss: 77.864, Residuals: 0.152\n",
      "Loss: 75.157, Residuals: 0.107\n",
      "Loss: 73.851, Residuals: 0.263\n",
      "Loss: 73.638, Residuals: 0.221\n",
      "Loss: 73.416, Residuals: 0.260\n",
      "Loss: 71.441, Residuals: 0.217\n",
      "Loss: 71.416, Residuals: 0.231\n",
      "Loss: 70.391, Residuals: 0.204\n",
      "Loss: 70.275, Residuals: 0.187\n",
      "Loss: 69.185, Residuals: 0.159\n",
      "Loss: 68.541, Residuals: 0.162\n",
      "Loss: 68.538, Residuals: 0.157\n",
      "Loss: 66.512, Residuals: 0.102\n",
      "Loss: 66.502, Residuals: 0.102\n",
      "Loss: 65.163, Residuals: 0.061\n",
      "Loss: 65.042, Residuals: 0.113\n",
      "Loss: 65.027, Residuals: 0.106\n",
      "Loss: 64.453, Residuals: 0.082\n",
      "Loss: 64.447, Residuals: 0.095\n",
      "Loss: 63.651, Residuals: 0.068\n",
      "Loss: 63.470, Residuals: 0.043\n",
      "Loss: 63.415, Residuals: 0.103\n",
      "Loss: 62.919, Residuals: 0.080\n",
      "Loss: 62.913, Residuals: 0.087\n",
      "Loss: 62.107, Residuals: 0.048\n",
      "Loss: 62.104, Residuals: 0.048\n",
      "Loss: 61.971, Residuals: 0.056\n",
      "Loss: 61.743, Residuals: 0.043\n",
      "Loss: 61.714, Residuals: 0.058\n",
      "Loss: 61.712, Residuals: 0.059\n",
      "Loss: 61.690, Residuals: 0.060\n",
      "Loss: 61.654, Residuals: 0.063\n",
      "Loss: 61.654, Residuals: 0.062\n",
      "Loss: 61.481, Residuals: 0.054\n",
      "Loss: 61.200, Residuals: 0.032\n",
      "Loss: 61.194, Residuals: 0.036\n",
      "Evidence -588.386\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.81e-01\n",
      "Loss: 109.981, Residuals: 0.023\n",
      "Loss: 109.011, Residuals: 0.027\n",
      "Loss: 107.420, Residuals: 0.046\n",
      "Loss: 107.412, Residuals: 0.050\n",
      "Loss: 106.358, Residuals: 0.082\n",
      "Loss: 106.354, Residuals: 0.085\n",
      "Evidence -152.971\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 5.81e-01\n",
      "Loss: 135.229, Residuals: 0.089\n",
      "Loss: 134.471, Residuals: 0.094\n",
      "Loss: 133.157, Residuals: 0.109\n",
      "Loss: 132.602, Residuals: 0.165\n",
      "Loss: 132.581, Residuals: 0.183\n",
      "Loss: 131.735, Residuals: 0.192\n",
      "Loss: 130.303, Residuals: 0.226\n",
      "Loss: 130.291, Residuals: 0.230\n",
      "Evidence -133.401\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 7.05e-01\n",
      "Loss: 143.977, Residuals: 0.232\n",
      "Evidence -128.262\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 7.29e-01\n",
      "Loss: 147.206, Residuals: 0.171\n",
      "Loss: 147.202, Residuals: 0.173\n",
      "Evidence -125.652\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 7.50e-01\n",
      "Loss: 149.297, Residuals: 0.175\n",
      "Evidence -124.510\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 7.55e-01\n",
      "Loss: 148.587, Residuals: 0.286\n",
      "Loss: 148.554, Residuals: 0.284\n",
      "Loss: 148.261, Residuals: 0.270\n",
      "Loss: 147.804, Residuals: 0.249\n",
      "Loss: 147.748, Residuals: 0.266\n",
      "Loss: 147.231, Residuals: 0.276\n",
      "Loss: 146.368, Residuals: 0.285\n",
      "Loss: 146.368, Residuals: 0.285\n",
      "Evidence -119.203\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 8.62e-01\n",
      "Loss: 149.308, Residuals: 0.287\n",
      "Evidence -117.842\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 8.71e-01\n",
      "Loss: 149.585, Residuals: 0.308\n",
      "Loss: 149.576, Residuals: 0.310\n",
      "Evidence -116.152\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 8.95e-01\n",
      "Loss: 150.084, Residuals: 0.297\n",
      "Loss: 149.410, Residuals: 0.302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 148.670, Residuals: 0.307\n",
      "Loss: 148.667, Residuals: 0.305\n",
      "Loss: 148.662, Residuals: 0.305\n",
      "Loss: 148.618, Residuals: 0.311\n",
      "Loss: 148.564, Residuals: 0.318\n",
      "Loss: 148.467, Residuals: 0.316\n",
      "Loss: 148.327, Residuals: 0.312\n",
      "Loss: 148.326, Residuals: 0.313\n",
      "Loss: 148.313, Residuals: 0.313\n",
      "Loss: 148.292, Residuals: 0.314\n",
      "Loss: 148.290, Residuals: 0.311\n",
      "Loss: 148.281, Residuals: 0.311\n",
      "Loss: 148.279, Residuals: 0.310\n",
      "Loss: 148.275, Residuals: 0.311\n",
      "Loss: 148.270, Residuals: 0.311\n",
      "Loss: 148.270, Residuals: 0.311\n",
      "Loss: 148.270, Residuals: 0.311\n",
      "Loss: 148.269, Residuals: 0.312\n",
      "Loss: 148.269, Residuals: 0.312\n",
      "Evidence -112.661\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.09e+00\n",
      "Loss: 149.625, Residuals: 0.316\n",
      "Loss: 149.556, Residuals: 0.311\n",
      "Loss: 149.480, Residuals: 0.307\n",
      "Loss: 149.480, Residuals: 0.307\n",
      "Evidence -111.171\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.13e+00\n",
      "Loss: 150.182, Residuals: 0.304\n",
      "Loss: 150.115, Residuals: 0.306\n",
      "Loss: 150.013, Residuals: 0.308\n",
      "Loss: 150.009, Residuals: 0.301\n",
      "Loss: 149.979, Residuals: 0.301\n",
      "Loss: 149.937, Residuals: 0.303\n",
      "Loss: 149.937, Residuals: 0.301\n",
      "Loss: 149.923, Residuals: 0.302\n",
      "Loss: 149.922, Residuals: 0.303\n",
      "Loss: 149.921, Residuals: 0.303\n",
      "Loss: 149.918, Residuals: 0.303\n",
      "Loss: 149.918, Residuals: 0.302\n",
      "Loss: 149.916, Residuals: 0.302\n",
      "Loss: 149.914, Residuals: 0.303\n",
      "Loss: 149.914, Residuals: 0.303\n",
      "Evidence -110.225\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.22e+00\n",
      "Loss: 150.317, Residuals: 0.302\n",
      "Loss: 150.312, Residuals: 0.302\n",
      "Loss: 150.302, Residuals: 0.302\n",
      "Loss: 150.286, Residuals: 0.300\n",
      "Loss: 150.284, Residuals: 0.296\n",
      "Loss: 150.267, Residuals: 0.296\n",
      "Loss: 150.239, Residuals: 0.296\n",
      "Loss: 150.239, Residuals: 0.295\n",
      "Loss: 150.232, Residuals: 0.296\n",
      "Loss: 150.220, Residuals: 0.296\n",
      "Loss: 150.220, Residuals: 0.295\n",
      "Loss: 150.216, Residuals: 0.295\n",
      "Loss: 150.211, Residuals: 0.296\n",
      "Loss: 150.210, Residuals: 0.295\n",
      "Loss: 150.208, Residuals: 0.295\n",
      "Evidence -109.569\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.26e+00\n",
      "Loss: 150.487, Residuals: 0.294\n",
      "Loss: 150.482, Residuals: 0.294\n",
      "Loss: 150.472, Residuals: 0.294\n",
      "Loss: 150.470, Residuals: 0.292\n",
      "Loss: 150.455, Residuals: 0.292\n",
      "Loss: 150.431, Residuals: 0.291\n",
      "Loss: 150.427, Residuals: 0.291\n",
      "Loss: 150.419, Residuals: 0.290\n",
      "Loss: 150.409, Residuals: 0.288\n",
      "Loss: 150.409, Residuals: 0.287\n",
      "Loss: 150.407, Residuals: 0.287\n",
      "Loss: 150.407, Residuals: 0.287\n",
      "Loss: 150.407, Residuals: 0.287\n",
      "Loss: 150.406, Residuals: 0.287\n",
      "Loss: 150.405, Residuals: 0.287\n",
      "Loss: 150.405, Residuals: 0.287\n",
      "Evidence -109.106\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.28e+00\n",
      "Loss: 150.571, Residuals: 0.286\n",
      "Loss: 150.566, Residuals: 0.287\n",
      "Loss: 150.557, Residuals: 0.287\n",
      "Loss: 150.545, Residuals: 0.287\n",
      "Loss: 150.543, Residuals: 0.283\n",
      "Loss: 150.532, Residuals: 0.283\n",
      "Loss: 150.518, Residuals: 0.284\n",
      "Loss: 150.517, Residuals: 0.280\n",
      "Loss: 150.515, Residuals: 0.280\n",
      "Loss: 150.513, Residuals: 0.280\n",
      "Loss: 150.509, Residuals: 0.279\n",
      "Loss: 150.509, Residuals: 0.280\n",
      "Loss: 150.509, Residuals: 0.280\n",
      "Loss: 150.508, Residuals: 0.280\n",
      "Loss: 150.508, Residuals: 0.280\n",
      "Evidence -108.735\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.29e+00\n",
      "Loss: 150.633, Residuals: 0.276\n",
      "Loss: 150.626, Residuals: 0.277\n",
      "Loss: 150.614, Residuals: 0.278\n",
      "Loss: 150.601, Residuals: 0.277\n",
      "Loss: 150.601, Residuals: 0.276\n",
      "Loss: 150.598, Residuals: 0.276\n",
      "Loss: 150.593, Residuals: 0.276\n",
      "Loss: 150.590, Residuals: 0.276\n",
      "Loss: 150.588, Residuals: 0.275\n",
      "Loss: 150.585, Residuals: 0.275\n",
      "Loss: 150.581, Residuals: 0.274\n",
      "Loss: 150.581, Residuals: 0.274\n",
      "Loss: 150.581, Residuals: 0.274\n",
      "Evidence -108.437\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.30e+00\n",
      "Loss: 150.673, Residuals: 0.271\n",
      "Loss: 150.666, Residuals: 0.272\n",
      "Loss: 150.655, Residuals: 0.272\n",
      "Loss: 150.647, Residuals: 0.275\n",
      "Loss: 150.646, Residuals: 0.270\n",
      "Loss: 150.636, Residuals: 0.269\n",
      "Loss: 150.628, Residuals: 0.268\n",
      "Loss: 150.627, Residuals: 0.268\n",
      "Loss: 150.626, Residuals: 0.268\n",
      "Loss: 150.625, Residuals: 0.268\n",
      "Loss: 150.625, Residuals: 0.269\n",
      "Loss: 150.625, Residuals: 0.269\n",
      "Loss: 150.624, Residuals: 0.268\n",
      "Evidence -108.158\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.29e+00\n",
      "Loss: 150.683, Residuals: 0.262\n",
      "Loss: 150.680, Residuals: 0.269\n",
      "Loss: 150.673, Residuals: 0.268\n",
      "Loss: 150.664, Residuals: 0.267\n",
      "Loss: 150.657, Residuals: 0.265\n",
      "Loss: 150.656, Residuals: 0.264\n",
      "Loss: 150.653, Residuals: 0.263\n",
      "Loss: 150.652, Residuals: 0.264\n",
      "Loss: 150.652, Residuals: 0.263\n",
      "Loss: 150.652, Residuals: 0.263\n",
      "Loss: 150.651, Residuals: 0.263\n",
      "Loss: 150.651, Residuals: 0.263\n",
      "Loss: 150.650, Residuals: 0.262\n",
      "Loss: 150.650, Residuals: 0.262\n",
      "Evidence -107.893\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.27e+00\n",
      "Loss: 150.697, Residuals: 0.260\n",
      "Loss: 150.693, Residuals: 0.261\n",
      "Loss: 150.687, Residuals: 0.261\n",
      "Loss: 150.679, Residuals: 0.261\n",
      "Loss: 150.678, Residuals: 0.259\n",
      "Loss: 150.676, Residuals: 0.259\n",
      "Loss: 150.672, Residuals: 0.258\n",
      "Loss: 150.672, Residuals: 0.258\n",
      "Loss: 150.671, Residuals: 0.257\n",
      "Loss: 150.671, Residuals: 0.257\n",
      "Loss: 150.671, Residuals: 0.257\n",
      "Loss: 150.671, Residuals: 0.257\n",
      "Loss: 150.671, Residuals: 0.257\n",
      "Loss: 150.671, Residuals: 0.257\n",
      "Loss: 150.671, Residuals: 0.257\n",
      "Evidence -107.658\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.24e+00\n",
      "Loss: 150.708, Residuals: 0.255\n",
      "Loss: 150.705, Residuals: 0.257\n",
      "Loss: 150.699, Residuals: 0.257\n",
      "Loss: 150.692, Residuals: 0.256\n",
      "Loss: 150.692, Residuals: 0.256\n",
      "Loss: 150.692, Residuals: 0.256\n",
      "Loss: 150.690, Residuals: 0.255\n",
      "Loss: 150.687, Residuals: 0.254\n",
      "Loss: 150.687, Residuals: 0.254\n",
      "Loss: 150.687, Residuals: 0.253\n",
      "Loss: 150.686, Residuals: 0.253\n",
      "Loss: 150.686, Residuals: 0.253\n",
      "Evidence -107.458\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.23e+00\n",
      "Loss: 150.718, Residuals: 0.256\n",
      "Loss: 150.717, Residuals: 0.253\n",
      "Loss: 150.714, Residuals: 0.253\n",
      "Loss: 150.710, Residuals: 0.253\n",
      "Loss: 150.706, Residuals: 0.252\n",
      "Loss: 150.705, Residuals: 0.251\n",
      "Loss: 150.704, Residuals: 0.251\n",
      "Loss: 150.702, Residuals: 0.250\n",
      "Loss: 150.702, Residuals: 0.250\n",
      "Loss: 150.701, Residuals: 0.249\n",
      "Loss: 150.701, Residuals: 0.249\n",
      "Loss: 150.701, Residuals: 0.249\n",
      "Evidence -107.275\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.21e+00\n",
      "Loss: 150.732, Residuals: 0.248\n",
      "Loss: 150.726, Residuals: 0.249\n",
      "Loss: 150.722, Residuals: 0.249\n",
      "Loss: 150.721, Residuals: 0.248\n",
      "Loss: 150.721, Residuals: 0.248\n",
      "Loss: 150.721, Residuals: 0.248\n",
      "Loss: 150.720, Residuals: 0.248\n",
      "Loss: 150.719, Residuals: 0.248\n",
      "Loss: 150.717, Residuals: 0.247\n",
      "Loss: 150.717, Residuals: 0.247\n",
      "Evidence -107.123\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.20e+00\n",
      "Loss: 150.753, Residuals: 0.245\n",
      "Loss: 150.748, Residuals: 0.246\n",
      "Loss: 150.744, Residuals: 0.248\n",
      "Loss: 150.744, Residuals: 0.246\n",
      "Loss: 150.743, Residuals: 0.246\n",
      "Loss: 150.741, Residuals: 0.245\n",
      "Loss: 150.739, Residuals: 0.243\n",
      "Loss: 150.739, Residuals: 0.243\n",
      "Evidence -106.974\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.18e+00\n",
      "Loss: 150.771, Residuals: 0.243\n",
      "Loss: 150.767, Residuals: 0.243\n",
      "Loss: 150.764, Residuals: 0.245\n",
      "Loss: 150.764, Residuals: 0.243\n",
      "Loss: 150.764, Residuals: 0.243\n",
      "Loss: 150.763, Residuals: 0.243\n",
      "Loss: 150.763, Residuals: 0.243\n",
      "Loss: 150.762, Residuals: 0.242\n",
      "Loss: 150.761, Residuals: 0.242\n",
      "Loss: 150.761, Residuals: 0.242\n",
      "Evidence -106.863\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.17e+00\n",
      "Loss: 150.785, Residuals: 0.242\n",
      "Loss: 150.783, Residuals: 0.244\n",
      "Loss: 150.780, Residuals: 0.244\n",
      "Loss: 150.779, Residuals: 0.242\n",
      "Loss: 150.778, Residuals: 0.241\n",
      "Loss: 150.778, Residuals: 0.242\n",
      "Loss: 150.777, Residuals: 0.242\n",
      "Evidence -106.760\n",
      "Pass count  1\n",
      "Total samples: 37, Updated regularization: 1.00e-05\n",
      "Loss: 220.616, Residuals: -1.209\n",
      "Loss: 149.740, Residuals: -1.119\n",
      "Loss: 124.764, Residuals: 0.267\n",
      "Loss: 122.385, Residuals: 0.643\n",
      "Loss: 101.905, Residuals: 0.472\n",
      "Loss: 87.043, Residuals: 0.108\n",
      "Loss: 85.935, Residuals: -0.208\n",
      "Loss: 83.965, Residuals: -0.040\n",
      "Loss: 80.710, Residuals: 0.137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 75.637, Residuals: 0.102\n",
      "Loss: 73.441, Residuals: 0.087\n",
      "Loss: 70.369, Residuals: 0.090\n",
      "Loss: 70.134, Residuals: 0.146\n",
      "Loss: 68.109, Residuals: 0.090\n",
      "Loss: 67.522, Residuals: 0.097\n",
      "Loss: 63.815, Residuals: 0.110\n",
      "Loss: 63.338, Residuals: 0.139\n",
      "Loss: 62.497, Residuals: 0.122\n",
      "Loss: 61.430, Residuals: 0.070\n",
      "Loss: 61.109, Residuals: 0.092\n",
      "Loss: 60.547, Residuals: 0.121\n",
      "Loss: 59.787, Residuals: 0.163\n",
      "Loss: 59.673, Residuals: 0.168\n",
      "Loss: 58.593, Residuals: 0.141\n",
      "Loss: 58.541, Residuals: 0.164\n",
      "Loss: 56.634, Residuals: 0.109\n",
      "Loss: 56.596, Residuals: 0.116\n",
      "Loss: 55.373, Residuals: 0.119\n",
      "Loss: 55.320, Residuals: 0.148\n",
      "Loss: 55.234, Residuals: 0.160\n",
      "Loss: 54.431, Residuals: 0.148\n",
      "Loss: 54.388, Residuals: 0.156\n",
      "Loss: 52.873, Residuals: 0.121\n",
      "Loss: 52.796, Residuals: 0.134\n",
      "Loss: 52.653, Residuals: 0.140\n",
      "Loss: 52.390, Residuals: 0.128\n",
      "Loss: 52.281, Residuals: 0.169\n",
      "Loss: 51.357, Residuals: 0.136\n",
      "Loss: 51.320, Residuals: 0.145\n",
      "Loss: 51.034, Residuals: 0.119\n",
      "Loss: 50.983, Residuals: 0.147\n",
      "Loss: 50.524, Residuals: 0.136\n",
      "Loss: 50.516, Residuals: 0.134\n",
      "Loss: 50.171, Residuals: 0.122\n",
      "Loss: 49.701, Residuals: 0.113\n",
      "Loss: 49.683, Residuals: 0.119\n",
      "Loss: 49.670, Residuals: 0.117\n",
      "Loss: 49.646, Residuals: 0.118\n",
      "Loss: 49.617, Residuals: 0.120\n",
      "Loss: 49.348, Residuals: 0.109\n",
      "Loss: 49.345, Residuals: 0.106\n",
      "Loss: 49.224, Residuals: 0.105\n",
      "Loss: 48.997, Residuals: 0.099\n",
      "Loss: 48.930, Residuals: 0.091\n",
      "Loss: 48.921, Residuals: 0.098\n",
      "Loss: 48.919, Residuals: 0.095\n",
      "Loss: 48.918, Residuals: 0.098\n",
      "Loss: 48.746, Residuals: 0.091\n",
      "Loss: 48.745, Residuals: 0.091\n",
      "Evidence -517.043\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 3.06e-02\n",
      "Loss: 90.321, Residuals: 0.090\n",
      "Loss: 90.043, Residuals: 0.097\n",
      "Loss: 89.547, Residuals: 0.071\n",
      "Loss: 88.804, Residuals: 0.030\n",
      "Loss: 88.800, Residuals: 0.031\n",
      "Loss: 88.207, Residuals: 0.047\n",
      "Loss: 87.168, Residuals: 0.072\n",
      "Loss: 86.549, Residuals: 0.155\n",
      "Loss: 86.540, Residuals: 0.156\n",
      "Loss: 86.524, Residuals: 0.154\n",
      "Loss: 86.495, Residuals: 0.151\n",
      "Loss: 86.447, Residuals: 0.149\n",
      "Loss: 86.363, Residuals: 0.141\n",
      "Loss: 86.201, Residuals: 0.145\n",
      "Loss: 86.162, Residuals: 0.141\n",
      "Loss: 85.806, Residuals: 0.150\n",
      "Loss: 85.789, Residuals: 0.145\n",
      "Loss: 85.626, Residuals: 0.151\n",
      "Loss: 85.349, Residuals: 0.165\n",
      "Loss: 85.346, Residuals: 0.164\n",
      "Loss: 85.234, Residuals: 0.169\n",
      "Loss: 85.190, Residuals: 0.174\n",
      "Loss: 85.171, Residuals: 0.172\n",
      "Loss: 85.135, Residuals: 0.174\n",
      "Loss: 85.132, Residuals: 0.174\n",
      "Loss: 85.098, Residuals: 0.177\n",
      "Loss: 85.043, Residuals: 0.181\n",
      "Loss: 85.043, Residuals: 0.180\n",
      "Loss: 85.037, Residuals: 0.181\n",
      "Loss: 85.036, Residuals: 0.180\n",
      "Loss: 84.990, Residuals: 0.181\n",
      "Loss: 84.990, Residuals: 0.182\n",
      "Loss: 84.990, Residuals: 0.182\n",
      "Loss: 84.958, Residuals: 0.183\n",
      "Loss: 84.953, Residuals: 0.185\n",
      "Loss: 84.952, Residuals: 0.185\n",
      "Loss: 84.952, Residuals: 0.185\n",
      "Loss: 84.946, Residuals: 0.185\n",
      "Loss: 84.938, Residuals: 0.184\n",
      "Loss: 84.938, Residuals: 0.184\n",
      "Loss: 84.937, Residuals: 0.183\n",
      "Loss: 84.934, Residuals: 0.184\n",
      "Loss: 84.933, Residuals: 0.184\n",
      "Loss: 84.929, Residuals: 0.184\n",
      "Loss: 84.921, Residuals: 0.184\n",
      "Loss: 84.921, Residuals: 0.184\n",
      "Loss: 84.918, Residuals: 0.184\n",
      "Loss: 84.917, Residuals: 0.186\n",
      "Loss: 84.917, Residuals: 0.185\n",
      "Loss: 84.917, Residuals: 0.185\n",
      "Evidence -128.096\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 9.72e-02\n",
      "Loss: 119.754, Residuals: 0.183\n",
      "Loss: 119.619, Residuals: 0.180\n",
      "Loss: 119.396, Residuals: 0.173\n",
      "Loss: 119.124, Residuals: 0.155\n",
      "Loss: 118.611, Residuals: 0.159\n",
      "Loss: 117.717, Residuals: 0.178\n",
      "Loss: 117.662, Residuals: 0.172\n",
      "Loss: 117.157, Residuals: 0.182\n",
      "Loss: 116.377, Residuals: 0.206\n",
      "Loss: 116.361, Residuals: 0.206\n",
      "Loss: 116.335, Residuals: 0.203\n",
      "Loss: 116.285, Residuals: 0.204\n",
      "Loss: 116.202, Residuals: 0.204\n",
      "Loss: 116.044, Residuals: 0.211\n",
      "Loss: 116.038, Residuals: 0.211\n",
      "Loss: 115.803, Residuals: 0.221\n",
      "Loss: 115.794, Residuals: 0.219\n",
      "Loss: 115.701, Residuals: 0.223\n",
      "Loss: 115.537, Residuals: 0.226\n",
      "Loss: 115.534, Residuals: 0.224\n",
      "Loss: 115.504, Residuals: 0.224\n",
      "Loss: 115.269, Residuals: 0.236\n",
      "Loss: 115.263, Residuals: 0.232\n",
      "Loss: 115.253, Residuals: 0.230\n",
      "Loss: 115.236, Residuals: 0.229\n",
      "Loss: 115.213, Residuals: 0.223\n",
      "Loss: 115.210, Residuals: 0.221\n",
      "Loss: 115.210, Residuals: 0.220\n",
      "Loss: 115.041, Residuals: 0.223\n",
      "Loss: 115.040, Residuals: 0.223\n",
      "Loss: 114.988, Residuals: 0.224\n",
      "Loss: 114.987, Residuals: 0.224\n",
      "Loss: 114.948, Residuals: 0.225\n",
      "Loss: 114.947, Residuals: 0.223\n",
      "Loss: 114.897, Residuals: 0.224\n",
      "Loss: 114.897, Residuals: 0.223\n",
      "Loss: 114.896, Residuals: 0.223\n",
      "Loss: 114.894, Residuals: 0.223\n",
      "Loss: 114.891, Residuals: 0.223\n",
      "Loss: 114.888, Residuals: 0.223\n",
      "Loss: 114.865, Residuals: 0.224\n",
      "Loss: 114.864, Residuals: 0.224\n",
      "Loss: 114.864, Residuals: 0.223\n",
      "Loss: 114.863, Residuals: 0.223\n",
      "Loss: 114.863, Residuals: 0.221\n",
      "Loss: 114.859, Residuals: 0.221\n",
      "Loss: 114.858, Residuals: 0.221\n",
      "Loss: 114.852, Residuals: 0.221\n",
      "Loss: 114.851, Residuals: 0.220\n",
      "Loss: 114.843, Residuals: 0.221\n",
      "Loss: 114.843, Residuals: 0.220\n",
      "Loss: 114.843, Residuals: 0.220\n",
      "Loss: 114.836, Residuals: 0.220\n",
      "Loss: 114.836, Residuals: 0.220\n",
      "Loss: 114.830, Residuals: 0.220\n",
      "Evidence -92.588\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.39e-01\n",
      "Loss: 132.794, Residuals: 0.217\n",
      "Loss: 132.341, Residuals: 0.193\n",
      "Loss: 131.828, Residuals: 0.132\n",
      "Loss: 131.797, Residuals: 0.131\n",
      "Loss: 130.737, Residuals: 0.170\n",
      "Loss: 130.691, Residuals: 0.181\n",
      "Loss: 130.266, Residuals: 0.196\n",
      "Loss: 129.613, Residuals: 0.226\n",
      "Loss: 129.584, Residuals: 0.219\n",
      "Loss: 129.534, Residuals: 0.218\n",
      "Loss: 129.149, Residuals: 0.219\n",
      "Loss: 129.133, Residuals: 0.217\n",
      "Loss: 129.110, Residuals: 0.216\n",
      "Loss: 128.898, Residuals: 0.214\n",
      "Loss: 128.867, Residuals: 0.215\n",
      "Loss: 128.862, Residuals: 0.210\n",
      "Loss: 128.647, Residuals: 0.213\n",
      "Loss: 128.643, Residuals: 0.214\n",
      "Loss: 128.497, Residuals: 0.216\n",
      "Loss: 128.490, Residuals: 0.214\n",
      "Loss: 128.477, Residuals: 0.213\n",
      "Loss: 128.372, Residuals: 0.218\n",
      "Loss: 128.364, Residuals: 0.214\n",
      "Loss: 128.358, Residuals: 0.211\n",
      "Loss: 128.347, Residuals: 0.209\n",
      "Loss: 128.346, Residuals: 0.209\n",
      "Loss: 128.314, Residuals: 0.210\n",
      "Loss: 128.313, Residuals: 0.210\n",
      "Loss: 128.301, Residuals: 0.210\n",
      "Loss: 128.288, Residuals: 0.209\n",
      "Loss: 128.287, Residuals: 0.209\n",
      "Loss: 128.286, Residuals: 0.207\n",
      "Loss: 128.275, Residuals: 0.207\n",
      "Loss: 128.274, Residuals: 0.206\n",
      "Loss: 128.274, Residuals: 0.207\n",
      "Loss: 128.268, Residuals: 0.207\n",
      "Loss: 128.259, Residuals: 0.207\n",
      "Loss: 128.259, Residuals: 0.207\n",
      "Loss: 128.256, Residuals: 0.207\n",
      "Loss: 128.256, Residuals: 0.207\n",
      "Loss: 128.255, Residuals: 0.207\n",
      "Loss: 128.253, Residuals: 0.207\n",
      "Loss: 128.250, Residuals: 0.207\n",
      "Loss: 128.250, Residuals: 0.207\n",
      "Loss: 128.250, Residuals: 0.206\n",
      "Loss: 128.248, Residuals: 0.206\n",
      "Loss: 128.248, Residuals: 0.206\n",
      "Loss: 128.247, Residuals: 0.206\n",
      "Loss: 128.247, Residuals: 0.206\n",
      "Evidence -75.466\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 4.99e-01\n",
      "Loss: 136.776, Residuals: 0.172\n",
      "Loss: 136.026, Residuals: 0.143\n",
      "Loss: 134.904, Residuals: 0.156\n",
      "Loss: 134.840, Residuals: 0.158\n",
      "Loss: 134.298, Residuals: 0.174\n",
      "Loss: 133.635, Residuals: 0.194\n",
      "Loss: 133.600, Residuals: 0.186\n",
      "Loss: 133.563, Residuals: 0.184\n",
      "Loss: 133.531, Residuals: 0.184\n",
      "Loss: 133.267, Residuals: 0.191\n",
      "Loss: 133.259, Residuals: 0.188\n",
      "Loss: 133.187, Residuals: 0.189\n",
      "Loss: 133.072, Residuals: 0.191\n",
      "Loss: 133.064, Residuals: 0.192\n",
      "Loss: 133.050, Residuals: 0.193\n",
      "Loss: 133.025, Residuals: 0.194\n",
      "Loss: 133.020, Residuals: 0.193\n",
      "Loss: 132.979, Residuals: 0.193\n",
      "Loss: 132.977, Residuals: 0.193\n",
      "Loss: 132.961, Residuals: 0.193\n",
      "Loss: 132.954, Residuals: 0.192\n",
      "Loss: 132.953, Residuals: 0.194\n",
      "Loss: 132.952, Residuals: 0.194\n",
      "Loss: 132.946, Residuals: 0.194\n",
      "Loss: 132.941, Residuals: 0.195\n",
      "Loss: 132.941, Residuals: 0.194\n",
      "Loss: 132.939, Residuals: 0.194\n",
      "Loss: 132.939, Residuals: 0.192\n",
      "Loss: 132.938, Residuals: 0.192\n",
      "Loss: 132.937, Residuals: 0.192\n",
      "Loss: 132.937, Residuals: 0.192\n",
      "Loss: 132.937, Residuals: 0.193\n",
      "Loss: 132.937, Residuals: 0.193\n",
      "Loss: 132.936, Residuals: 0.193\n",
      "Loss: 132.936, Residuals: 0.193\n",
      "Loss: 132.936, Residuals: 0.193\n",
      "Evidence -60.445\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 9.50e-01\n",
      "Loss: 138.543, Residuals: 0.184\n",
      "Loss: 137.794, Residuals: 0.174\n",
      "Loss: 137.655, Residuals: 0.186\n",
      "Loss: 137.397, Residuals: 0.189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 136.969, Residuals: 0.201\n",
      "Loss: 136.604, Residuals: 0.217\n",
      "Loss: 136.569, Residuals: 0.206\n",
      "Loss: 136.551, Residuals: 0.215\n",
      "Loss: 136.518, Residuals: 0.215\n",
      "Loss: 136.466, Residuals: 0.217\n",
      "Loss: 136.451, Residuals: 0.210\n",
      "Loss: 136.425, Residuals: 0.212\n",
      "Loss: 136.400, Residuals: 0.212\n",
      "Loss: 136.399, Residuals: 0.213\n",
      "Loss: 136.397, Residuals: 0.212\n",
      "Loss: 136.396, Residuals: 0.213\n",
      "Loss: 136.396, Residuals: 0.213\n",
      "Loss: 136.394, Residuals: 0.213\n",
      "Loss: 136.393, Residuals: 0.212\n",
      "Loss: 136.393, Residuals: 0.212\n",
      "Loss: 136.393, Residuals: 0.212\n",
      "Loss: 136.393, Residuals: 0.212\n",
      "Loss: 136.393, Residuals: 0.212\n",
      "Evidence -49.513\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.42e+00\n",
      "Loss: 140.620, Residuals: 0.211\n",
      "Loss: 140.005, Residuals: 0.206\n",
      "Loss: 139.883, Residuals: 0.207\n",
      "Loss: 139.658, Residuals: 0.213\n",
      "Loss: 139.304, Residuals: 0.225\n",
      "Loss: 139.288, Residuals: 0.218\n",
      "Loss: 139.155, Residuals: 0.221\n",
      "Loss: 139.019, Residuals: 0.226\n",
      "Loss: 139.005, Residuals: 0.228\n",
      "Loss: 138.990, Residuals: 0.230\n",
      "Loss: 138.984, Residuals: 0.229\n",
      "Loss: 138.974, Residuals: 0.229\n",
      "Loss: 138.961, Residuals: 0.227\n",
      "Loss: 138.959, Residuals: 0.227\n",
      "Loss: 138.958, Residuals: 0.227\n",
      "Loss: 138.957, Residuals: 0.228\n",
      "Loss: 138.956, Residuals: 0.228\n",
      "Loss: 138.956, Residuals: 0.228\n",
      "Loss: 138.956, Residuals: 0.228\n",
      "Loss: 138.956, Residuals: 0.227\n",
      "Loss: 138.956, Residuals: 0.227\n",
      "Loss: 138.956, Residuals: 0.227\n",
      "Loss: 138.956, Residuals: 0.228\n",
      "Loss: 138.956, Residuals: 0.228\n",
      "Loss: 138.956, Residuals: 0.228\n",
      "Loss: 138.956, Residuals: 0.228\n",
      "Evidence -44.451\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 1.88e+00\n",
      "Loss: 141.530, Residuals: 0.226\n",
      "Loss: 140.732, Residuals: 0.223\n",
      "Loss: 140.622, Residuals: 0.233\n",
      "Loss: 140.426, Residuals: 0.234\n",
      "Loss: 140.152, Residuals: 0.240\n",
      "Loss: 140.124, Residuals: 0.246\n",
      "Loss: 140.079, Residuals: 0.242\n",
      "Loss: 140.070, Residuals: 0.238\n",
      "Loss: 140.053, Residuals: 0.238\n",
      "Loss: 140.029, Residuals: 0.237\n",
      "Loss: 140.029, Residuals: 0.239\n",
      "Loss: 140.023, Residuals: 0.239\n",
      "Loss: 140.022, Residuals: 0.239\n",
      "Loss: 140.021, Residuals: 0.238\n",
      "Loss: 140.020, Residuals: 0.238\n",
      "Loss: 140.020, Residuals: 0.238\n",
      "Loss: 140.019, Residuals: 0.238\n",
      "Loss: 140.019, Residuals: 0.238\n",
      "Loss: 140.019, Residuals: 0.237\n",
      "Loss: 140.019, Residuals: 0.238\n",
      "Loss: 140.019, Residuals: 0.238\n",
      "Loss: 140.019, Residuals: 0.238\n",
      "Evidence -40.235\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.18e+00\n",
      "Loss: 142.428, Residuals: 0.239\n",
      "Loss: 142.109, Residuals: 0.241\n",
      "Loss: 141.982, Residuals: 0.250\n",
      "Loss: 141.969, Residuals: 0.247\n",
      "Loss: 141.949, Residuals: 0.248\n",
      "Loss: 141.926, Residuals: 0.245\n",
      "Loss: 141.926, Residuals: 0.244\n",
      "Loss: 141.924, Residuals: 0.244\n",
      "Loss: 141.921, Residuals: 0.244\n",
      "Loss: 141.918, Residuals: 0.243\n",
      "Loss: 141.918, Residuals: 0.243\n",
      "Loss: 141.918, Residuals: 0.243\n",
      "Loss: 141.918, Residuals: 0.244\n",
      "Loss: 141.918, Residuals: 0.244\n",
      "Loss: 141.917, Residuals: 0.244\n",
      "Loss: 141.917, Residuals: 0.243\n",
      "Loss: 141.917, Residuals: 0.243\n",
      "Evidence -37.295\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.32e+00\n",
      "Loss: 143.476, Residuals: 0.245\n",
      "Loss: 143.380, Residuals: 0.245\n",
      "Loss: 143.330, Residuals: 0.255\n",
      "Loss: 143.325, Residuals: 0.253\n",
      "Loss: 143.317, Residuals: 0.253\n",
      "Loss: 143.306, Residuals: 0.253\n",
      "Loss: 143.306, Residuals: 0.252\n",
      "Loss: 143.304, Residuals: 0.252\n",
      "Loss: 143.301, Residuals: 0.251\n",
      "Loss: 143.301, Residuals: 0.251\n",
      "Loss: 143.301, Residuals: 0.251\n",
      "Evidence -35.949\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.41e+00\n",
      "Loss: 144.114, Residuals: 0.248\n",
      "Loss: 144.072, Residuals: 0.256\n",
      "Loss: 144.054, Residuals: 0.256\n",
      "Loss: 144.053, Residuals: 0.257\n",
      "Loss: 144.048, Residuals: 0.258\n",
      "Loss: 144.045, Residuals: 0.260\n",
      "Loss: 144.044, Residuals: 0.259\n",
      "Loss: 144.044, Residuals: 0.258\n",
      "Loss: 144.044, Residuals: 0.258\n",
      "Loss: 144.044, Residuals: 0.258\n",
      "Loss: 144.044, Residuals: 0.258\n",
      "Loss: 144.044, Residuals: 0.258\n",
      "Evidence -35.219\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.45e+00\n",
      "Loss: 144.488, Residuals: 0.265\n",
      "Loss: 144.482, Residuals: 0.265\n",
      "Loss: 144.474, Residuals: 0.265\n",
      "Loss: 144.474, Residuals: 0.264\n",
      "Loss: 144.471, Residuals: 0.264\n",
      "Loss: 144.468, Residuals: 0.264\n",
      "Loss: 144.468, Residuals: 0.264\n",
      "Loss: 144.468, Residuals: 0.264\n",
      "Loss: 144.468, Residuals: 0.264\n",
      "Loss: 144.468, Residuals: 0.264\n",
      "Evidence -34.764\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.49e+00\n",
      "Loss: 144.756, Residuals: 0.267\n",
      "Loss: 144.754, Residuals: 0.269\n",
      "Loss: 144.751, Residuals: 0.269\n",
      "Loss: 144.749, Residuals: 0.269\n",
      "Loss: 144.749, Residuals: 0.269\n",
      "Loss: 144.748, Residuals: 0.269\n",
      "Loss: 144.748, Residuals: 0.269\n",
      "Loss: 144.748, Residuals: 0.268\n",
      "Evidence -34.446\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.51e+00\n",
      "Loss: 144.957, Residuals: 0.273\n",
      "Loss: 144.956, Residuals: 0.273\n",
      "Loss: 144.954, Residuals: 0.273\n",
      "Loss: 144.953, Residuals: 0.272\n",
      "Evidence -34.224\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.53e+00\n",
      "Loss: 145.112, Residuals: 0.277\n",
      "Loss: 145.111, Residuals: 0.275\n",
      "Loss: 145.110, Residuals: 0.275\n",
      "Loss: 145.109, Residuals: 0.275\n",
      "Loss: 145.109, Residuals: 0.275\n",
      "Loss: 145.109, Residuals: 0.275\n",
      "Evidence -34.045\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.55e+00\n",
      "Loss: 145.237, Residuals: 0.279\n",
      "Loss: 145.237, Residuals: 0.277\n",
      "Loss: 145.236, Residuals: 0.277\n",
      "Loss: 145.236, Residuals: 0.277\n",
      "Loss: 145.235, Residuals: 0.277\n",
      "Evidence -33.903\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.57e+00\n",
      "Loss: 145.341, Residuals: 0.280\n",
      "Loss: 145.341, Residuals: 0.279\n",
      "Loss: 145.340, Residuals: 0.279\n",
      "Loss: 145.340, Residuals: 0.279\n",
      "Loss: 145.339, Residuals: 0.279\n",
      "Loss: 145.339, Residuals: 0.279\n",
      "Evidence -33.791\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.58e+00\n",
      "Loss: 145.427, Residuals: 0.282\n",
      "Loss: 145.427, Residuals: 0.281\n",
      "Loss: 145.427, Residuals: 0.281\n",
      "Loss: 145.426, Residuals: 0.281\n",
      "Loss: 145.426, Residuals: 0.281\n",
      "Loss: 145.426, Residuals: 0.281\n",
      "Evidence -33.695\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.59e+00\n",
      "Loss: 145.497, Residuals: 0.283\n",
      "Loss: 145.497, Residuals: 0.282\n",
      "Loss: 145.497, Residuals: 0.282\n",
      "Loss: 145.496, Residuals: 0.282\n",
      "Evidence -33.613\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.60e+00\n",
      "Loss: 145.559, Residuals: 0.284\n",
      "Loss: 145.559, Residuals: 0.283\n",
      "Loss: 145.558, Residuals: 0.283\n",
      "Loss: 145.558, Residuals: 0.283\n",
      "Loss: 145.558, Residuals: 0.283\n",
      "Loss: 145.558, Residuals: 0.283\n",
      "Evidence -33.545\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.61e+00\n",
      "Loss: 145.612, Residuals: 0.284\n",
      "Loss: 145.612, Residuals: 0.284\n",
      "Loss: 145.612, Residuals: 0.284\n",
      "Loss: 145.611, Residuals: 0.284\n",
      "Loss: 145.611, Residuals: 0.284\n",
      "Evidence -33.486\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.61e+00\n",
      "Loss: 145.656, Residuals: 0.285\n",
      "Loss: 145.656, Residuals: 0.284\n",
      "Loss: 145.656, Residuals: 0.284\n",
      "Loss: 145.656, Residuals: 0.284\n",
      "Loss: 145.655, Residuals: 0.284\n",
      "Evidence -33.433\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.62e+00\n",
      "Loss: 145.696, Residuals: 0.285\n",
      "Loss: 145.696, Residuals: 0.285\n",
      "Loss: 145.696, Residuals: 0.285\n",
      "Evidence -33.389\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.63e+00\n",
      "Loss: 145.731, Residuals: 0.286\n",
      "Loss: 145.731, Residuals: 0.285\n",
      "Loss: 145.731, Residuals: 0.285\n",
      "Loss: 145.731, Residuals: 0.285\n",
      "Loss: 145.731, Residuals: 0.285\n",
      "Evidence -33.347\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.63e+00\n",
      "Loss: 145.762, Residuals: 0.286\n",
      "Loss: 145.762, Residuals: 0.285\n",
      "Loss: 145.762, Residuals: 0.285\n",
      "Loss: 145.762, Residuals: 0.286\n",
      "Loss: 145.762, Residuals: 0.286\n",
      "Loss: 145.762, Residuals: 0.285\n",
      "Evidence -33.309\n",
      "Updating hyper-parameters...\n",
      "Total samples: 37, Updated regularization: 2.63e+00\n",
      "Loss: 145.789, Residuals: 0.286\n",
      "Loss: 145.789, Residuals: 0.286\n",
      "Loss: 145.789, Residuals: 0.286\n",
      "Evidence -33.277\n",
      "Pass count  1\n",
      "Total samples: 40, Updated regularization: 1.00e-05\n",
      "Loss: 409.913, Residuals: -1.925\n",
      "Loss: 312.748, Residuals: -1.454\n",
      "Loss: 273.045, Residuals: -0.455\n",
      "Loss: 256.536, Residuals: 0.512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 193.663, Residuals: -0.018\n",
      "Loss: 181.267, Residuals: -0.094\n",
      "Loss: 178.878, Residuals: 0.220\n",
      "Loss: 159.346, Residuals: -0.002\n",
      "Loss: 155.366, Residuals: 0.069\n",
      "Loss: 147.796, Residuals: 0.065\n",
      "Loss: 134.559, Residuals: 0.136\n",
      "Loss: 129.654, Residuals: 0.107\n",
      "Loss: 122.147, Residuals: 0.233\n",
      "Loss: 121.362, Residuals: 0.324\n",
      "Loss: 119.861, Residuals: 0.351\n",
      "Loss: 117.161, Residuals: 0.390\n",
      "Loss: 112.526, Residuals: 0.368\n",
      "Loss: 111.838, Residuals: 0.440\n",
      "Loss: 106.437, Residuals: 0.393\n",
      "Loss: 106.293, Residuals: 0.359\n",
      "Loss: 106.017, Residuals: 0.371\n",
      "Loss: 103.642, Residuals: 0.327\n",
      "Loss: 102.877, Residuals: 0.385\n",
      "Loss: 101.435, Residuals: 0.349\n",
      "Loss: 98.886, Residuals: 0.287\n",
      "Loss: 98.849, Residuals: 0.272\n",
      "Loss: 97.387, Residuals: 0.248\n",
      "Loss: 95.192, Residuals: 0.205\n",
      "Loss: 94.726, Residuals: 0.265\n",
      "Loss: 91.115, Residuals: 0.198\n",
      "Loss: 91.058, Residuals: 0.173\n",
      "Loss: 91.018, Residuals: 0.171\n",
      "Loss: 89.519, Residuals: 0.148\n",
      "Loss: 89.310, Residuals: 0.164\n",
      "Loss: 87.555, Residuals: 0.150\n",
      "Loss: 87.542, Residuals: 0.137\n",
      "Loss: 87.525, Residuals: 0.121\n",
      "Loss: 87.357, Residuals: 0.121\n",
      "Loss: 85.958, Residuals: 0.116\n",
      "Loss: 85.941, Residuals: 0.101\n",
      "Loss: 85.917, Residuals: 0.082\n",
      "Loss: 85.708, Residuals: 0.084\n",
      "Loss: 85.394, Residuals: 0.110\n",
      "Loss: 85.383, Residuals: 0.115\n",
      "Loss: 84.963, Residuals: 0.109\n",
      "Loss: 84.930, Residuals: 0.112\n",
      "Loss: 83.797, Residuals: 0.101\n",
      "Loss: 83.789, Residuals: 0.090\n",
      "Loss: 83.778, Residuals: 0.076\n",
      "Loss: 83.680, Residuals: 0.071\n",
      "Loss: 83.498, Residuals: 0.058\n",
      "Loss: 83.182, Residuals: 0.050\n",
      "Loss: 83.176, Residuals: 0.045\n",
      "Loss: 83.164, Residuals: 0.046\n",
      "Loss: 83.144, Residuals: 0.048\n",
      "Loss: 83.109, Residuals: 0.062\n",
      "Loss: 83.109, Residuals: 0.061\n",
      "Loss: 82.829, Residuals: 0.059\n",
      "Loss: 82.326, Residuals: 0.056\n",
      "Loss: 82.321, Residuals: 0.038\n",
      "Loss: 81.576, Residuals: 0.044\n",
      "Loss: 81.574, Residuals: 0.038\n",
      "Loss: 81.558, Residuals: 0.037\n",
      "Loss: 81.529, Residuals: 0.036\n",
      "Loss: 81.478, Residuals: 0.037\n",
      "Loss: 81.462, Residuals: 0.029\n",
      "Loss: 81.330, Residuals: 0.031\n",
      "Loss: 81.328, Residuals: 0.027\n",
      "Loss: 81.309, Residuals: 0.031\n",
      "Loss: 81.308, Residuals: 0.030\n",
      "Loss: 81.102, Residuals: 0.029\n",
      "Loss: 80.830, Residuals: 0.021\n",
      "Loss: 80.824, Residuals: 0.029\n",
      "Loss: 80.816, Residuals: 0.029\n",
      "Loss: 80.802, Residuals: 0.030\n",
      "Loss: 80.780, Residuals: 0.030\n",
      "Loss: 80.778, Residuals: 0.029\n",
      "Loss: 80.761, Residuals: 0.030\n",
      "Loss: 80.731, Residuals: 0.033\n",
      "Loss: 80.731, Residuals: 0.032\n",
      "Loss: 80.523, Residuals: 0.031\n",
      "Loss: 80.491, Residuals: 0.032\n",
      "Loss: 80.491, Residuals: 0.030\n",
      "Loss: 80.484, Residuals: 0.031\n",
      "Loss: 80.472, Residuals: 0.034\n",
      "Loss: 80.467, Residuals: 0.033\n",
      "Loss: 80.466, Residuals: 0.032\n",
      "Loss: 80.215, Residuals: 0.032\n",
      "Loss: 80.215, Residuals: 0.027\n",
      "Loss: 79.909, Residuals: 0.027\n",
      "Loss: 79.894, Residuals: 0.022\n",
      "Loss: 79.884, Residuals: 0.024\n",
      "Loss: 79.883, Residuals: 0.023\n",
      "Loss: 79.513, Residuals: 0.024\n",
      "Loss: 79.511, Residuals: 0.014\n",
      "Loss: 79.495, Residuals: 0.015\n",
      "Loss: 79.466, Residuals: 0.016\n",
      "Loss: 79.463, Residuals: 0.022\n",
      "Loss: 79.461, Residuals: 0.015\n",
      "Loss: 79.198, Residuals: 0.015\n",
      "Loss: 79.197, Residuals: 0.012\n",
      "Loss: 79.192, Residuals: 0.012\n",
      "Loss: 79.147, Residuals: 0.010\n",
      "Loss: 79.147, Residuals: 0.009\n",
      "Loss: 79.144, Residuals: 0.011\n",
      "Loss: 78.741, Residuals: 0.015\n",
      "Loss: 78.737, Residuals: 0.000\n",
      "Loss: 78.736, Residuals: 0.001\n",
      "Loss: 78.730, Residuals: 0.002\n",
      "Loss: 78.718, Residuals: 0.003\n",
      "Loss: 78.715, Residuals: 0.006\n",
      "Loss: 78.714, Residuals: 0.002\n",
      "Loss: 78.713, Residuals: 0.005\n",
      "Evidence -541.038\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.06e-02\n",
      "Loss: 108.374, Residuals: -0.011\n",
      "Loss: 108.201, Residuals: -0.028\n",
      "Loss: 107.870, Residuals: -0.032\n",
      "Loss: 107.283, Residuals: -0.040\n",
      "Loss: 106.370, Residuals: -0.046\n",
      "Loss: 106.239, Residuals: -0.041\n",
      "Loss: 105.032, Residuals: -0.031\n",
      "Loss: 104.393, Residuals: -0.078\n",
      "Loss: 104.363, Residuals: -0.059\n",
      "Loss: 104.332, Residuals: -0.064\n",
      "Loss: 104.071, Residuals: -0.062\n",
      "Loss: 103.985, Residuals: -0.028\n",
      "Loss: 103.962, Residuals: -0.040\n",
      "Loss: 103.131, Residuals: -0.021\n",
      "Loss: 103.123, Residuals: -0.020\n",
      "Loss: 103.110, Residuals: -0.018\n",
      "Loss: 102.634, Residuals: -0.004\n",
      "Loss: 102.628, Residuals: -0.009\n",
      "Loss: 102.390, Residuals: 0.000\n",
      "Loss: 102.228, Residuals: 0.017\n",
      "Loss: 102.193, Residuals: 0.001\n",
      "Loss: 102.177, Residuals: 0.005\n",
      "Loss: 102.150, Residuals: 0.008\n",
      "Loss: 102.105, Residuals: 0.010\n",
      "Loss: 100.957, Residuals: -0.054\n",
      "Loss: 100.744, Residuals: -0.006\n",
      "Loss: 100.607, Residuals: 0.039\n",
      "Loss: 100.570, Residuals: 0.044\n",
      "Loss: 100.539, Residuals: 0.064\n",
      "Loss: 100.280, Residuals: 0.082\n",
      "Loss: 100.278, Residuals: 0.080\n",
      "Loss: 100.276, Residuals: 0.081\n",
      "Loss: 100.018, Residuals: 0.096\n",
      "Loss: 100.014, Residuals: 0.094\n",
      "Loss: 100.009, Residuals: 0.093\n",
      "Loss: 100.000, Residuals: 0.092\n",
      "Loss: 99.931, Residuals: 0.099\n",
      "Loss: 99.930, Residuals: 0.099\n",
      "Loss: 99.588, Residuals: 0.106\n",
      "Loss: 99.587, Residuals: 0.103\n",
      "Loss: 99.582, Residuals: 0.104\n",
      "Loss: 99.575, Residuals: 0.103\n",
      "Loss: 99.327, Residuals: 0.111\n",
      "Loss: 99.326, Residuals: 0.109\n",
      "Evidence -205.015\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 5.58e-02\n",
      "Loss: 132.411, Residuals: 0.086\n",
      "Loss: 132.221, Residuals: 0.097\n",
      "Loss: 131.866, Residuals: 0.082\n",
      "Loss: 131.258, Residuals: 0.048\n",
      "Loss: 130.252, Residuals: 0.016\n",
      "Loss: 130.247, Residuals: 0.022\n",
      "Loss: 129.522, Residuals: 0.012\n",
      "Loss: 129.464, Residuals: 0.016\n",
      "Loss: 127.399, Residuals: 0.058\n",
      "Loss: 127.384, Residuals: 0.058\n",
      "Loss: 127.244, Residuals: 0.064\n",
      "Loss: 126.982, Residuals: 0.079\n",
      "Loss: 126.896, Residuals: 0.088\n",
      "Loss: 126.101, Residuals: 0.103\n",
      "Loss: 126.087, Residuals: 0.107\n",
      "Loss: 124.121, Residuals: 0.134\n",
      "Loss: 124.116, Residuals: 0.128\n",
      "Loss: 124.107, Residuals: 0.132\n",
      "Loss: 124.090, Residuals: 0.137\n",
      "Loss: 124.063, Residuals: 0.147\n",
      "Loss: 124.013, Residuals: 0.152\n",
      "Loss: 123.926, Residuals: 0.164\n",
      "Loss: 123.792, Residuals: 0.170\n",
      "Loss: 123.777, Residuals: 0.173\n",
      "Loss: 123.773, Residuals: 0.174\n",
      "Loss: 123.089, Residuals: 0.186\n",
      "Loss: 123.087, Residuals: 0.183\n",
      "Loss: 123.015, Residuals: 0.190\n",
      "Loss: 122.991, Residuals: 0.182\n",
      "Loss: 122.764, Residuals: 0.185\n",
      "Loss: 122.520, Residuals: 0.185\n",
      "Loss: 122.518, Residuals: 0.189\n",
      "Loss: 122.502, Residuals: 0.190\n",
      "Loss: 122.472, Residuals: 0.190\n",
      "Loss: 122.424, Residuals: 0.189\n",
      "Loss: 122.415, Residuals: 0.188\n",
      "Loss: 122.398, Residuals: 0.187\n",
      "Loss: 122.235, Residuals: 0.190\n",
      "Loss: 122.192, Residuals: 0.190\n",
      "Loss: 122.187, Residuals: 0.191\n",
      "Loss: 121.980, Residuals: 0.197\n",
      "Loss: 121.976, Residuals: 0.196\n",
      "Loss: 121.946, Residuals: 0.193\n",
      "Loss: 121.942, Residuals: 0.192\n",
      "Loss: 121.786, Residuals: 0.199\n",
      "Loss: 121.782, Residuals: 0.198\n",
      "Loss: 121.775, Residuals: 0.198\n",
      "Loss: 121.761, Residuals: 0.197\n",
      "Loss: 121.738, Residuals: 0.196\n",
      "Loss: 121.724, Residuals: 0.196\n",
      "Loss: 121.722, Residuals: 0.195\n",
      "Loss: 121.703, Residuals: 0.196\n",
      "Loss: 121.668, Residuals: 0.197\n",
      "Loss: 121.652, Residuals: 0.202\n",
      "Loss: 121.650, Residuals: 0.201\n",
      "Loss: 121.630, Residuals: 0.199\n",
      "Loss: 121.626, Residuals: 0.195\n",
      "Loss: 121.617, Residuals: 0.196\n",
      "Loss: 121.602, Residuals: 0.200\n",
      "Loss: 121.600, Residuals: 0.201\n",
      "Loss: 121.596, Residuals: 0.202\n",
      "Loss: 121.589, Residuals: 0.202\n",
      "Loss: 121.576, Residuals: 0.203\n",
      "Loss: 121.564, Residuals: 0.203\n",
      "Loss: 121.562, Residuals: 0.202\n",
      "Loss: 121.542, Residuals: 0.204\n",
      "Loss: 121.539, Residuals: 0.203\n",
      "Loss: 121.533, Residuals: 0.204\n",
      "Loss: 121.532, Residuals: 0.203\n",
      "Loss: 121.522, Residuals: 0.204\n",
      "Loss: 121.504, Residuals: 0.206\n",
      "Loss: 121.500, Residuals: 0.209\n",
      "Loss: 121.492, Residuals: 0.206\n",
      "Loss: 121.491, Residuals: 0.206\n",
      "Loss: 121.481, Residuals: 0.208\n",
      "Loss: 121.479, Residuals: 0.208\n",
      "Loss: 121.478, Residuals: 0.208\n",
      "Loss: 121.474, Residuals: 0.207\n",
      "Loss: 121.473, Residuals: 0.208\n",
      "Loss: 121.473, Residuals: 0.209\n",
      "Loss: 121.468, Residuals: 0.210\n",
      "Loss: 121.464, Residuals: 0.209\n",
      "Loss: 121.463, Residuals: 0.207\n",
      "Loss: 121.462, Residuals: 0.211\n",
      "Loss: 121.461, Residuals: 0.211\n",
      "Loss: 121.454, Residuals: 0.212\n",
      "Loss: 121.454, Residuals: 0.212\n",
      "Loss: 121.454, Residuals: 0.212\n",
      "Loss: 121.451, Residuals: 0.213\n",
      "Loss: 121.450, Residuals: 0.212\n",
      "Loss: 121.450, Residuals: 0.213\n",
      "Loss: 121.449, Residuals: 0.213\n",
      "Loss: 121.448, Residuals: 0.212\n",
      "Loss: 121.447, Residuals: 0.212\n",
      "Loss: 121.447, Residuals: 0.212\n",
      "Loss: 121.447, Residuals: 0.212\n",
      "Loss: 121.447, Residuals: 0.213\n",
      "Loss: 121.444, Residuals: 0.214\n",
      "Loss: 121.444, Residuals: 0.215\n",
      "Loss: 121.444, Residuals: 0.215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 121.443, Residuals: 0.214\n",
      "Loss: 121.443, Residuals: 0.214\n",
      "Loss: 121.443, Residuals: 0.214\n",
      "Loss: 121.443, Residuals: 0.214\n",
      "Loss: 121.443, Residuals: 0.214\n",
      "Loss: 121.443, Residuals: 0.214\n",
      "Evidence -162.198\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.70e-01\n",
      "Loss: 140.599, Residuals: 0.163\n",
      "Loss: 139.748, Residuals: 0.127\n",
      "Loss: 138.856, Residuals: 0.082\n",
      "Loss: 138.850, Residuals: 0.090\n",
      "Loss: 137.834, Residuals: 0.120\n",
      "Loss: 136.175, Residuals: 0.193\n",
      "Loss: 135.732, Residuals: 0.247\n",
      "Loss: 135.712, Residuals: 0.227\n",
      "Loss: 135.675, Residuals: 0.229\n",
      "Loss: 135.323, Residuals: 0.239\n",
      "Loss: 134.706, Residuals: 0.264\n",
      "Loss: 134.670, Residuals: 0.268\n",
      "Loss: 134.663, Residuals: 0.264\n",
      "Loss: 134.356, Residuals: 0.275\n",
      "Loss: 133.859, Residuals: 0.305\n",
      "Loss: 133.856, Residuals: 0.305\n",
      "Loss: 133.834, Residuals: 0.306\n",
      "Loss: 133.646, Residuals: 0.313\n",
      "Loss: 133.609, Residuals: 0.316\n",
      "Loss: 133.599, Residuals: 0.309\n",
      "Loss: 133.596, Residuals: 0.311\n",
      "Loss: 133.563, Residuals: 0.312\n",
      "Loss: 133.504, Residuals: 0.316\n",
      "Loss: 133.419, Residuals: 0.324\n",
      "Loss: 133.414, Residuals: 0.322\n",
      "Loss: 133.407, Residuals: 0.318\n",
      "Loss: 133.406, Residuals: 0.318\n",
      "Loss: 133.394, Residuals: 0.320\n",
      "Loss: 133.392, Residuals: 0.322\n",
      "Loss: 133.391, Residuals: 0.318\n",
      "Loss: 133.389, Residuals: 0.319\n",
      "Loss: 133.384, Residuals: 0.322\n",
      "Loss: 133.384, Residuals: 0.323\n",
      "Loss: 133.383, Residuals: 0.323\n",
      "Loss: 133.383, Residuals: 0.323\n",
      "Loss: 133.382, Residuals: 0.323\n",
      "Loss: 133.381, Residuals: 0.322\n",
      "Loss: 133.381, Residuals: 0.322\n",
      "Loss: 133.381, Residuals: 0.322\n",
      "Loss: 133.381, Residuals: 0.323\n",
      "Loss: 133.380, Residuals: 0.323\n",
      "Loss: 133.380, Residuals: 0.323\n",
      "Loss: 133.380, Residuals: 0.323\n",
      "Loss: 133.380, Residuals: 0.323\n",
      "Evidence -125.190\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 5.92e-01\n",
      "Loss: 147.137, Residuals: 0.298\n",
      "Loss: 146.555, Residuals: 0.271\n",
      "Loss: 145.534, Residuals: 0.307\n",
      "Loss: 144.211, Residuals: 0.408\n",
      "Loss: 144.092, Residuals: 0.368\n",
      "Loss: 143.879, Residuals: 0.365\n",
      "Loss: 143.555, Residuals: 0.370\n",
      "Loss: 143.536, Residuals: 0.358\n",
      "Loss: 143.501, Residuals: 0.363\n",
      "Loss: 143.466, Residuals: 0.365\n",
      "Loss: 143.407, Residuals: 0.370\n",
      "Loss: 143.358, Residuals: 0.389\n",
      "Loss: 143.349, Residuals: 0.381\n",
      "Loss: 143.346, Residuals: 0.382\n",
      "Loss: 143.344, Residuals: 0.383\n",
      "Loss: 143.344, Residuals: 0.384\n",
      "Loss: 143.343, Residuals: 0.384\n",
      "Loss: 143.342, Residuals: 0.385\n",
      "Loss: 143.341, Residuals: 0.386\n",
      "Loss: 143.341, Residuals: 0.386\n",
      "Loss: 143.341, Residuals: 0.385\n",
      "Evidence -109.802\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 9.22e-01\n",
      "Loss: 151.082, Residuals: 0.350\n",
      "Loss: 150.597, Residuals: 0.360\n",
      "Loss: 149.792, Residuals: 0.383\n",
      "Loss: 149.101, Residuals: 0.408\n",
      "Loss: 149.009, Residuals: 0.405\n",
      "Loss: 148.874, Residuals: 0.392\n",
      "Loss: 148.855, Residuals: 0.388\n",
      "Loss: 148.824, Residuals: 0.393\n",
      "Loss: 148.823, Residuals: 0.395\n",
      "Loss: 148.809, Residuals: 0.398\n",
      "Loss: 148.809, Residuals: 0.399\n",
      "Loss: 148.807, Residuals: 0.400\n",
      "Loss: 148.803, Residuals: 0.401\n",
      "Loss: 148.798, Residuals: 0.404\n",
      "Loss: 148.798, Residuals: 0.406\n",
      "Loss: 148.798, Residuals: 0.405\n",
      "Loss: 148.797, Residuals: 0.405\n",
      "Loss: 148.797, Residuals: 0.405\n",
      "Evidence -102.511\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.17e+00\n",
      "Loss: 152.779, Residuals: 0.346\n",
      "Loss: 152.591, Residuals: 0.390\n",
      "Loss: 152.327, Residuals: 0.393\n",
      "Loss: 152.183, Residuals: 0.396\n",
      "Loss: 152.164, Residuals: 0.407\n",
      "Loss: 152.148, Residuals: 0.398\n",
      "Loss: 152.144, Residuals: 0.395\n",
      "Loss: 152.143, Residuals: 0.399\n",
      "Loss: 152.133, Residuals: 0.401\n",
      "Loss: 152.132, Residuals: 0.401\n",
      "Loss: 152.127, Residuals: 0.402\n",
      "Loss: 152.122, Residuals: 0.407\n",
      "Loss: 152.122, Residuals: 0.408\n",
      "Loss: 152.122, Residuals: 0.407\n",
      "Loss: 152.122, Residuals: 0.407\n",
      "Loss: 152.122, Residuals: 0.407\n",
      "Evidence -98.684\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.30e+00\n",
      "Loss: 154.467, Residuals: 0.356\n",
      "Loss: 154.291, Residuals: 0.405\n",
      "Loss: 154.270, Residuals: 0.391\n",
      "Loss: 154.264, Residuals: 0.389\n",
      "Loss: 154.252, Residuals: 0.390\n",
      "Loss: 154.232, Residuals: 0.390\n",
      "Loss: 154.206, Residuals: 0.390\n",
      "Loss: 154.206, Residuals: 0.392\n",
      "Loss: 154.203, Residuals: 0.393\n",
      "Loss: 154.199, Residuals: 0.396\n",
      "Loss: 154.199, Residuals: 0.397\n",
      "Loss: 154.199, Residuals: 0.397\n",
      "Loss: 154.199, Residuals: 0.397\n",
      "Loss: 154.199, Residuals: 0.397\n",
      "Loss: 154.199, Residuals: 0.397\n",
      "Evidence -96.720\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.37e+00\n",
      "Loss: 155.386, Residuals: 0.371\n",
      "Loss: 155.305, Residuals: 0.398\n",
      "Loss: 155.263, Residuals: 0.389\n",
      "Loss: 155.259, Residuals: 0.397\n",
      "Loss: 155.255, Residuals: 0.392\n",
      "Loss: 155.251, Residuals: 0.392\n",
      "Loss: 155.250, Residuals: 0.392\n",
      "Loss: 155.250, Residuals: 0.393\n",
      "Loss: 155.250, Residuals: 0.393\n",
      "Loss: 155.249, Residuals: 0.394\n",
      "Loss: 155.249, Residuals: 0.395\n",
      "Loss: 155.249, Residuals: 0.395\n",
      "Evidence -95.614\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.41e+00\n",
      "Loss: 155.830, Residuals: 0.380\n",
      "Loss: 155.802, Residuals: 0.388\n",
      "Loss: 155.794, Residuals: 0.394\n",
      "Loss: 155.783, Residuals: 0.393\n",
      "Loss: 155.775, Residuals: 0.395\n",
      "Loss: 155.774, Residuals: 0.397\n",
      "Loss: 155.774, Residuals: 0.397\n",
      "Loss: 155.773, Residuals: 0.397\n",
      "Loss: 155.773, Residuals: 0.397\n",
      "Loss: 155.773, Residuals: 0.397\n",
      "Loss: 155.772, Residuals: 0.398\n",
      "Loss: 155.772, Residuals: 0.398\n",
      "Evidence -94.910\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.43e+00\n",
      "Loss: 156.122, Residuals: 0.384\n",
      "Loss: 156.110, Residuals: 0.407\n",
      "Loss: 156.093, Residuals: 0.402\n",
      "Loss: 156.084, Residuals: 0.398\n",
      "Loss: 156.082, Residuals: 0.401\n",
      "Loss: 156.082, Residuals: 0.402\n",
      "Loss: 156.081, Residuals: 0.402\n",
      "Loss: 156.080, Residuals: 0.402\n",
      "Loss: 156.080, Residuals: 0.403\n",
      "Loss: 156.080, Residuals: 0.403\n",
      "Loss: 156.080, Residuals: 0.403\n",
      "Evidence -94.431\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.45e+00\n",
      "Loss: 156.315, Residuals: 0.394\n",
      "Loss: 156.302, Residuals: 0.406\n",
      "Loss: 156.289, Residuals: 0.405\n",
      "Loss: 156.288, Residuals: 0.409\n",
      "Loss: 156.287, Residuals: 0.408\n",
      "Loss: 156.286, Residuals: 0.409\n",
      "Loss: 156.286, Residuals: 0.410\n",
      "Loss: 156.286, Residuals: 0.410\n",
      "Loss: 156.286, Residuals: 0.410\n",
      "Loss: 156.286, Residuals: 0.410\n",
      "Evidence -94.106\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.47e+00\n",
      "Loss: 156.458, Residuals: 0.404\n",
      "Loss: 156.449, Residuals: 0.413\n",
      "Loss: 156.440, Residuals: 0.414\n",
      "Loss: 156.440, Residuals: 0.415\n",
      "Loss: 156.439, Residuals: 0.415\n",
      "Loss: 156.439, Residuals: 0.415\n",
      "Loss: 156.438, Residuals: 0.416\n",
      "Loss: 156.438, Residuals: 0.416\n",
      "Loss: 156.438, Residuals: 0.416\n",
      "Loss: 156.438, Residuals: 0.416\n",
      "Loss: 156.438, Residuals: 0.416\n",
      "Evidence -93.865\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.48e+00\n",
      "Loss: 156.570, Residuals: 0.412\n",
      "Loss: 156.563, Residuals: 0.419\n",
      "Loss: 156.557, Residuals: 0.420\n",
      "Loss: 156.556, Residuals: 0.422\n",
      "Loss: 156.556, Residuals: 0.422\n",
      "Loss: 156.555, Residuals: 0.421\n",
      "Loss: 156.555, Residuals: 0.421\n",
      "Loss: 156.555, Residuals: 0.421\n",
      "Evidence -93.691\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.49e+00\n",
      "Loss: 156.660, Residuals: 0.418\n",
      "Loss: 156.654, Residuals: 0.424\n",
      "Loss: 156.653, Residuals: 0.425\n",
      "Loss: 156.651, Residuals: 0.425\n",
      "Loss: 156.649, Residuals: 0.426\n",
      "Evidence -93.572\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.50e+00\n",
      "Loss: 156.732, Residuals: 0.423\n",
      "Loss: 156.728, Residuals: 0.430\n",
      "Loss: 156.727, Residuals: 0.428\n",
      "Loss: 156.726, Residuals: 0.429\n",
      "Loss: 156.723, Residuals: 0.430\n",
      "Loss: 156.723, Residuals: 0.431\n",
      "Loss: 156.723, Residuals: 0.431\n",
      "Loss: 156.723, Residuals: 0.431\n",
      "Loss: 156.723, Residuals: 0.431\n",
      "Evidence -93.494\n",
      "Pass count  1\n",
      "Total samples: 38, Updated regularization: 1.00e-05\n",
      "Loss: 398.373, Residuals: -2.033\n",
      "Loss: 307.277, Residuals: -1.445\n",
      "Loss: 266.152, Residuals: -0.368\n",
      "Loss: 251.696, Residuals: 0.525\n",
      "Loss: 176.052, Residuals: -0.340\n",
      "Loss: 168.352, Residuals: -0.080\n",
      "Loss: 155.248, Residuals: 0.114\n",
      "Loss: 136.561, Residuals: 0.052\n",
      "Loss: 133.079, Residuals: -0.028\n",
      "Loss: 125.968, Residuals: 0.126\n",
      "Loss: 116.799, Residuals: 0.497\n",
      "Loss: 115.849, Residuals: 0.533\n",
      "Loss: 108.658, Residuals: 0.480\n",
      "Loss: 108.415, Residuals: 0.437\n",
      "Loss: 108.016, Residuals: 0.476\n",
      "Loss: 107.260, Residuals: 0.450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 106.854, Residuals: 0.503\n",
      "Loss: 103.309, Residuals: 0.438\n",
      "Loss: 103.264, Residuals: 0.425\n",
      "Loss: 101.511, Residuals: 0.384\n",
      "Loss: 98.470, Residuals: 0.311\n",
      "Loss: 97.825, Residuals: 0.385\n",
      "Loss: 96.602, Residuals: 0.339\n",
      "Loss: 94.409, Residuals: 0.262\n",
      "Loss: 93.186, Residuals: 0.246\n",
      "Loss: 93.110, Residuals: 0.264\n",
      "Loss: 90.425, Residuals: 0.205\n",
      "Loss: 90.381, Residuals: 0.175\n",
      "Loss: 88.751, Residuals: 0.150\n",
      "Loss: 88.745, Residuals: 0.142\n",
      "Loss: 87.693, Residuals: 0.128\n",
      "Loss: 87.394, Residuals: 0.218\n",
      "Loss: 87.289, Residuals: 0.221\n",
      "Loss: 86.305, Residuals: 0.199\n",
      "Loss: 84.611, Residuals: 0.175\n",
      "Loss: 84.594, Residuals: 0.159\n",
      "Loss: 84.570, Residuals: 0.140\n",
      "Loss: 84.351, Residuals: 0.141\n",
      "Loss: 83.971, Residuals: 0.142\n",
      "Loss: 83.959, Residuals: 0.154\n",
      "Loss: 82.175, Residuals: 0.155\n",
      "Loss: 82.158, Residuals: 0.139\n",
      "Loss: 82.132, Residuals: 0.121\n",
      "Loss: 82.085, Residuals: 0.128\n",
      "Loss: 82.012, Residuals: 0.144\n",
      "Loss: 79.590, Residuals: 0.150\n",
      "Loss: 79.520, Residuals: 0.117\n",
      "Loss: 79.406, Residuals: 0.114\n",
      "Loss: 79.208, Residuals: 0.119\n",
      "Loss: 78.890, Residuals: 0.130\n",
      "Loss: 78.358, Residuals: 0.175\n",
      "Loss: 78.278, Residuals: 0.202\n",
      "Loss: 75.819, Residuals: 0.087\n",
      "Loss: 75.654, Residuals: 0.111\n",
      "Loss: 75.397, Residuals: 0.131\n",
      "Loss: 74.954, Residuals: 0.158\n",
      "Loss: 74.341, Residuals: 0.193\n",
      "Loss: 74.264, Residuals: 0.227\n",
      "Loss: 73.538, Residuals: 0.211\n",
      "Loss: 72.973, Residuals: 0.213\n",
      "Loss: 72.921, Residuals: 0.210\n",
      "Loss: 72.452, Residuals: 0.203\n",
      "Loss: 72.385, Residuals: 0.192\n",
      "Loss: 72.377, Residuals: 0.199\n",
      "Loss: 72.046, Residuals: 0.196\n",
      "Loss: 71.450, Residuals: 0.181\n",
      "Loss: 71.428, Residuals: 0.169\n",
      "Loss: 70.632, Residuals: 0.157\n",
      "Loss: 70.523, Residuals: 0.164\n",
      "Loss: 70.507, Residuals: 0.160\n",
      "Loss: 70.489, Residuals: 0.152\n",
      "Loss: 69.824, Residuals: 0.141\n",
      "Loss: 69.673, Residuals: 0.146\n",
      "Loss: 69.611, Residuals: 0.148\n",
      "Loss: 69.594, Residuals: 0.131\n",
      "Loss: 69.589, Residuals: 0.139\n",
      "Loss: 69.587, Residuals: 0.139\n",
      "Loss: 69.586, Residuals: 0.138\n",
      "Loss: 69.082, Residuals: 0.125\n",
      "Loss: 69.079, Residuals: 0.124\n",
      "Loss: 68.669, Residuals: 0.110\n",
      "Loss: 68.658, Residuals: 0.100\n",
      "Loss: 68.653, Residuals: 0.109\n",
      "Loss: 68.652, Residuals: 0.106\n",
      "Loss: 67.894, Residuals: 0.075\n",
      "Loss: 67.885, Residuals: 0.087\n",
      "Loss: 67.880, Residuals: 0.082\n",
      "Loss: 67.871, Residuals: 0.081\n",
      "Loss: 67.789, Residuals: 0.082\n",
      "Loss: 67.761, Residuals: 0.080\n",
      "Loss: 66.816, Residuals: 0.060\n",
      "Loss: 66.777, Residuals: 0.086\n",
      "Loss: 66.748, Residuals: 0.074\n",
      "Loss: 66.732, Residuals: 0.069\n",
      "Loss: 66.716, Residuals: 0.058\n",
      "Loss: 66.570, Residuals: 0.065\n",
      "Loss: 66.531, Residuals: 0.049\n",
      "Loss: 66.515, Residuals: 0.049\n",
      "Loss: 65.943, Residuals: 0.049\n",
      "Loss: 65.936, Residuals: 0.042\n",
      "Loss: 65.929, Residuals: 0.035\n",
      "Loss: 65.925, Residuals: 0.034\n",
      "Loss: 65.352, Residuals: 0.064\n",
      "Loss: 65.342, Residuals: 0.045\n",
      "Loss: 65.327, Residuals: 0.050\n",
      "Loss: 65.208, Residuals: 0.054\n",
      "Loss: 65.207, Residuals: 0.053\n",
      "Loss: 64.764, Residuals: 0.052\n",
      "Loss: 64.758, Residuals: 0.047\n",
      "Loss: 64.755, Residuals: 0.044\n",
      "Loss: 64.726, Residuals: 0.037\n",
      "Loss: 64.470, Residuals: 0.055\n",
      "Loss: 64.467, Residuals: 0.057\n",
      "Loss: 64.462, Residuals: 0.058\n",
      "Loss: 64.454, Residuals: 0.057\n",
      "Loss: 64.437, Residuals: 0.053\n",
      "Loss: 64.408, Residuals: 0.046\n",
      "Loss: 64.396, Residuals: 0.054\n",
      "Loss: 63.974, Residuals: 0.060\n",
      "Loss: 63.971, Residuals: 0.054\n",
      "Evidence -543.568\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 2.30e-02\n",
      "Loss: 106.101, Residuals: 0.091\n",
      "Loss: 106.003, Residuals: 0.080\n",
      "Loss: 105.090, Residuals: 0.063\n",
      "Loss: 104.300, Residuals: 0.037\n",
      "Loss: 104.281, Residuals: 0.045\n",
      "Loss: 104.116, Residuals: 0.041\n",
      "Loss: 102.759, Residuals: 0.043\n",
      "Loss: 102.588, Residuals: 0.053\n",
      "Loss: 101.041, Residuals: 0.089\n",
      "Loss: 99.246, Residuals: 0.234\n",
      "Loss: 99.157, Residuals: 0.172\n",
      "Loss: 99.000, Residuals: 0.173\n",
      "Loss: 98.772, Residuals: 0.174\n",
      "Loss: 98.771, Residuals: 0.171\n",
      "Evidence -165.899\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 4.22e-02\n",
      "Loss: 131.099, Residuals: 0.163\n",
      "Loss: 130.582, Residuals: 0.097\n",
      "Loss: 130.359, Residuals: 0.160\n",
      "Loss: 128.437, Residuals: 0.189\n",
      "Loss: 125.784, Residuals: 0.248\n",
      "Loss: 125.770, Residuals: 0.234\n",
      "Loss: 125.746, Residuals: 0.224\n",
      "Loss: 125.718, Residuals: 0.206\n",
      "Loss: 124.628, Residuals: 0.215\n",
      "Loss: 123.124, Residuals: 0.225\n",
      "Loss: 123.083, Residuals: 0.243\n",
      "Loss: 123.021, Residuals: 0.245\n",
      "Loss: 122.961, Residuals: 0.254\n",
      "Loss: 122.960, Residuals: 0.254\n",
      "Loss: 122.673, Residuals: 0.255\n",
      "Loss: 122.672, Residuals: 0.251\n",
      "Loss: 122.524, Residuals: 0.254\n",
      "Loss: 122.487, Residuals: 0.249\n",
      "Loss: 122.484, Residuals: 0.252\n",
      "Loss: 122.482, Residuals: 0.252\n",
      "Loss: 122.166, Residuals: 0.256\n",
      "Loss: 122.164, Residuals: 0.250\n",
      "Loss: 122.159, Residuals: 0.250\n",
      "Loss: 121.995, Residuals: 0.252\n",
      "Loss: 121.994, Residuals: 0.250\n",
      "Loss: 121.991, Residuals: 0.250\n",
      "Loss: 121.987, Residuals: 0.250\n",
      "Loss: 121.820, Residuals: 0.255\n",
      "Loss: 121.819, Residuals: 0.254\n",
      "Loss: 121.817, Residuals: 0.253\n",
      "Loss: 121.813, Residuals: 0.253\n",
      "Loss: 121.807, Residuals: 0.251\n",
      "Loss: 121.593, Residuals: 0.265\n",
      "Loss: 121.585, Residuals: 0.256\n",
      "Loss: 121.579, Residuals: 0.255\n",
      "Loss: 121.577, Residuals: 0.254\n",
      "Loss: 121.577, Residuals: 0.254\n",
      "Loss: 121.571, Residuals: 0.254\n",
      "Loss: 121.562, Residuals: 0.254\n",
      "Loss: 121.545, Residuals: 0.255\n",
      "Loss: 121.545, Residuals: 0.256\n",
      "Loss: 121.541, Residuals: 0.256\n",
      "Loss: 121.535, Residuals: 0.256\n",
      "Loss: 121.534, Residuals: 0.256\n",
      "Loss: 121.533, Residuals: 0.255\n",
      "Loss: 121.467, Residuals: 0.258\n",
      "Loss: 121.467, Residuals: 0.258\n",
      "Loss: 121.466, Residuals: 0.258\n",
      "Loss: 121.466, Residuals: 0.258\n",
      "Loss: 121.460, Residuals: 0.257\n",
      "Loss: 121.459, Residuals: 0.258\n",
      "Loss: 121.459, Residuals: 0.258\n",
      "Evidence -131.598\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 6.90e-02\n",
      "Loss: 140.514, Residuals: 0.268\n",
      "Loss: 139.951, Residuals: 0.271\n",
      "Loss: 139.341, Residuals: 0.263\n",
      "Loss: 138.504, Residuals: 0.296\n",
      "Loss: 138.481, Residuals: 0.294\n",
      "Loss: 138.266, Residuals: 0.302\n",
      "Loss: 137.889, Residuals: 0.318\n",
      "Loss: 137.367, Residuals: 0.358\n",
      "Loss: 137.361, Residuals: 0.359\n",
      "Loss: 137.099, Residuals: 0.367\n",
      "Loss: 137.070, Residuals: 0.371\n",
      "Loss: 137.045, Residuals: 0.375\n",
      "Loss: 137.035, Residuals: 0.362\n",
      "Loss: 137.030, Residuals: 0.369\n",
      "Loss: 136.335, Residuals: 0.386\n",
      "Loss: 136.318, Residuals: 0.366\n",
      "Loss: 136.309, Residuals: 0.366\n",
      "Loss: 136.221, Residuals: 0.372\n",
      "Loss: 136.059, Residuals: 0.382\n",
      "Loss: 136.056, Residuals: 0.379\n",
      "Loss: 136.051, Residuals: 0.378\n",
      "Loss: 136.041, Residuals: 0.377\n",
      "Loss: 135.679, Residuals: 0.394\n",
      "Loss: 135.669, Residuals: 0.390\n",
      "Loss: 135.658, Residuals: 0.382\n",
      "Loss: 135.646, Residuals: 0.381\n",
      "Loss: 135.625, Residuals: 0.380\n",
      "Loss: 135.596, Residuals: 0.377\n",
      "Loss: 135.595, Residuals: 0.376\n",
      "Loss: 135.402, Residuals: 0.383\n",
      "Loss: 135.400, Residuals: 0.383\n",
      "Loss: 135.399, Residuals: 0.380\n",
      "Loss: 135.384, Residuals: 0.379\n",
      "Loss: 134.956, Residuals: 0.432\n",
      "Loss: 134.893, Residuals: 0.409\n",
      "Loss: 134.864, Residuals: 0.392\n",
      "Loss: 134.822, Residuals: 0.389\n",
      "Loss: 134.762, Residuals: 0.386\n",
      "Loss: 134.705, Residuals: 0.380\n",
      "Loss: 134.700, Residuals: 0.385\n",
      "Loss: 134.691, Residuals: 0.385\n",
      "Loss: 134.614, Residuals: 0.393\n",
      "Loss: 134.613, Residuals: 0.395\n",
      "Evidence -115.712\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.02e-01\n",
      "Loss: 145.444, Residuals: 0.416\n",
      "Loss: 144.979, Residuals: 0.405\n",
      "Loss: 144.288, Residuals: 0.381\n",
      "Loss: 143.308, Residuals: 0.398\n",
      "Loss: 143.288, Residuals: 0.381\n",
      "Loss: 142.561, Residuals: 0.420\n",
      "Loss: 142.551, Residuals: 0.436\n",
      "Loss: 142.144, Residuals: 0.463\n",
      "Loss: 142.046, Residuals: 0.473\n",
      "Loss: 141.997, Residuals: 0.459\n",
      "Loss: 141.965, Residuals: 0.474\n",
      "Loss: 141.686, Residuals: 0.491\n",
      "Loss: 141.684, Residuals: 0.492\n",
      "Loss: 141.464, Residuals: 0.513\n",
      "Loss: 141.461, Residuals: 0.512\n",
      "Loss: 141.456, Residuals: 0.512\n",
      "Loss: 141.411, Residuals: 0.516\n",
      "Loss: 141.388, Residuals: 0.512\n",
      "Loss: 141.382, Residuals: 0.513\n",
      "Loss: 141.328, Residuals: 0.519\n",
      "Loss: 141.327, Residuals: 0.520\n",
      "Loss: 141.286, Residuals: 0.527\n",
      "Loss: 141.286, Residuals: 0.527\n",
      "Loss: 141.283, Residuals: 0.526\n",
      "Loss: 141.260, Residuals: 0.531\n",
      "Loss: 141.260, Residuals: 0.530\n",
      "Loss: 141.259, Residuals: 0.530\n",
      "Loss: 141.258, Residuals: 0.529\n",
      "Loss: 141.256, Residuals: 0.528\n",
      "Loss: 141.253, Residuals: 0.527\n",
      "Loss: 141.252, Residuals: 0.526\n",
      "Loss: 141.243, Residuals: 0.529\n",
      "Loss: 141.243, Residuals: 0.529\n",
      "Loss: 141.243, Residuals: 0.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 141.240, Residuals: 0.530\n",
      "Loss: 141.240, Residuals: 0.531\n",
      "Loss: 141.240, Residuals: 0.530\n",
      "Loss: 141.237, Residuals: 0.532\n",
      "Loss: 141.237, Residuals: 0.532\n",
      "Evidence -105.450\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.21e-01\n",
      "Loss: 147.530, Residuals: 0.490\n",
      "Loss: 147.158, Residuals: 0.479\n",
      "Loss: 146.578, Residuals: 0.454\n",
      "Loss: 146.515, Residuals: 0.480\n",
      "Loss: 146.001, Residuals: 0.499\n",
      "Loss: 145.989, Residuals: 0.487\n",
      "Loss: 145.550, Residuals: 0.515\n",
      "Loss: 145.543, Residuals: 0.512\n",
      "Loss: 145.478, Residuals: 0.505\n",
      "Loss: 144.915, Residuals: 0.528\n",
      "Loss: 144.906, Residuals: 0.524\n",
      "Loss: 144.831, Residuals: 0.519\n",
      "Loss: 144.799, Residuals: 0.505\n",
      "Loss: 144.522, Residuals: 0.520\n",
      "Loss: 144.515, Residuals: 0.517\n",
      "Loss: 144.454, Residuals: 0.507\n",
      "Loss: 144.379, Residuals: 0.499\n",
      "Loss: 144.373, Residuals: 0.491\n",
      "Loss: 144.138, Residuals: 0.504\n",
      "Loss: 144.136, Residuals: 0.503\n",
      "Loss: 143.865, Residuals: 0.519\n",
      "Loss: 143.861, Residuals: 0.516\n",
      "Loss: 143.851, Residuals: 0.516\n",
      "Loss: 143.834, Residuals: 0.516\n",
      "Loss: 143.804, Residuals: 0.514\n",
      "Loss: 143.582, Residuals: 0.529\n",
      "Loss: 143.577, Residuals: 0.526\n",
      "Loss: 143.568, Residuals: 0.524\n",
      "Loss: 143.553, Residuals: 0.522\n",
      "Loss: 143.533, Residuals: 0.519\n",
      "Loss: 143.531, Residuals: 0.514\n",
      "Loss: 143.485, Residuals: 0.518\n",
      "Loss: 143.405, Residuals: 0.525\n",
      "Loss: 143.404, Residuals: 0.522\n",
      "Loss: 143.396, Residuals: 0.518\n",
      "Loss: 143.385, Residuals: 0.515\n",
      "Loss: 143.384, Residuals: 0.511\n",
      "Loss: 143.383, Residuals: 0.512\n",
      "Loss: 143.350, Residuals: 0.512\n",
      "Loss: 143.293, Residuals: 0.520\n",
      "Loss: 143.293, Residuals: 0.516\n",
      "Loss: 143.292, Residuals: 0.516\n",
      "Loss: 143.285, Residuals: 0.517\n",
      "Loss: 143.283, Residuals: 0.515\n",
      "Loss: 143.282, Residuals: 0.514\n",
      "Loss: 143.228, Residuals: 0.517\n",
      "Loss: 143.228, Residuals: 0.514\n",
      "Loss: 143.226, Residuals: 0.514\n",
      "Loss: 143.224, Residuals: 0.513\n",
      "Loss: 143.224, Residuals: 0.511\n",
      "Loss: 143.221, Residuals: 0.512\n",
      "Loss: 143.221, Residuals: 0.512\n",
      "Loss: 143.192, Residuals: 0.513\n",
      "Loss: 143.192, Residuals: 0.512\n",
      "Loss: 143.188, Residuals: 0.511\n",
      "Loss: 143.188, Residuals: 0.510\n",
      "Evidence -101.610\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.49e-01\n",
      "Loss: 146.530, Residuals: 0.479\n",
      "Loss: 146.058, Residuals: 0.467\n",
      "Loss: 145.392, Residuals: 0.465\n",
      "Loss: 144.788, Residuals: 0.401\n",
      "Loss: 144.724, Residuals: 0.378\n",
      "Loss: 144.611, Residuals: 0.400\n",
      "Loss: 144.419, Residuals: 0.427\n",
      "Loss: 144.406, Residuals: 0.432\n",
      "Loss: 144.279, Residuals: 0.440\n",
      "Loss: 144.057, Residuals: 0.455\n",
      "Loss: 143.964, Residuals: 0.473\n",
      "Loss: 143.956, Residuals: 0.461\n",
      "Loss: 143.880, Residuals: 0.468\n",
      "Loss: 143.877, Residuals: 0.472\n",
      "Loss: 143.751, Residuals: 0.473\n",
      "Loss: 143.679, Residuals: 0.470\n",
      "Loss: 143.675, Residuals: 0.469\n",
      "Loss: 143.668, Residuals: 0.466\n",
      "Loss: 143.603, Residuals: 0.471\n",
      "Loss: 143.601, Residuals: 0.470\n",
      "Loss: 143.529, Residuals: 0.471\n",
      "Loss: 143.528, Residuals: 0.469\n",
      "Loss: 143.457, Residuals: 0.471\n",
      "Loss: 143.454, Residuals: 0.469\n",
      "Loss: 143.453, Residuals: 0.467\n",
      "Loss: 143.363, Residuals: 0.471\n",
      "Loss: 143.361, Residuals: 0.469\n",
      "Loss: 143.358, Residuals: 0.465\n",
      "Loss: 143.357, Residuals: 0.463\n",
      "Loss: 143.345, Residuals: 0.464\n",
      "Loss: 143.341, Residuals: 0.461\n",
      "Loss: 143.341, Residuals: 0.459\n",
      "Loss: 143.329, Residuals: 0.461\n",
      "Loss: 143.329, Residuals: 0.459\n",
      "Loss: 143.318, Residuals: 0.461\n",
      "Loss: 143.317, Residuals: 0.461\n",
      "Loss: 143.307, Residuals: 0.464\n",
      "Loss: 143.306, Residuals: 0.463\n",
      "Loss: 143.302, Residuals: 0.464\n",
      "Loss: 143.301, Residuals: 0.460\n",
      "Loss: 143.301, Residuals: 0.460\n",
      "Loss: 143.295, Residuals: 0.461\n",
      "Loss: 143.295, Residuals: 0.461\n",
      "Loss: 143.290, Residuals: 0.462\n",
      "Loss: 143.289, Residuals: 0.462\n",
      "Loss: 143.289, Residuals: 0.461\n",
      "Loss: 143.289, Residuals: 0.461\n",
      "Loss: 143.285, Residuals: 0.462\n",
      "Loss: 143.285, Residuals: 0.461\n",
      "Loss: 143.285, Residuals: 0.460\n",
      "Loss: 143.284, Residuals: 0.461\n",
      "Loss: 143.284, Residuals: 0.461\n",
      "Loss: 143.284, Residuals: 0.460\n",
      "Evidence -93.498\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 2.07e-01\n",
      "Loss: 147.169, Residuals: 0.426\n",
      "Loss: 146.963, Residuals: 0.414\n",
      "Loss: 146.645, Residuals: 0.396\n",
      "Loss: 146.346, Residuals: 0.361\n",
      "Loss: 146.283, Residuals: 0.380\n",
      "Loss: 146.260, Residuals: 0.379\n",
      "Loss: 146.064, Residuals: 0.396\n",
      "Loss: 146.009, Residuals: 0.416\n",
      "Loss: 145.991, Residuals: 0.408\n",
      "Loss: 145.844, Residuals: 0.419\n",
      "Loss: 145.842, Residuals: 0.420\n",
      "Loss: 145.765, Residuals: 0.427\n",
      "Loss: 145.763, Residuals: 0.427\n",
      "Loss: 145.757, Residuals: 0.427\n",
      "Loss: 145.714, Residuals: 0.435\n",
      "Loss: 145.712, Residuals: 0.434\n",
      "Loss: 145.708, Residuals: 0.430\n",
      "Loss: 145.678, Residuals: 0.434\n",
      "Loss: 145.678, Residuals: 0.431\n",
      "Loss: 145.662, Residuals: 0.433\n",
      "Loss: 145.661, Residuals: 0.433\n",
      "Loss: 145.661, Residuals: 0.431\n",
      "Loss: 145.652, Residuals: 0.432\n",
      "Loss: 145.652, Residuals: 0.431\n",
      "Loss: 145.649, Residuals: 0.432\n",
      "Loss: 145.648, Residuals: 0.430\n",
      "Loss: 145.648, Residuals: 0.430\n",
      "Loss: 145.646, Residuals: 0.430\n",
      "Loss: 145.646, Residuals: 0.430\n",
      "Loss: 145.642, Residuals: 0.431\n",
      "Loss: 145.641, Residuals: 0.431\n",
      "Evidence -89.487\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 2.52e-01\n",
      "Loss: 148.806, Residuals: 0.394\n",
      "Loss: 148.685, Residuals: 0.386\n",
      "Loss: 148.519, Residuals: 0.399\n",
      "Loss: 148.504, Residuals: 0.396\n",
      "Loss: 148.475, Residuals: 0.399\n",
      "Loss: 148.424, Residuals: 0.406\n",
      "Loss: 148.355, Residuals: 0.421\n",
      "Loss: 148.352, Residuals: 0.422\n",
      "Loss: 148.348, Residuals: 0.420\n",
      "Loss: 148.313, Residuals: 0.428\n",
      "Loss: 148.312, Residuals: 0.427\n",
      "Loss: 148.303, Residuals: 0.428\n",
      "Loss: 148.299, Residuals: 0.429\n",
      "Loss: 148.299, Residuals: 0.427\n",
      "Loss: 148.280, Residuals: 0.430\n",
      "Loss: 148.280, Residuals: 0.429\n",
      "Loss: 148.275, Residuals: 0.430\n",
      "Loss: 148.274, Residuals: 0.430\n",
      "Loss: 148.264, Residuals: 0.430\n",
      "Loss: 148.264, Residuals: 0.429\n",
      "Loss: 148.261, Residuals: 0.429\n",
      "Loss: 148.261, Residuals: 0.428\n",
      "Loss: 148.257, Residuals: 0.428\n",
      "Loss: 148.257, Residuals: 0.428\n",
      "Loss: 148.255, Residuals: 0.429\n",
      "Loss: 148.255, Residuals: 0.429\n",
      "Loss: 148.255, Residuals: 0.428\n",
      "Loss: 148.255, Residuals: 0.427\n",
      "Loss: 148.255, Residuals: 0.427\n",
      "Evidence -87.766\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 2.79e-01\n",
      "Loss: 150.063, Residuals: 0.411\n",
      "Loss: 149.989, Residuals: 0.413\n",
      "Loss: 149.985, Residuals: 0.412\n",
      "Loss: 149.953, Residuals: 0.417\n",
      "Loss: 149.912, Residuals: 0.427\n",
      "Loss: 149.910, Residuals: 0.429\n",
      "Loss: 149.907, Residuals: 0.429\n",
      "Loss: 149.902, Residuals: 0.431\n",
      "Loss: 149.901, Residuals: 0.429\n",
      "Loss: 149.886, Residuals: 0.433\n",
      "Loss: 149.886, Residuals: 0.433\n",
      "Evidence -86.983\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 2.88e-01\n",
      "Loss: 150.797, Residuals: 0.424\n",
      "Loss: 150.771, Residuals: 0.420\n",
      "Loss: 150.764, Residuals: 0.418\n",
      "Loss: 150.749, Residuals: 0.420\n",
      "Loss: 150.726, Residuals: 0.427\n",
      "Loss: 150.724, Residuals: 0.430\n",
      "Loss: 150.711, Residuals: 0.434\n",
      "Loss: 150.710, Residuals: 0.432\n",
      "Loss: 150.696, Residuals: 0.435\n",
      "Loss: 150.686, Residuals: 0.440\n",
      "Loss: 150.685, Residuals: 0.439\n",
      "Loss: 150.683, Residuals: 0.439\n",
      "Loss: 150.674, Residuals: 0.442\n",
      "Loss: 150.674, Residuals: 0.442\n",
      "Loss: 150.671, Residuals: 0.442\n",
      "Loss: 150.671, Residuals: 0.441\n",
      "Loss: 150.668, Residuals: 0.441\n",
      "Loss: 150.668, Residuals: 0.440\n",
      "Evidence -86.529\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.02e-01\n",
      "Loss: 151.166, Residuals: 0.431\n",
      "Loss: 151.149, Residuals: 0.428\n",
      "Loss: 151.145, Residuals: 0.428\n",
      "Loss: 151.117, Residuals: 0.435\n",
      "Loss: 151.116, Residuals: 0.434\n",
      "Loss: 151.111, Residuals: 0.436\n",
      "Loss: 151.101, Residuals: 0.440\n",
      "Loss: 151.100, Residuals: 0.439\n",
      "Loss: 151.095, Residuals: 0.441\n",
      "Loss: 151.087, Residuals: 0.445\n",
      "Loss: 151.087, Residuals: 0.444\n",
      "Loss: 151.087, Residuals: 0.444\n",
      "Loss: 151.086, Residuals: 0.445\n",
      "Loss: 151.084, Residuals: 0.445\n",
      "Loss: 151.084, Residuals: 0.443\n",
      "Loss: 151.081, Residuals: 0.444\n",
      "Evidence -86.198\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.11e-01\n",
      "Loss: 151.380, Residuals: 0.433\n",
      "Loss: 151.365, Residuals: 0.431\n",
      "Loss: 151.362, Residuals: 0.433\n",
      "Loss: 151.336, Residuals: 0.441\n",
      "Loss: 151.335, Residuals: 0.442\n",
      "Loss: 151.329, Residuals: 0.444\n",
      "Loss: 151.318, Residuals: 0.449\n",
      "Loss: 151.318, Residuals: 0.448\n",
      "Loss: 151.313, Residuals: 0.450\n",
      "Loss: 151.312, Residuals: 0.447\n",
      "Loss: 151.306, Residuals: 0.450\n",
      "Loss: 151.306, Residuals: 0.450\n",
      "Loss: 151.306, Residuals: 0.449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 151.304, Residuals: 0.450\n",
      "Loss: 151.304, Residuals: 0.449\n",
      "Loss: 151.303, Residuals: 0.449\n",
      "Loss: 151.303, Residuals: 0.449\n",
      "Evidence -85.961\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.21e-01\n",
      "Loss: 151.511, Residuals: 0.436\n",
      "Loss: 151.494, Residuals: 0.439\n",
      "Loss: 151.492, Residuals: 0.437\n",
      "Loss: 151.478, Residuals: 0.442\n",
      "Loss: 151.456, Residuals: 0.451\n",
      "Loss: 151.456, Residuals: 0.451\n",
      "Loss: 151.455, Residuals: 0.450\n",
      "Loss: 151.449, Residuals: 0.453\n",
      "Loss: 151.449, Residuals: 0.451\n",
      "Loss: 151.443, Residuals: 0.453\n",
      "Loss: 151.443, Residuals: 0.453\n",
      "Loss: 151.442, Residuals: 0.453\n",
      "Loss: 151.439, Residuals: 0.454\n",
      "Loss: 151.439, Residuals: 0.453\n",
      "Loss: 151.438, Residuals: 0.454\n",
      "Loss: 151.438, Residuals: 0.452\n",
      "Loss: 151.437, Residuals: 0.453\n",
      "Loss: 151.437, Residuals: 0.452\n",
      "Evidence -85.781\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.32e-01\n",
      "Loss: 151.593, Residuals: 0.438\n",
      "Loss: 151.581, Residuals: 0.438\n",
      "Loss: 151.578, Residuals: 0.440\n",
      "Loss: 151.554, Residuals: 0.448\n",
      "Loss: 151.553, Residuals: 0.448\n",
      "Loss: 151.545, Residuals: 0.450\n",
      "Loss: 151.533, Residuals: 0.456\n",
      "Loss: 151.533, Residuals: 0.455\n",
      "Loss: 151.530, Residuals: 0.456\n",
      "Loss: 151.526, Residuals: 0.457\n",
      "Loss: 151.525, Residuals: 0.456\n",
      "Loss: 151.523, Residuals: 0.456\n",
      "Loss: 151.520, Residuals: 0.456\n",
      "Loss: 151.520, Residuals: 0.455\n",
      "Loss: 151.519, Residuals: 0.455\n",
      "Loss: 151.518, Residuals: 0.456\n",
      "Loss: 151.518, Residuals: 0.456\n",
      "Loss: 151.518, Residuals: 0.456\n",
      "Loss: 151.518, Residuals: 0.456\n",
      "Loss: 151.517, Residuals: 0.456\n",
      "Loss: 151.517, Residuals: 0.456\n",
      "Evidence -85.652\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.43e-01\n",
      "Loss: 151.645, Residuals: 0.440\n",
      "Loss: 151.623, Residuals: 0.447\n",
      "Loss: 151.621, Residuals: 0.444\n",
      "Loss: 151.604, Residuals: 0.448\n",
      "Loss: 151.579, Residuals: 0.457\n",
      "Loss: 151.579, Residuals: 0.458\n",
      "Loss: 151.578, Residuals: 0.456\n",
      "Loss: 151.572, Residuals: 0.458\n",
      "Loss: 151.563, Residuals: 0.462\n",
      "Loss: 151.563, Residuals: 0.461\n",
      "Loss: 151.563, Residuals: 0.459\n",
      "Loss: 151.563, Residuals: 0.459\n",
      "Loss: 151.562, Residuals: 0.459\n",
      "Loss: 151.562, Residuals: 0.459\n",
      "Loss: 151.561, Residuals: 0.459\n",
      "Loss: 151.561, Residuals: 0.459\n",
      "Loss: 151.561, Residuals: 0.459\n",
      "Evidence -85.565\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 3.54e-01\n",
      "Loss: 151.657, Residuals: 0.443\n",
      "Loss: 151.607, Residuals: 0.459\n",
      "Loss: 151.605, Residuals: 0.458\n",
      "Loss: 151.588, Residuals: 0.462\n",
      "Loss: 151.587, Residuals: 0.460\n",
      "Loss: 151.580, Residuals: 0.462\n",
      "Loss: 151.579, Residuals: 0.460\n",
      "Loss: 151.575, Residuals: 0.462\n",
      "Loss: 151.575, Residuals: 0.462\n",
      "Loss: 151.574, Residuals: 0.462\n",
      "Loss: 151.572, Residuals: 0.462\n",
      "Loss: 151.572, Residuals: 0.461\n",
      "Evidence -85.508\n",
      "Pass count  1\n",
      "Total samples: 40, Updated regularization: 1.00e-05\n",
      "Loss: 414.422, Residuals: -1.767\n",
      "Loss: 311.813, Residuals: -1.363\n",
      "Loss: 269.770, Residuals: -0.563\n",
      "Loss: 238.567, Residuals: 0.337\n",
      "Loss: 234.199, Residuals: 0.739\n",
      "Loss: 193.175, Residuals: 0.330\n",
      "Loss: 170.539, Residuals: 0.102\n",
      "Loss: 164.053, Residuals: 0.202\n",
      "Loss: 152.393, Residuals: 0.368\n",
      "Loss: 134.072, Residuals: 0.346\n",
      "Loss: 129.157, Residuals: 0.358\n",
      "Loss: 120.296, Residuals: 0.272\n",
      "Loss: 115.993, Residuals: 0.261\n",
      "Loss: 111.608, Residuals: 0.324\n",
      "Loss: 110.757, Residuals: 0.424\n",
      "Loss: 109.125, Residuals: 0.402\n",
      "Loss: 106.169, Residuals: 0.350\n",
      "Loss: 102.951, Residuals: 0.388\n",
      "Loss: 98.728, Residuals: 0.187\n",
      "Loss: 98.539, Residuals: 0.219\n",
      "Loss: 96.806, Residuals: 0.189\n",
      "Loss: 93.931, Residuals: 0.119\n",
      "Loss: 93.774, Residuals: 0.135\n",
      "Loss: 92.342, Residuals: 0.120\n",
      "Loss: 92.327, Residuals: 0.115\n",
      "Loss: 90.246, Residuals: 0.103\n",
      "Loss: 90.199, Residuals: 0.081\n",
      "Loss: 90.110, Residuals: 0.097\n",
      "Loss: 89.953, Residuals: 0.128\n",
      "Loss: 89.657, Residuals: 0.134\n",
      "Loss: 87.337, Residuals: 0.117\n",
      "Loss: 87.309, Residuals: 0.099\n",
      "Loss: 87.267, Residuals: 0.080\n",
      "Loss: 87.189, Residuals: 0.091\n",
      "Loss: 87.052, Residuals: 0.106\n",
      "Loss: 86.844, Residuals: 0.129\n",
      "Loss: 85.165, Residuals: 0.132\n",
      "Loss: 85.139, Residuals: 0.115\n",
      "Loss: 85.110, Residuals: 0.090\n",
      "Loss: 84.872, Residuals: 0.091\n",
      "Loss: 84.836, Residuals: 0.116\n",
      "Loss: 83.554, Residuals: 0.114\n",
      "Loss: 83.548, Residuals: 0.105\n",
      "Loss: 83.538, Residuals: 0.094\n",
      "Loss: 83.449, Residuals: 0.093\n",
      "Loss: 83.291, Residuals: 0.087\n",
      "Loss: 83.018, Residuals: 0.072\n",
      "Loss: 83.004, Residuals: 0.074\n",
      "Loss: 82.488, Residuals: 0.071\n",
      "Loss: 82.486, Residuals: 0.066\n",
      "Loss: 82.150, Residuals: 0.063\n",
      "Loss: 82.123, Residuals: 0.071\n",
      "Loss: 81.171, Residuals: 0.074\n",
      "Loss: 81.168, Residuals: 0.068\n",
      "Loss: 81.163, Residuals: 0.061\n",
      "Loss: 81.155, Residuals: 0.056\n",
      "Loss: 81.081, Residuals: 0.055\n",
      "Loss: 81.011, Residuals: 0.061\n",
      "Loss: 80.990, Residuals: 0.065\n",
      "Loss: 80.978, Residuals: 0.067\n",
      "Loss: 80.542, Residuals: 0.066\n",
      "Loss: 80.540, Residuals: 0.060\n",
      "Loss: 80.294, Residuals: 0.059\n",
      "Loss: 80.052, Residuals: 0.029\n",
      "Loss: 80.027, Residuals: 0.046\n",
      "Loss: 79.998, Residuals: 0.046\n",
      "Loss: 79.982, Residuals: 0.060\n",
      "Loss: 79.956, Residuals: 0.057\n",
      "Loss: 79.919, Residuals: 0.051\n",
      "Loss: 79.581, Residuals: 0.051\n",
      "Loss: 79.580, Residuals: 0.044\n",
      "Loss: 79.376, Residuals: 0.043\n",
      "Loss: 79.369, Residuals: 0.036\n",
      "Loss: 79.364, Residuals: 0.042\n",
      "Loss: 79.169, Residuals: 0.041\n",
      "Loss: 79.168, Residuals: 0.037\n",
      "Loss: 78.879, Residuals: 0.037\n",
      "Loss: 78.879, Residuals: 0.036\n",
      "Evidence -546.152\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.27e-02\n",
      "Loss: 108.697, Residuals: -0.004\n",
      "Loss: 108.307, Residuals: 0.019\n",
      "Loss: 107.561, Residuals: 0.018\n",
      "Loss: 106.245, Residuals: 0.014\n",
      "Loss: 105.546, Residuals: -0.052\n",
      "Loss: 105.190, Residuals: -0.015\n",
      "Loss: 104.546, Residuals: -0.013\n",
      "Loss: 104.499, Residuals: 0.005\n",
      "Loss: 102.948, Residuals: -0.004\n",
      "Loss: 102.933, Residuals: 0.009\n",
      "Loss: 102.917, Residuals: 0.009\n",
      "Loss: 102.321, Residuals: 0.020\n",
      "Loss: 102.319, Residuals: 0.022\n",
      "Loss: 101.915, Residuals: 0.034\n",
      "Loss: 101.852, Residuals: 0.036\n",
      "Loss: 101.821, Residuals: 0.040\n",
      "Loss: 101.533, Residuals: 0.044\n",
      "Loss: 101.530, Residuals: 0.040\n",
      "Loss: 100.038, Residuals: 0.044\n",
      "Loss: 99.992, Residuals: 0.086\n",
      "Loss: 99.973, Residuals: 0.089\n",
      "Loss: 99.942, Residuals: 0.096\n",
      "Loss: 99.670, Residuals: 0.114\n",
      "Loss: 99.662, Residuals: 0.114\n",
      "Loss: 99.647, Residuals: 0.115\n",
      "Loss: 99.620, Residuals: 0.117\n",
      "Loss: 99.370, Residuals: 0.123\n",
      "Loss: 99.369, Residuals: 0.122\n",
      "Loss: 99.052, Residuals: 0.129\n",
      "Loss: 99.052, Residuals: 0.127\n",
      "Evidence -192.496\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 9.04e-02\n",
      "Loss: 132.003, Residuals: 0.075\n",
      "Loss: 130.976, Residuals: 0.046\n",
      "Loss: 130.920, Residuals: 0.073\n",
      "Loss: 129.034, Residuals: 0.066\n",
      "Loss: 129.031, Residuals: 0.070\n",
      "Loss: 128.502, Residuals: 0.077\n",
      "Loss: 127.524, Residuals: 0.095\n",
      "Loss: 127.014, Residuals: 0.105\n",
      "Loss: 127.013, Residuals: 0.108\n",
      "Evidence -145.029\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.21e-01\n",
      "Loss: 144.961, Residuals: 0.077\n",
      "Loss: 144.507, Residuals: 0.065\n",
      "Loss: 143.726, Residuals: 0.061\n",
      "Loss: 143.723, Residuals: 0.065\n",
      "Loss: 142.037, Residuals: 0.100\n",
      "Loss: 141.981, Residuals: 0.096\n",
      "Loss: 139.946, Residuals: 0.152\n",
      "Loss: 139.934, Residuals: 0.153\n",
      "Loss: 139.923, Residuals: 0.162\n",
      "Loss: 138.202, Residuals: 0.178\n",
      "Loss: 138.196, Residuals: 0.166\n",
      "Loss: 137.213, Residuals: 0.180\n",
      "Loss: 135.581, Residuals: 0.230\n",
      "Loss: 135.571, Residuals: 0.222\n",
      "Loss: 135.563, Residuals: 0.220\n",
      "Loss: 135.500, Residuals: 0.227\n",
      "Loss: 135.399, Residuals: 0.237\n",
      "Loss: 134.605, Residuals: 0.270\n",
      "Loss: 134.598, Residuals: 0.264\n",
      "Loss: 134.594, Residuals: 0.257\n",
      "Loss: 134.561, Residuals: 0.260\n",
      "Loss: 134.517, Residuals: 0.266\n",
      "Loss: 134.516, Residuals: 0.263\n",
      "Loss: 134.354, Residuals: 0.262\n",
      "Loss: 134.351, Residuals: 0.261\n",
      "Loss: 134.249, Residuals: 0.262\n",
      "Loss: 134.106, Residuals: 0.269\n",
      "Loss: 134.105, Residuals: 0.265\n",
      "Loss: 134.097, Residuals: 0.266\n",
      "Loss: 134.096, Residuals: 0.267\n",
      "Loss: 134.093, Residuals: 0.267\n",
      "Loss: 134.088, Residuals: 0.266\n",
      "Loss: 134.081, Residuals: 0.264\n",
      "Loss: 134.081, Residuals: 0.263\n",
      "Loss: 134.077, Residuals: 0.265\n",
      "Loss: 134.077, Residuals: 0.265\n",
      "Loss: 134.073, Residuals: 0.265\n",
      "Loss: 134.066, Residuals: 0.265\n",
      "Loss: 134.065, Residuals: 0.267\n",
      "Loss: 134.065, Residuals: 0.267\n",
      "Loss: 134.065, Residuals: 0.267\n",
      "Loss: 134.063, Residuals: 0.267\n",
      "Loss: 134.060, Residuals: 0.267\n",
      "Loss: 134.060, Residuals: 0.267\n",
      "Loss: 134.059, Residuals: 0.267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 134.059, Residuals: 0.267\n",
      "Loss: 134.058, Residuals: 0.267\n",
      "Loss: 134.058, Residuals: 0.268\n",
      "Loss: 134.057, Residuals: 0.268\n",
      "Loss: 134.057, Residuals: 0.268\n",
      "Loss: 134.057, Residuals: 0.268\n",
      "Loss: 134.057, Residuals: 0.268\n",
      "Evidence -131.844\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 5.09e-01\n",
      "Loss: 147.286, Residuals: 0.259\n",
      "Loss: 146.896, Residuals: 0.259\n",
      "Loss: 146.244, Residuals: 0.259\n",
      "Loss: 146.135, Residuals: 0.262\n",
      "Loss: 145.196, Residuals: 0.303\n",
      "Loss: 144.100, Residuals: 0.402\n",
      "Loss: 144.078, Residuals: 0.404\n",
      "Loss: 144.038, Residuals: 0.403\n",
      "Loss: 143.962, Residuals: 0.400\n",
      "Loss: 143.870, Residuals: 0.397\n",
      "Loss: 143.705, Residuals: 0.394\n",
      "Loss: 143.454, Residuals: 0.391\n",
      "Loss: 143.430, Residuals: 0.376\n",
      "Loss: 143.389, Residuals: 0.380\n",
      "Loss: 143.325, Residuals: 0.386\n",
      "Loss: 143.317, Residuals: 0.384\n",
      "Loss: 143.315, Residuals: 0.382\n",
      "Loss: 143.299, Residuals: 0.383\n",
      "Loss: 143.297, Residuals: 0.383\n",
      "Loss: 143.296, Residuals: 0.382\n",
      "Loss: 143.296, Residuals: 0.384\n",
      "Loss: 143.293, Residuals: 0.385\n",
      "Loss: 143.289, Residuals: 0.385\n",
      "Loss: 143.289, Residuals: 0.384\n",
      "Loss: 143.287, Residuals: 0.385\n",
      "Loss: 143.287, Residuals: 0.384\n",
      "Loss: 143.287, Residuals: 0.384\n",
      "Loss: 143.286, Residuals: 0.385\n",
      "Loss: 143.286, Residuals: 0.385\n",
      "Loss: 143.286, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Loss: 143.285, Residuals: 0.385\n",
      "Evidence -112.449\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 9.57e-01\n",
      "Loss: 151.166, Residuals: 0.310\n",
      "Loss: 150.568, Residuals: 0.334\n",
      "Loss: 149.598, Residuals: 0.391\n",
      "Loss: 149.476, Residuals: 0.390\n",
      "Loss: 148.565, Residuals: 0.443\n",
      "Loss: 148.561, Residuals: 0.439\n",
      "Loss: 148.447, Residuals: 0.437\n",
      "Loss: 148.361, Residuals: 0.430\n",
      "Loss: 148.210, Residuals: 0.441\n",
      "Loss: 148.018, Residuals: 0.460\n",
      "Loss: 148.010, Residuals: 0.454\n",
      "Loss: 147.997, Residuals: 0.450\n",
      "Loss: 147.982, Residuals: 0.441\n",
      "Loss: 147.962, Residuals: 0.445\n",
      "Loss: 147.959, Residuals: 0.441\n",
      "Loss: 147.959, Residuals: 0.441\n",
      "Loss: 147.958, Residuals: 0.442\n",
      "Loss: 147.956, Residuals: 0.443\n",
      "Loss: 147.956, Residuals: 0.442\n",
      "Loss: 147.955, Residuals: 0.442\n",
      "Loss: 147.955, Residuals: 0.443\n",
      "Loss: 147.955, Residuals: 0.443\n",
      "Loss: 147.955, Residuals: 0.443\n",
      "Loss: 147.955, Residuals: 0.443\n",
      "Loss: 147.955, Residuals: 0.443\n",
      "Loss: 147.955, Residuals: 0.443\n",
      "Loss: 147.955, Residuals: 0.442\n",
      "Loss: 147.955, Residuals: 0.443\n",
      "Evidence -106.168\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.54e+00\n",
      "Loss: 151.028, Residuals: 0.351\n",
      "Loss: 150.950, Residuals: 0.324\n",
      "Loss: 150.811, Residuals: 0.335\n",
      "Loss: 150.591, Residuals: 0.363\n",
      "Loss: 150.218, Residuals: 0.395\n",
      "Loss: 149.757, Residuals: 0.456\n",
      "Loss: 149.737, Residuals: 0.472\n",
      "Loss: 149.702, Residuals: 0.466\n",
      "Loss: 149.649, Residuals: 0.455\n",
      "Loss: 149.639, Residuals: 0.449\n",
      "Loss: 149.622, Residuals: 0.453\n",
      "Loss: 149.611, Residuals: 0.458\n",
      "Loss: 149.611, Residuals: 0.460\n",
      "Loss: 149.611, Residuals: 0.458\n",
      "Loss: 149.611, Residuals: 0.458\n",
      "Loss: 149.611, Residuals: 0.458\n",
      "Loss: 149.611, Residuals: 0.459\n",
      "Loss: 149.611, Residuals: 0.459\n",
      "Evidence -99.441\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.91e+00\n",
      "Loss: 152.893, Residuals: 0.346\n",
      "Loss: 152.493, Residuals: 0.440\n",
      "Loss: 152.467, Residuals: 0.428\n",
      "Loss: 152.418, Residuals: 0.431\n",
      "Loss: 152.336, Residuals: 0.436\n",
      "Loss: 152.309, Residuals: 0.448\n",
      "Loss: 152.271, Residuals: 0.451\n",
      "Loss: 152.247, Residuals: 0.443\n",
      "Loss: 152.245, Residuals: 0.442\n",
      "Loss: 152.244, Residuals: 0.448\n",
      "Loss: 152.244, Residuals: 0.447\n",
      "Loss: 152.243, Residuals: 0.447\n",
      "Loss: 152.243, Residuals: 0.447\n",
      "Loss: 152.243, Residuals: 0.447\n",
      "Loss: 152.243, Residuals: 0.446\n",
      "Evidence -94.677\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.01e+00\n",
      "Loss: 154.327, Residuals: 0.403\n",
      "Loss: 154.209, Residuals: 0.413\n",
      "Loss: 154.187, Residuals: 0.429\n",
      "Loss: 154.154, Residuals: 0.429\n",
      "Loss: 154.131, Residuals: 0.438\n",
      "Loss: 154.127, Residuals: 0.429\n",
      "Loss: 154.126, Residuals: 0.432\n",
      "Loss: 154.125, Residuals: 0.431\n",
      "Loss: 154.122, Residuals: 0.430\n",
      "Loss: 154.121, Residuals: 0.428\n",
      "Evidence -92.503\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.02e+00\n",
      "Loss: 155.211, Residuals: 0.401\n",
      "Loss: 155.165, Residuals: 0.424\n",
      "Loss: 155.140, Residuals: 0.422\n",
      "Loss: 155.136, Residuals: 0.420\n",
      "Loss: 155.130, Residuals: 0.418\n",
      "Loss: 155.127, Residuals: 0.416\n",
      "Loss: 155.127, Residuals: 0.416\n",
      "Loss: 155.127, Residuals: 0.416\n",
      "Evidence -91.341\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.01e+00\n",
      "Loss: 155.755, Residuals: 0.404\n",
      "Loss: 155.726, Residuals: 0.417\n",
      "Loss: 155.716, Residuals: 0.413\n",
      "Loss: 155.713, Residuals: 0.414\n",
      "Loss: 155.709, Residuals: 0.412\n",
      "Loss: 155.708, Residuals: 0.411\n",
      "Loss: 155.708, Residuals: 0.410\n",
      "Loss: 155.707, Residuals: 0.409\n",
      "Loss: 155.707, Residuals: 0.408\n",
      "Loss: 155.707, Residuals: 0.408\n",
      "Evidence -90.648\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 2.00e+00\n",
      "Loss: 156.097, Residuals: 0.403\n",
      "Loss: 156.084, Residuals: 0.410\n",
      "Loss: 156.081, Residuals: 0.412\n",
      "Loss: 156.078, Residuals: 0.410\n",
      "Loss: 156.075, Residuals: 0.407\n",
      "Loss: 156.074, Residuals: 0.405\n",
      "Loss: 156.074, Residuals: 0.406\n",
      "Evidence -90.221\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.99e+00\n",
      "Loss: 156.326, Residuals: 0.405\n",
      "Loss: 156.319, Residuals: 0.411\n",
      "Loss: 156.317, Residuals: 0.409\n",
      "Loss: 156.315, Residuals: 0.408\n",
      "Loss: 156.312, Residuals: 0.405\n",
      "Loss: 156.312, Residuals: 0.405\n",
      "Loss: 156.312, Residuals: 0.405\n",
      "Loss: 156.311, Residuals: 0.405\n",
      "Loss: 156.311, Residuals: 0.404\n",
      "Loss: 156.311, Residuals: 0.404\n",
      "Loss: 156.311, Residuals: 0.404\n",
      "Evidence -89.918\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.98e+00\n",
      "Loss: 156.487, Residuals: 0.409\n",
      "Loss: 156.485, Residuals: 0.407\n",
      "Loss: 156.483, Residuals: 0.407\n",
      "Loss: 156.479, Residuals: 0.405\n",
      "Loss: 156.479, Residuals: 0.405\n",
      "Loss: 156.478, Residuals: 0.405\n",
      "Loss: 156.478, Residuals: 0.405\n",
      "Loss: 156.478, Residuals: 0.405\n",
      "Evidence -89.702\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.97e+00\n",
      "Loss: 156.603, Residuals: 0.405\n",
      "Loss: 156.600, Residuals: 0.409\n",
      "Loss: 156.596, Residuals: 0.406\n",
      "Loss: 156.596, Residuals: 0.407\n",
      "Loss: 156.596, Residuals: 0.407\n",
      "Loss: 156.595, Residuals: 0.406\n",
      "Loss: 156.595, Residuals: 0.405\n",
      "Evidence -89.533\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.96e+00\n",
      "Loss: 156.689, Residuals: 0.407\n",
      "Loss: 156.686, Residuals: 0.410\n",
      "Loss: 156.683, Residuals: 0.408\n",
      "Loss: 156.683, Residuals: 0.407\n",
      "Loss: 156.682, Residuals: 0.407\n",
      "Loss: 156.682, Residuals: 0.407\n",
      "Loss: 156.682, Residuals: 0.407\n",
      "Loss: 156.682, Residuals: 0.406\n",
      "Loss: 156.682, Residuals: 0.406\n",
      "Loss: 156.682, Residuals: 0.406\n",
      "Evidence -89.402\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.95e+00\n",
      "Loss: 156.751, Residuals: 0.409\n",
      "Loss: 156.749, Residuals: 0.411\n",
      "Loss: 156.747, Residuals: 0.409\n",
      "Loss: 156.747, Residuals: 0.408\n",
      "Loss: 156.746, Residuals: 0.408\n",
      "Loss: 156.746, Residuals: 0.408\n",
      "Loss: 156.746, Residuals: 0.408\n",
      "Loss: 156.746, Residuals: 0.407\n",
      "Evidence -89.296\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.94e+00\n",
      "Loss: 156.802, Residuals: 0.410\n",
      "Loss: 156.800, Residuals: 0.412\n",
      "Loss: 156.798, Residuals: 0.410\n",
      "Loss: 156.798, Residuals: 0.410\n",
      "Loss: 156.798, Residuals: 0.409\n",
      "Loss: 156.798, Residuals: 0.409\n",
      "Loss: 156.798, Residuals: 0.409\n",
      "Loss: 156.797, Residuals: 0.409\n",
      "Loss: 156.797, Residuals: 0.408\n",
      "Loss: 156.797, Residuals: 0.409\n",
      "Evidence -89.206\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 1.94e+00\n",
      "Loss: 156.842, Residuals: 0.412\n",
      "Loss: 156.840, Residuals: 0.411\n",
      "Loss: 156.840, Residuals: 0.411\n",
      "Loss: 156.840, Residuals: 0.412\n",
      "Loss: 156.840, Residuals: 0.411\n",
      "Loss: 156.839, Residuals: 0.411\n",
      "Loss: 156.839, Residuals: 0.410\n",
      "Loss: 156.839, Residuals: 0.410\n",
      "Evidence -89.127\n",
      "Pass count  1\n",
      "Total samples: 38, Updated regularization: 1.00e-05\n",
      "Loss: 395.767, Residuals: -1.859\n",
      "Loss: 299.393, Residuals: -1.329\n",
      "Loss: 261.767, Residuals: -0.289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 249.282, Residuals: 0.507\n",
      "Loss: 171.884, Residuals: -0.504\n",
      "Loss: 162.560, Residuals: -0.112\n",
      "Loss: 147.251, Residuals: 0.141\n",
      "Loss: 127.892, Residuals: 0.317\n",
      "Loss: 122.093, Residuals: 0.095\n",
      "Loss: 118.219, Residuals: 0.138\n",
      "Loss: 111.741, Residuals: 0.051\n",
      "Loss: 108.814, Residuals: 0.489\n",
      "Loss: 108.578, Residuals: 0.453\n",
      "Loss: 106.381, Residuals: 0.421\n",
      "Loss: 102.618, Residuals: 0.344\n",
      "Loss: 102.464, Residuals: 0.351\n",
      "Loss: 100.993, Residuals: 0.330\n",
      "Loss: 98.310, Residuals: 0.293\n",
      "Loss: 94.099, Residuals: 0.187\n",
      "Loss: 94.046, Residuals: 0.213\n",
      "Loss: 93.945, Residuals: 0.216\n",
      "Loss: 93.173, Residuals: 0.311\n",
      "Loss: 93.070, Residuals: 0.305\n",
      "Loss: 89.286, Residuals: 0.242\n",
      "Loss: 89.248, Residuals: 0.230\n",
      "Loss: 87.754, Residuals: 0.205\n",
      "Loss: 85.027, Residuals: 0.136\n",
      "Loss: 85.005, Residuals: 0.135\n",
      "Loss: 81.734, Residuals: 0.058\n",
      "Loss: 81.711, Residuals: 0.073\n",
      "Loss: 80.925, Residuals: 0.114\n",
      "Loss: 80.736, Residuals: 0.189\n",
      "Loss: 79.100, Residuals: 0.147\n",
      "Loss: 79.092, Residuals: 0.155\n",
      "Loss: 79.078, Residuals: 0.159\n",
      "Loss: 78.944, Residuals: 0.160\n",
      "Loss: 78.708, Residuals: 0.163\n",
      "Loss: 76.908, Residuals: 0.097\n",
      "Loss: 76.849, Residuals: 0.120\n",
      "Loss: 76.819, Residuals: 0.108\n",
      "Loss: 76.775, Residuals: 0.109\n",
      "Loss: 76.373, Residuals: 0.106\n",
      "Loss: 76.361, Residuals: 0.120\n",
      "Loss: 75.885, Residuals: 0.108\n",
      "Loss: 75.884, Residuals: 0.107\n",
      "Loss: 74.821, Residuals: 0.073\n",
      "Loss: 74.797, Residuals: 0.088\n",
      "Loss: 74.788, Residuals: 0.081\n",
      "Loss: 74.483, Residuals: 0.074\n",
      "Loss: 74.443, Residuals: 0.093\n",
      "Loss: 74.064, Residuals: 0.082\n",
      "Loss: 74.052, Residuals: 0.083\n",
      "Loss: 73.553, Residuals: 0.074\n",
      "Loss: 73.551, Residuals: 0.078\n",
      "Loss: 72.519, Residuals: 0.084\n",
      "Loss: 72.499, Residuals: 0.073\n",
      "Loss: 72.463, Residuals: 0.092\n",
      "Loss: 72.406, Residuals: 0.114\n",
      "Loss: 72.306, Residuals: 0.118\n",
      "Loss: 72.145, Residuals: 0.122\n",
      "Loss: 72.141, Residuals: 0.127\n",
      "Loss: 71.508, Residuals: 0.112\n",
      "Loss: 71.500, Residuals: 0.119\n",
      "Loss: 71.498, Residuals: 0.115\n",
      "Loss: 70.516, Residuals: 0.086\n",
      "Loss: 70.466, Residuals: 0.107\n",
      "Loss: 70.461, Residuals: 0.100\n",
      "Loss: 70.409, Residuals: 0.095\n",
      "Loss: 70.385, Residuals: 0.115\n",
      "Loss: 70.165, Residuals: 0.108\n",
      "Loss: 70.165, Residuals: 0.107\n",
      "Evidence -545.194\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 6.18e-02\n",
      "Loss: 102.988, Residuals: 0.077\n",
      "Loss: 102.752, Residuals: 0.072\n",
      "Loss: 102.353, Residuals: 0.065\n",
      "Loss: 102.312, Residuals: 0.065\n",
      "Loss: 100.705, Residuals: 0.047\n",
      "Loss: 100.698, Residuals: 0.041\n",
      "Loss: 99.602, Residuals: 0.047\n",
      "Loss: 99.594, Residuals: 0.049\n",
      "Loss: 98.367, Residuals: 0.073\n",
      "Loss: 98.347, Residuals: 0.068\n",
      "Loss: 97.605, Residuals: 0.087\n",
      "Loss: 97.337, Residuals: 0.061\n",
      "Loss: 97.306, Residuals: 0.075\n",
      "Loss: 97.251, Residuals: 0.078\n",
      "Loss: 96.756, Residuals: 0.100\n",
      "Loss: 96.037, Residuals: 0.181\n",
      "Loss: 96.027, Residuals: 0.167\n",
      "Loss: 96.022, Residuals: 0.156\n",
      "Loss: 95.820, Residuals: 0.162\n",
      "Loss: 95.820, Residuals: 0.164\n",
      "Evidence -167.883\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.28e-01\n",
      "Loss: 130.209, Residuals: 0.148\n",
      "Loss: 130.124, Residuals: 0.141\n",
      "Loss: 130.056, Residuals: 0.124\n",
      "Loss: 129.929, Residuals: 0.116\n",
      "Loss: 129.717, Residuals: 0.101\n",
      "Loss: 129.715, Residuals: 0.103\n",
      "Evidence -137.340\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.46e-01\n",
      "Loss: 144.441, Residuals: 0.082\n",
      "Loss: 144.042, Residuals: 0.059\n",
      "Loss: 144.040, Residuals: 0.062\n",
      "Evidence -132.162\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.51e-01\n",
      "Loss: 149.395, Residuals: 0.063\n",
      "Loss: 148.960, Residuals: 0.075\n",
      "Loss: 148.180, Residuals: 0.097\n",
      "Loss: 148.176, Residuals: 0.094\n",
      "Loss: 146.306, Residuals: 0.166\n",
      "Loss: 146.285, Residuals: 0.174\n",
      "Loss: 146.262, Residuals: 0.161\n",
      "Loss: 146.045, Residuals: 0.173\n",
      "Loss: 145.647, Residuals: 0.183\n",
      "Loss: 145.645, Residuals: 0.181\n",
      "Loss: 145.413, Residuals: 0.191\n",
      "Loss: 145.000, Residuals: 0.214\n",
      "Loss: 144.927, Residuals: 0.211\n",
      "Loss: 144.788, Residuals: 0.217\n",
      "Loss: 144.531, Residuals: 0.224\n",
      "Loss: 144.531, Residuals: 0.225\n",
      "Evidence -129.356\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.75e-01\n",
      "Loss: 149.389, Residuals: 0.224\n",
      "Loss: 149.366, Residuals: 0.207\n",
      "Loss: 149.337, Residuals: 0.195\n",
      "Loss: 148.324, Residuals: 0.243\n",
      "Loss: 148.321, Residuals: 0.246\n",
      "Evidence -125.844\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.82e-01\n",
      "Loss: 150.771, Residuals: 0.240\n",
      "Loss: 150.323, Residuals: 0.236\n",
      "Loss: 150.264, Residuals: 0.244\n",
      "Loss: 149.755, Residuals: 0.268\n",
      "Loss: 149.754, Residuals: 0.270\n",
      "Evidence -124.685\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.87e-01\n",
      "Loss: 151.203, Residuals: 0.259\n",
      "Loss: 151.084, Residuals: 0.259\n",
      "Loss: 151.072, Residuals: 0.251\n",
      "Loss: 150.635, Residuals: 0.269\n",
      "Loss: 150.634, Residuals: 0.270\n",
      "Evidence -123.841\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.91e-01\n",
      "Loss: 151.614, Residuals: 0.270\n",
      "Loss: 151.172, Residuals: 0.285\n",
      "Loss: 151.172, Residuals: 0.286\n",
      "Evidence -123.410\n",
      "Updating hyper-parameters...\n",
      "Total samples: 38, Updated regularization: 1.94e-01\n",
      "Loss: 151.470, Residuals: 0.273\n",
      "Loss: 151.468, Residuals: 0.275\n",
      "Evidence -123.354\n",
      "Pass count  1\n",
      "Total samples: 40, Updated regularization: 1.00e-05\n",
      "Loss: 414.188, Residuals: -1.773\n",
      "Loss: 294.392, Residuals: -1.915\n",
      "Loss: 257.961, Residuals: -1.102\n",
      "Loss: 239.311, Residuals: -0.060\n",
      "Loss: 203.853, Residuals: -0.215\n",
      "Loss: 163.096, Residuals: -0.418\n",
      "Loss: 151.312, Residuals: -0.171\n",
      "Loss: 146.314, Residuals: 0.218\n",
      "Loss: 137.675, Residuals: 0.233\n",
      "Loss: 123.822, Residuals: 0.060\n",
      "Loss: 119.971, Residuals: 0.104\n",
      "Loss: 115.384, Residuals: 0.188\n",
      "Loss: 114.843, Residuals: 0.267\n",
      "Loss: 110.678, Residuals: 0.309\n",
      "Loss: 110.204, Residuals: 0.345\n",
      "Loss: 106.258, Residuals: 0.319\n",
      "Loss: 106.116, Residuals: 0.329\n",
      "Loss: 104.754, Residuals: 0.314\n",
      "Loss: 102.310, Residuals: 0.285\n",
      "Loss: 99.674, Residuals: 0.136\n",
      "Loss: 98.533, Residuals: 0.251\n",
      "Loss: 98.487, Residuals: 0.274\n",
      "Loss: 98.069, Residuals: 0.275\n",
      "Loss: 94.660, Residuals: 0.197\n",
      "Loss: 94.624, Residuals: 0.179\n",
      "Loss: 94.331, Residuals: 0.205\n",
      "Loss: 91.820, Residuals: 0.161\n",
      "Loss: 91.786, Residuals: 0.150\n",
      "Loss: 90.472, Residuals: 0.134\n",
      "Loss: 90.090, Residuals: 0.158\n",
      "Loss: 87.052, Residuals: 0.141\n",
      "Loss: 87.028, Residuals: 0.125\n",
      "Loss: 86.994, Residuals: 0.108\n",
      "Loss: 86.692, Residuals: 0.118\n",
      "Loss: 86.139, Residuals: 0.115\n",
      "Loss: 85.107, Residuals: 0.092\n",
      "Loss: 84.843, Residuals: 0.122\n",
      "Loss: 84.373, Residuals: 0.115\n",
      "Loss: 84.369, Residuals: 0.107\n",
      "Loss: 83.655, Residuals: 0.099\n",
      "Loss: 82.409, Residuals: 0.084\n",
      "Loss: 81.822, Residuals: 0.109\n",
      "Loss: 81.699, Residuals: 0.101\n",
      "Loss: 81.648, Residuals: 0.121\n",
      "Loss: 81.612, Residuals: 0.101\n",
      "Loss: 81.269, Residuals: 0.096\n",
      "Loss: 80.650, Residuals: 0.080\n",
      "Loss: 80.317, Residuals: 0.107\n",
      "Loss: 80.255, Residuals: 0.108\n",
      "Loss: 79.694, Residuals: 0.092\n",
      "Loss: 79.691, Residuals: 0.088\n",
      "Loss: 79.298, Residuals: 0.082\n",
      "Loss: 79.297, Residuals: 0.079\n",
      "Loss: 78.767, Residuals: 0.064\n",
      "Loss: 78.763, Residuals: 0.050\n",
      "Loss: 78.726, Residuals: 0.047\n",
      "Loss: 78.712, Residuals: 0.046\n",
      "Loss: 78.592, Residuals: 0.040\n",
      "Loss: 78.591, Residuals: 0.048\n",
      "Loss: 78.381, Residuals: 0.045\n",
      "Loss: 78.381, Residuals: 0.043\n",
      "Loss: 78.150, Residuals: 0.040\n",
      "Loss: 78.149, Residuals: 0.038\n",
      "Loss: 77.689, Residuals: 0.038\n",
      "Loss: 77.686, Residuals: 0.028\n",
      "Loss: 77.662, Residuals: 0.032\n",
      "Loss: 77.627, Residuals: 0.040\n",
      "Loss: 77.563, Residuals: 0.036\n",
      "Loss: 77.445, Residuals: 0.030\n",
      "Loss: 77.305, Residuals: 0.021\n",
      "Loss: 77.304, Residuals: 0.016\n",
      "Loss: 77.302, Residuals: 0.020\n",
      "Loss: 77.298, Residuals: 0.024\n",
      "Loss: 77.293, Residuals: 0.030\n",
      "Loss: 77.082, Residuals: 0.029\n",
      "Loss: 77.082, Residuals: 0.027\n",
      "Loss: 76.584, Residuals: 0.030\n",
      "Loss: 76.580, Residuals: 0.020\n",
      "Loss: 76.575, Residuals: 0.008\n",
      "Loss: 76.566, Residuals: 0.012\n",
      "Loss: 76.550, Residuals: 0.020\n",
      "Loss: 76.529, Residuals: 0.032\n",
      "Loss: 76.345, Residuals: 0.022\n",
      "Loss: 76.343, Residuals: 0.025\n",
      "Loss: 76.341, Residuals: 0.025\n",
      "Loss: 76.338, Residuals: 0.025\n",
      "Loss: 76.308, Residuals: 0.022\n",
      "Loss: 76.308, Residuals: 0.021\n",
      "Loss: 75.887, Residuals: 0.025\n",
      "Loss: 75.885, Residuals: 0.018\n",
      "Loss: 75.882, Residuals: 0.011\n",
      "Loss: 75.875, Residuals: 0.014\n",
      "Loss: 75.864, Residuals: 0.019\n",
      "Loss: 75.486, Residuals: -0.012\n",
      "Loss: 75.474, Residuals: 0.002\n",
      "Loss: 75.456, Residuals: 0.003\n",
      "Loss: 75.434, Residuals: 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 75.400, Residuals: 0.018\n",
      "Loss: 75.399, Residuals: 0.019\n",
      "Loss: 75.353, Residuals: 0.020\n",
      "Loss: 75.352, Residuals: 0.019\n",
      "Loss: 75.283, Residuals: 0.022\n",
      "Loss: 75.282, Residuals: 0.021\n",
      "Loss: 75.274, Residuals: 0.024\n",
      "Loss: 74.987, Residuals: 0.033\n",
      "Loss: 74.968, Residuals: 0.019\n",
      "Loss: 74.941, Residuals: 0.025\n",
      "Loss: 74.892, Residuals: 0.029\n",
      "Loss: 74.808, Residuals: 0.033\n",
      "Loss: 74.804, Residuals: 0.038\n",
      "Loss: 73.613, Residuals: 0.097\n",
      "Loss: 73.420, Residuals: 0.075\n",
      "Loss: 73.327, Residuals: 0.044\n",
      "Loss: 73.173, Residuals: 0.057\n",
      "Loss: 72.913, Residuals: 0.072\n",
      "Loss: 72.551, Residuals: 0.086\n",
      "Loss: 72.545, Residuals: 0.093\n",
      "Loss: 71.551, Residuals: 0.105\n",
      "Loss: 71.539, Residuals: 0.091\n",
      "Loss: 71.525, Residuals: 0.087\n",
      "Loss: 69.936, Residuals: 0.130\n",
      "Loss: 69.832, Residuals: 0.098\n",
      "Loss: 69.709, Residuals: 0.112\n",
      "Loss: 69.499, Residuals: 0.112\n",
      "Loss: 69.172, Residuals: 0.117\n",
      "Loss: 69.168, Residuals: 0.122\n",
      "Loss: 68.482, Residuals: 0.134\n",
      "Loss: 68.480, Residuals: 0.134\n",
      "Loss: 68.079, Residuals: 0.135\n",
      "Loss: 67.355, Residuals: 0.143\n",
      "Loss: 67.349, Residuals: 0.141\n",
      "Loss: 67.301, Residuals: 0.144\n",
      "Loss: 67.220, Residuals: 0.147\n",
      "Loss: 67.106, Residuals: 0.144\n",
      "Loss: 67.091, Residuals: 0.163\n",
      "Loss: 66.522, Residuals: 0.156\n",
      "Loss: 66.520, Residuals: 0.154\n",
      "Loss: 66.450, Residuals: 0.151\n",
      "Loss: 66.448, Residuals: 0.152\n",
      "Loss: 65.516, Residuals: 0.119\n",
      "Loss: 65.509, Residuals: 0.119\n",
      "Loss: 65.506, Residuals: 0.123\n",
      "Loss: 65.501, Residuals: 0.126\n",
      "Loss: 65.465, Residuals: 0.131\n",
      "Loss: 65.464, Residuals: 0.131\n",
      "Evidence -532.578\n",
      "Updating hyper-parameters...\n",
      "Total samples: 40, Updated regularization: 9.42e-03\n",
      "Loss: 110.105, Residuals: 0.129\n",
      "Loss: 109.937, Residuals: 0.185\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29664/2507251102.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# fit to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# plot fitness to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UW/Venturelli/Sulaiman_Cdiff_ExpDesign/exp2/glove/log_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, evidence_tol, nlp_tol, alpha_0, patience, max_fails, beta)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;31m# fit using updated Alpha and Beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             self.res = minimize(fun=self.objective,\n\u001b[0m\u001b[1;32m    292\u001b[0m                                 \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjacobian_fwd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                                 \u001b[0mhess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0m\u001b[1;32m    679\u001b[0m                                  **options)\n\u001b[1;32m    680\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_minimize_newtoncg\u001b[0;34m(fun, x0, args, jac, hess, hessp, callback, xtol, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfhess\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m             \u001b[0;31m# you want to compute hessian once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1933\u001b[0;31m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1934\u001b[0m             \u001b[0mhcalls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhcalls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mhess\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_hess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_hess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_hess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_hess_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_hess\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mupdate_hess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhess_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFD_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mhess_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mhess_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnhev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UW/Venturelli/Sulaiman_Cdiff_ExpDesign/exp2/glove/log_model.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# for each output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunODEZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_measured\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_present\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# compute Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run kfold for each file \n",
    "for file in files:\n",
    "    \n",
    "    # import data\n",
    "    df = pd.read_csv(f\"data/{file}\")\n",
    "\n",
    "    # determine species names \n",
    "    species = df.columns.values[2:]\n",
    "\n",
    "    # separate mono culture data \n",
    "    # mono_df = pd.concat([df_i for name, df_i in df.groupby(\"Treatments\") if \"Mono\" in name])\n",
    "    dfs = [df_i for name, df_i in df.groupby(\"Treatments\") if \"Mono\" not in name]\n",
    "\n",
    "    # init kfold object\n",
    "    kf = KFold(n_splits=20, shuffle=True, random_state=21)\n",
    "\n",
    "    # keep track of all predictions\n",
    "    all_pred_species = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    all_stdv = []\n",
    "\n",
    "    # run Kfold \n",
    "    # train_index, test_index = next(iter(kf.split(dfs)))\n",
    "    for train_index, test_index in kf.split(dfs):\n",
    "\n",
    "        # get train df \n",
    "        train_df = pd.concat([dfs[i] for i in train_index])\n",
    "        # train_df = pd.concat((mono_df, train_df))\n",
    "\n",
    "        # get test df\n",
    "        test_df = pd.concat([dfs[i] for i in test_index])\n",
    "\n",
    "        # instantiate gLV fit \n",
    "        model = gLV(species, train_df)\n",
    "\n",
    "        # fit to data \n",
    "        model.fit() \n",
    "\n",
    "        # plot fitness to data\n",
    "        pred_species, true, pred, stdv = predict_df(model, test_df, species)\n",
    "\n",
    "        # append predictions \n",
    "        all_pred_species = np.append(all_pred_species, pred_species)\n",
    "        all_true = np.append(all_true, true)\n",
    "        all_pred = np.append(all_pred, pred)\n",
    "        all_stdv = np.append(all_stdv, stdv)\n",
    "\n",
    "    # save prediction results to a .csv\n",
    "    strain = file.split(\"_\")[1]\n",
    "    kfold_df = pd.DataFrame()\n",
    "    kfold_df['species'] = all_pred_species\n",
    "    kfold_df['true'] = all_true\n",
    "    kfold_df['pred'] = all_pred\n",
    "    kfold_df['stdv'] = all_stdv\n",
    "    kfold_df.to_csv(f\"kfold/{strain}_kfold.csv\", index=False)\n",
    "        \n",
    "    # show prediction performance of individual species\n",
    "    for sp in species:\n",
    "        sp_inds = all_pred_species == sp\n",
    "        R = linregress(all_true[sp_inds], all_pred[sp_inds]).rvalue\n",
    "        plt.scatter(all_true[sp_inds], all_pred[sp_inds], label=f\"{sp} \" + \"R={:.3f}\".format(R))\n",
    "        plt.errorbar(all_true[sp_inds], all_pred[sp_inds], yerr=all_stdv[sp_inds], \n",
    "                     fmt='.', capsize=3)\n",
    "\n",
    "    plt.xlabel(\"Measured OD\")\n",
    "    plt.ylabel(\"Predicted OD\")\n",
    "    plt.legend()\n",
    "    plt.title(strain)\n",
    "    plt.savefig(f\"kfold/{strain}_kfold.pdf\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6eedfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # save prediction results to a .csv\n",
    "    strain = file.split(\"_\")[1]\n",
    "    kfold_df = pd.DataFrame()\n",
    "    kfold_df['species'] = all_pred_species\n",
    "    kfold_df['true'] = all_true\n",
    "    kfold_df['pred'] = all_pred\n",
    "    kfold_df['stdv'] = all_stdv\n",
    "    # kfold_df.to_csv(f\"kfold/{strain}_kfold.csv\", index=False)\n",
    "        \n",
    "    # show prediction performance of individual species\n",
    "    for sp in species:\n",
    "        sp_inds = all_pred_species == sp\n",
    "        R = linregress(all_true[sp_inds], all_pred[sp_inds]).rvalue\n",
    "        plt.scatter(all_true[sp_inds], all_pred[sp_inds], label=f\"{sp} \" + \"R={:.3f}\".format(R))\n",
    "        plt.errorbar(all_true[sp_inds], all_pred[sp_inds], yerr=all_stdv[sp_inds], \n",
    "                     fmt='.', capsize=3)\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.ylim([0, 2.])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db59dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for file in files:\n",
    "#     strain = file.split(\"_\")[1]\n",
    "#     kfold_df = pd.read_csv(f\"kfold/{strain}_kfold.csv\")\n",
    "        \n",
    "#     all_pred_species = kfold_df['species'].values\n",
    "#     all_true = kfold_df['true'].values \n",
    "#     all_pred = kfold_df['pred'].values\n",
    "#     all_stdv = kfold_df['stdv'].values\n",
    "        \n",
    "#     R_overall = linregress(all_true, all_pred).rvalue\n",
    "        \n",
    "#     # show prediction performance of individual species\n",
    "#     for sp in species:\n",
    "#         sp_inds = all_pred_species == sp\n",
    "#         R = linregress(all_true[sp_inds], all_pred[sp_inds]).rvalue\n",
    "#         plt.scatter(all_true[sp_inds], all_pred[sp_inds], label=f\"{sp} \" + \"R={:.3f}\".format(R))\n",
    "#         plt.errorbar(all_true[sp_inds], all_pred[sp_inds], yerr=all_stdv[sp_inds], \n",
    "#                      fmt='.', capsize=3)\n",
    "\n",
    "#     plt.xlabel(\"Measured OD\")\n",
    "#     plt.ylabel(\"Predicted OD\")\n",
    "#     plt.legend()\n",
    "#     plt.title(strain + \" R={:.2f}\".format(R_overall))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f30f45",
   "metadata": {},
   "source": [
    "# Show fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_species, true, pred, stdv = predict_df(model, train_df, species)\n",
    "\n",
    "for sp in species:\n",
    "    sp_inds = pred_species == sp\n",
    "    if sum(true[sp_inds]) > 0:\n",
    "        R = linregress(true[sp_inds], pred[sp_inds]).rvalue\n",
    "        plt.scatter(true[sp_inds], pred[sp_inds], label=f\"{sp} \" + \"R={:.3f}\".format(R))\n",
    "        plt.errorbar(true[sp_inds], pred[sp_inds], yerr= stdv[sp_inds], \n",
    "                     fmt='.', capsize=3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf1e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model, df):\n",
    "\n",
    "    # pull just the community data\n",
    "    test_data = process_df(df, species) \n",
    "\n",
    "    # plot the results\n",
    "    for exp, t_span, Y_m, s_present in test_data:\n",
    "\n",
    "        # increase evaluation time\n",
    "        t_eval = np.linspace(t_span[0], t_span[-1])\n",
    "\n",
    "        # predict \n",
    "        pred, L, U = model.predict(Y_m, t_eval, log=True)\n",
    "        \n",
    "        # un log scale y\n",
    "        Y_m = np.einsum(\"ij,j->ij\", np.exp(Y_m), s_present)\n",
    "\n",
    "        plt.figure(figsize=(9, 6))\n",
    "        for i, s in enumerate(species):\n",
    "            if Y_m[0,i] > 0:\n",
    "                plt.scatter(t_span, Y_m[:,i], label=\"True species \" + s, color='C{}'.format(i), marker='o', s=75)\n",
    "                plt.plot(t_eval, pred[:,i], label=\"Predicted species \" + s, color='C{}'.format(i))\n",
    "                plt.fill_between(t_eval, L[:,i], U[:,i], color='C{}'.format(i), alpha=0.2)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Time\", fontsize=16)\n",
    "        plt.ylabel(\"Abundance\", fontsize=16)\n",
    "        exp_name, passage_num = exp.split(\"_\")\n",
    "        plt.title(f\"{exp_name} passage {passage_num} predictions\")\n",
    "        # plt.savefig(f\"figures/{exp}.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(model, train_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
